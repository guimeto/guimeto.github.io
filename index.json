[{"authors":["admin"],"categories":null,"content":"I am able to discuss and negotiate with scientists/analysts from different backgrounds in order to understand their needs and offer them the best solution and more importantly develop tools that are unanimously accepted. I worked in various environments, ranging in the development of extreme weather indices (droughts, extreme precipitation\u0026hellip;), validation of regional climate models, development of epidemiological indices, monitoring of vector disease, analysis of storm trajectories or work on instrumental data. I am interested by Open Science and willing to learn about Data Science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am able to discuss and negotiate with scientists/analysts from different backgrounds in order to understand their needs and offer them the best solution and more importantly develop tools that are unanimously accepted. I worked in various environments, ranging in the development of extreme weather indices (droughts, extreme precipitation\u0026hellip;), validation of regional climate models, development of epidemiological indices, monitoring of vector disease, analysis of storm trajectories or work on instrumental data.","tags":null,"title":"Guillaume Dueymes","type":"authors"},{"authors":null,"categories":null,"content":"Python basics List in python Flow control Functions in Python Dictionaries Numpy library Pandas library Matplotlib library Project 1 Project 2 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"28e269de8b1350c009cf8456e3f29d72","permalink":"/courses/tutorial_python/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/tutorial_python/","section":"courses","summary":"Learn how to use Python from basics .","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Python Netcdf Part1 Python Netcdf Part2 Cartopy library Xarray library ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"e99f1d7c22efb7c4a58bf423c441bd9e","permalink":"/courses/tutorial_python_netcdf/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/tutorial_python_netcdf/","section":"courses","summary":"Learn how to use Python from basics .","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"1.1 Variables A variable is an identifier or name to store information or results. All python programs use variables. It is important to remember that each type of information is stored in a special type of variable. A type of variable is an information about the contents of the variable. The type of variable will tell the python interpreter what it can do or not with this variable.\n Python can make the difference between upper and lower case letters You can not start a variable with a number Do not use accents in variable names Function : type() give you informations on the type of our variable  m=3.2 int(m) 3  1.1.1 Numeric type:    Type Description     Integer: int() 1 is an integer, 1.0 is not an integer   Float: float() Number that includes a decimal part   Complex numbers Association between a real number and an imaginary number    type(m) float  int(3) 3  int(3.5) 3  float(3) 3.0  3+5 8  1.1.2 Booleans:  variable storing binary values: True or False data types result from logical operations  Very useful in if() conditional structures.\n3\u0026gt;5 False  3\u0026lt;5 True  1.1.3 Strings: Python can also manipulate strings, which can be expressed in several ways. They can be enclosed in single quotes ('\u0026hellip;') or double quotes (\u0026quot;\u0026hellip;\u0026quot;)\nprint(\u0026#34;Hello\u0026#34;) Hello  type(\u0026#39;Hello\u0026#39;) str  type(\u0026#34;15\u0026#34;) str  \u0026#34;15\u0026#34; + 2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-13-3fe5846cb1c2\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 \u0026quot;15\u0026quot; + 2 TypeError: can only concatenate str (not \u0026quot;int\u0026quot;) to str ----------------------------------------------------------------------------  But we can concatenate two strings together.\n\u0026#34;15\u0026#34; + \u0026#34;56\u0026#34; '1556'  str(15) '15'  String objects have a bunch of useful methods; for example:\ns = \u0026#34;hello\u0026#34; print(s.capitalize()) # Capitalize a string; prints \u0026#34;Hello\u0026#34; print(s.upper()) # Convert a string to uppercase; prints \u0026#34;HELLO\u0026#34; print(s.rjust(7)) # Right-justify a string, padding with spaces; prints \u0026#34; hello\u0026#34; print(s.center(7)) # Center a string, padding with spaces; prints \u0026#34; hello \u0026#34; print(s.replace(\u0026#39;l\u0026#39;, \u0026#39;(ell)\u0026#39;)) # Replace all instances of one substring with another; # prints \u0026#34;he(ell)(ell)o\u0026#34; print(\u0026#39;world \u0026#39;.strip()) # Strip leading and trailing whitespace; prints \u0026#34;world\u0026#34; Hello HELLO hello hello he(ell)(ell)o world  1.2 Python Arithmetic Operators    Operator Description     + Adds values   - Subtracts right hand operand from left hand operand   * Multiplies values on either side of the operator   / Divides left hand operand by right hand operand   % Divides left hand operand by right hand operand and returns remainder (modulus)   %% Performs exponential (power) calculation on operators   // Floor Division - The division of operands where the result is the quotient in which the digits after the decimal point are removed. But if one of the operands is negative, the result is floored, i.e., rounded away from zero (towards negative infinity)    5-3 # Subtraction  2  5+3 # Addition  8  5*3 # Multiplication 15  5/3 # division  1.6666666666666667  5//3 # Floor Division  1  5%3 # Modulus  2  5**3 # power 125  ma_variable1=10 ma_variable2=20 ma_variable1*ma_variable2 200  ma_variable1*10 100  We cannot apply arithmetic operators on text.\n\u0026#34;mon texte\u0026#34; - \u0026#34;mon texte\u0026#34; --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-26-5483fafb4a74\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 \u0026quot;mon texte\u0026quot; - \u0026quot;mon texte\u0026quot; TypeError: unsupported operand type(s) for -: 'str' and 'str' ----------------------------------------------------------------------------  \u0026#34;Hi\u0026#34; + \u0026#34;how are you ?\u0026#34; 'Hi how are you ?'  \u0026#34;Hi \u0026#34; * 5 'Hi Hi Hi Hi Hi '  1.3 Python Comparison Operators These operators compare the values on either sides of them and decide the relation among them. They are also called Relational operators. The true value of relationships is often used to make decisions by ensuring that conditions are met to perform a certain task.\n   Operator Description     == If the values of two operands are equal, then the condition becomes true   != If values of two operands are not equal, then condition becomes true   \u0026gt; If the value of left operand is greater than the value of right operand, then condition becomes true   \u0026lt; If the value of left operand is less than the value of right operand, then condition becomes true   \u0026gt;= If the value of left operand is greater than or equal to the value of right operand, then condition becomes true   \u0026lt;= If the value of left operand is less than or equal to the value of right operand, then condition becomes true   is Identity operators compare the memory locations of two objects. Evaluates to true if the variables on either side of the operator point to the same object and false otherwise   is not Identity operators compare the memory locations of two objects. Evaluates to false if the variables on either side of the operator point to the same object and true otherwise    1 == 1 True  1==2 False  1!=2 True  1\u0026gt;2 False  2\u0026gt;1 True  1\u0026gt;1 False  1\u0026lt;2 True  1\u0026gt;=1 True  2 \u0026lt;= 5 True  a=1 # Warning !!!! this is an assignment operator a 1  1.4 Python Logical Operators Logical operators compare Boolean expressions instead of values as in the previous point.\nThey are used to create Boolean expressions that help us know if a certain task should be performed or not. Operator| Description | |\u0026mdash;\u0026mdash;|\u0026mdash;\u0026mdash;| | and| If both operands are true then condition becomes true| | or| If any of the two operands are non-zero then condition becomes true| | not| Used to reverse the logical state of its operand|\n# and (3==3) and (4==4) True  (3==3) and (4==5) False  # or (3==3) or (4==5) True  (3!=3) or (4==5) False  # not not((3!=3) or (4==5)) True  1.5 Python Assignment Operators Assignment operators store a value in a variable. We have already seen the simplest case (=), but python offers many other operators of this type, in particular to perform an arithmetic operation at the same time as the assignment.\n   Operator Description     = Assigns values from right side operands to left side operand   += It adds right operand to the left operand and assign the result to left operand   -= It subtracts right operand from the left operand and assign the result to left operand   *= It multiplies right operand with the left operand and assign the result to left operand   /= It divides left operand with the right operand and assign the result to left operand   %= It takes modulus using two operands and assign the result to left operand   **= Performs exponential (power) calculation on operators and assign value to the left operande   //= It performs floor division on operators and assign value to the left operand    Mavariable = 5 Mavariable = 2 Mavariable 2  Mavariable = 5 Mavariable += 2 Mavariable 7  Mavariable = 5 Mavariable -= 2 Mavariable 3  Mavariable = 5 Mavariable *= 2 Mavariable 10  Mavariable = 5. Mavariable /= 2 Mavariable 2.5  Mavariable = 5 Mavariable %= 2 Mavariable 1  Mavariable = 5 Mavariable **= 2 Mavariable 25  Mavariable = 5. Mavariable //= 2 Mavariable 2.0  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2fa7b87af3ee9cc1febd8a01857099e5","permalink":"/courses/tutorial_python/1-python_basics/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/1-python_basics/","section":"courses","summary":"1.1 Variables A variable is an identifier or name to store information or results. All python programs use variables. It is important to remember that each type of information is stored in a special type of variable. A type of variable is an information about the contents of the variable. The type of variable will tell the python interpreter what it can do or not with this variable.\n Python can make the difference between upper and lower case letters You can not start a variable with a number Do not use accents in variable names Function : type() give you informations on the type of our variable  m=3.","tags":null,"title":"1 Basic data types","type":"docs"},{"authors":null,"categories":null,"content":"Network Common Data Form (Netcdf) is a way to create, access, and share scientific data in a format that is self-documenting and transparent for many types of machines.\nThe Netcdf file itself has information describing the data it contains.\nA NetCDF file has dimensions, variables, and attributes.\nHere is a small example of a NetCDF file, to illustrate these concepts of dimensions, variables and attributes.\nThe notation used to describe this NetCDF file is called Network Common Data Language (CDL). It gives a \u0026ldquo;text\u0026rdquo; version that allows an easy understanding of the structure and contents of a binary NetCDF file:\nnetcdf ./data/NARR_tasmax_201701.nc { dimensions: x = 349; y = 277; time = 31; variables: float lon(y=277, x=349); :units = \u0026quot;degrees_east\u0026quot;; :long_name = \u0026quot;Longitude\u0026quot;; :CoordinateAxisType = \u0026quot;Lon\u0026quot;; float lat(y=277, x=349); :units = \u0026quot;degrees_north\u0026quot;; :long_name = \u0026quot;Latitude\u0026quot;; :CoordinateAxisType = \u0026quot;Lat\u0026quot;; double time(time=31); :long_name = \u0026quot;Time\u0026quot;; :delta_t = \u0026quot;\u0026quot;; float tasmax(time=31, y=277, x=349); :long_name = \u0026quot;Daily maximum temperature\u0026quot;; :units = \u0026quot;Celcius\u0026quot;; :missing_value = -999.0; // double :coordinates = \u0026quot;lon lat\u0026quot;;  }\nPanoply is an apensource software used to quickly read aned visualize Netcdf file: https://www.giss.nasa.gov/tools/panoply/download/\nThe python module that we will use in this section is netCDF4: https://pypi.org/project/netCDF4/\nTo install it under anaconda: conda install -c anaconda netcdf41- Create a Netcdf file: from netCDF4 import Dataset import os import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) file_name = \u0026#34;./DATA/2D_Temperature.nc\u0026#34; if os.path.isfile(file_name): os.remove(file_name) #open the file for writing, you can Also specify format=\u0026#34;NETCDF4_CLASSIC\u0026#34; or \u0026#34;NETCDF3_CLASSIC\u0026#34; #The format is NETCDF4 by default ds = Dataset(file_name, mode=\u0026#34;w\u0026#34;)  Netcdf4-python: createDimension  We will create here a 2D field: (20,20)\nds.createDimension(\u0026#34;x\u0026#34;, 20) ds.createDimension(\u0026#34;y\u0026#34;, 20) ds.createDimension(\u0026#34;time\u0026#34;, None) \u0026lt;class 'netCDF4._netCDF4.Dimension'\u0026gt; (unlimited): name = 'time', size = 0   Netcdf4-python: createVariable  var1 = ds.createVariable(\u0026#34;field1\u0026#34;, \u0026#34;f4\u0026#34;, (\u0026#34;time\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) var2 = ds.createVariable(\u0026#34;field2\u0026#34;, \u0026#34;f4\u0026#34;, (\u0026#34;time\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) #add netcdf attributes var1.units = \u0026#34;Celcius\u0026#34; var1.long_name = \u0026#34;Surface air temperature\u0026#34; var2.units = \u0026#34;Kelvin\u0026#34; var2.long_name = \u0026#34;Surface air temperature\u0026#34; We can now assign our field or variable.\n#generate random data and tell the program where it should go import numpy as np data = np.random.randn(10, 20, 20) var1[:] = data var2[:] = data + 273.15 #actually write data to the disk ds.close(); 2- Read a Netcdf file We will read the file Netcdf4 that we created previously.\nfrom netCDF4 import Dataset ds = Dataset(\u0026#34;./DATA/2D_Temperature.nc\u0026#34;) We will select our variables of interest: no data is loaded at this level.\n# Which variables are in our Netcdf file ? print(ds.variables.keys()) data1_var = ds.variables[\u0026#34;field1\u0026#34;] data2_var = ds.variables[\u0026#34;field2\u0026#34;] #What\u0026#39;s the dimension ? print(data1_var.dimensions, data1_var.shape) odict_keys(['field1', 'field2']) ('time', 'x', 'y') (10, 20, 20)  We can now read our file:\n#now we ask to really read the data into the memory all_data = data1_var[:] #print all_data.shape data1 = data1_var[1,:,:] data2 = data2_var[2,:,:] #data1 print(data1.shape, all_data.shape, all_data.mean(axis = 0).mean(axis = 0).mean(axis = 0)) (20, 20) (10, 20, 20) 0.0055584796  3- Examples of manipulations of a Netcdf file In this example we will work with daily maximum temperature data for all January months from 1971 to 2000 from the CRCM5 regional model developed at the ESCER center.\nWe will import our own module for calculating temperature indices, this module is calld Indices_Temperature.\nAt first we will import the necessary libraries and define the input parameters.\nfrom netCDF4 import Dataset import Indices_Temperature import numpy as np Mois=\u0026#39;01\u0026#39; model=\u0026#39;CRCM5-v1_NAM-44_ll_CCCma-CanESM2\u0026#39; path_model=\u0026#39;CRCM5-v1_CCCma-CanESM2_historical\u0026#39; variable=\u0026#39;tasmax\u0026#39; indice = \u0026#39;Tmax90p\u0026#39; yi = 1971 yf = 2000 ######################################################### rep_data=\u0026#39;./DATA/CRCM5/\u0026#39; rep_out=\u0026#39;./DATA/CRCM5/\u0026#39; tot=(yf-yi)+1  Let's open \u0026lsquo;CRCM5-v1_NAM-44_ll_CCCma-CanESM2_historical_tasmax_197101.nc\u0026rsquo; file:  nc_Modc=Dataset(\u0026#39;./DATA/CRCM5/\u0026#39;+model+\u0026#39;_historical_\u0026#39;+variable+\u0026#39;_197101.nc\u0026#39;,\u0026#39;r\u0026#39;) lats=nc_Modc.variables[\u0026#39;lat\u0026#39;][:] lons=nc_Modc.variables[\u0026#39;lon\u0026#39;][:] varc=nc_Modc.variables[variable][:] ## Quick look of our file:  print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;Temperature dimension = \u0026#39;,varc.shape) print(\u0026#39;Minimum of temperature = \u0026#39;, np.nanmin(varc)) print(\u0026#39;Maximum of temperature = \u0026#39;, np.nanmax(varc)) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;Latitude dimension= \u0026#39;,lats.shape) print(\u0026#39;Minimum of latitude = \u0026#39;, np.min(lats)) print(\u0026#39;Maximum of latitude = \u0026#39;, np.max(lats)) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;Longitude dimesion = \u0026#39;,lons.shape) print(\u0026#39;Minimum of longitude = \u0026#39;, np.min(lons)) print(\u0026#39;Maximum of longitude = \u0026#39;, np.max(lons)) print(\u0026#39;-----------------------------------------\u0026#39;) ----------------------------------------- Temperature dimension = (31, 130, 155) Minimum of temperature = -57.66677 Maximum of temperature = 30.657953 ----------------------------------------- ----------------------------------------- Latitude dimension= (130, 155) Minimum of latitude = 12.538727 Maximum of latitude = 75.86 ----------------------------------------- ----------------------------------------- Longitude dimesion = (130, 155) Minimum of longitude = -170.71053 Maximum of longitude = -23.28948 -----------------------------------------  We can now loop over all our Netcdf files and apply a function to calculate our indice.\nnt=0 IND = np.zeros((tot,130,155),dtype=float) for year in range(yi,yf+1): ###### ouverture et lecture des fichiers Netcdf hist=model+\u0026#39;_historical_\u0026#39;+variable+\u0026#39;_\u0026#39;+str(year)+Mois+\u0026#39;.nc\u0026#39; modelc=rep_data+\u0026#39;/\u0026#39;+hist nc_Modc=Dataset(modelc,\u0026#39;r\u0026#39;) lats=nc_Modc.variables[\u0026#39;lat\u0026#39;][:] lons=nc_Modc.variables[\u0026#39;lon\u0026#39;][:] varc=nc_Modc.variables[variable][:] ###### boucle sur tous les points de grille et calcul de l\u0026#39;indice for ni in range(0, len(varc[0])): for nj in range(0, len(varc[0][0])): if indice == \u0026#39;Mean_tasmax\u0026#39;: IND[nt,ni,nj]=Indices_Temperature.MOY(varc[:,ni,nj]) description=\u0026#39;Monthly Mean of tasmax\u0026#39; unite=\u0026#39;°Celcius\u0026#39; elif indice == \u0026#39;Tmax90p\u0026#39;: IND[nt,ni,nj]=Indices_Temperature.Tmax90p(varc[:,ni,nj]) description=\u0026#39;Monthly Mean of Tmax90p\u0026#39; unite=\u0026#39;°Celcius\u0026#39; nt+=1 ###### Écriture du fichier Netcdf en sortie C = Dataset(rep_out+model+\u0026#39;_historical_\u0026#39;+indice+\u0026#39;_\u0026#39;+str(yi)+\u0026#39;-\u0026#39;+str(yf)+\u0026#39;_\u0026#39;+Mois+\u0026#39;.nc\u0026#39;, \u0026#39;w\u0026#39;) C.description = \u0026#39;Indice temperature\u0026#39; C.conventions = \u0026#39;CF-1.0\u0026#39; C.model_id = model C.grid=\u0026#39;latlon\u0026#39; C.CDO = \u0026#39;Climate Data Operators version 1.6.2 (http://code.zmaw.de/projects/cdo)\u0026#39; C.institution = \u0026#39;UQAM - ESCER Center, University of Quebec in Montreal\u0026#39; C.contact = \u0026#39;Guillaume Dueymes\u0026#39; ######################################## # Dimensions C.createDimension(\u0026#39;x\u0026#39;, len(varc[0][0])) C.createDimension(\u0026#39;y\u0026#39;, len(varc[0])) C.createDimension(\u0026#39;time\u0026#39;, tot) var=C.createVariable(str(indice), np.float32, (\u0026#39;time\u0026#39;,\u0026#39;y\u0026#39;,\u0026#39;x\u0026#39;)) var.long_name = str(description) var.unit = str(unite) lat=C.createVariable(\u0026#39;lat\u0026#39;, np.float32, (\u0026#39;y\u0026#39;,\u0026#39;x\u0026#39;)) lon=C.createVariable(\u0026#39;lon\u0026#39;, np.float32, (\u0026#39;y\u0026#39;,\u0026#39;x\u0026#39;)) time = C.createVariable(\u0026#39;time\u0026#39;, np.float64, (\u0026#39;time\u0026#39;,)) time.long_name = \u0026#39;time\u0026#39; for var in [\u0026#39;lon\u0026#39;,\u0026#39;lat\u0026#39;,\u0026#39;time\u0026#39;]: for att in nc_Modc.variables[var].ncattrs(): setattr(C.variables[var],att,getattr(nc_Modc.variables[var],att)) time[:]=range(1,nt+1) lat[:,:] = lats lon[:,:] = lons C.variables[str(indice)][:,:,:] = IND[::] C.close() IND.shape (30, 130, 155)  4- Netcdf gridpoint extraction In this example we will work with maximum daily temperature data from Regional Reanalysis (NARR) Regional Reanalysis. For more information about this product:\nwww.emc.ncep.noaa.gov/mmb/rreanl\nThe Netcdf files that we will open are daily values from January 1, 2017 to December 31, 2017. Files are archived by month.\nThe NARR grid point closest to the ECCC Montréal / McTavish weather station (45.5N, -73.8W) will be extracted here.\nWe first import the libraries.\nimport netCDF4 import numpy as np import pandas as pd import datetime from datetime import date, timedelta from dateutil.relativedelta import relativedelta rep1=\u0026#39;./DATA/NARR/\u0026#39; model=\u0026#39;NARR\u0026#39; variable=\u0026#39;tasmax\u0026#39; variable_name=\u0026#39;Temperature maximale\u0026#39; yeari=2017 monthi=1 yearf = 2017 monthf = 12 station=\u0026#39;Montreal\u0026#39; lati = 45.5 loni = -73.8 day_start=1 day_end = pd.date_range(\u0026#39;{}-{}\u0026#39;.format(yearf, monthf), periods=1, freq=\u0026#39;M\u0026#39;).day.tolist()[0] start=datetime.datetime(yeari,monthi,day_start) end=datetime.datetime(yearf,monthf,day_end) d0 = date(yeari, monthi, day_start) d1 = date(yearf, monthf, day_end) delta = d1 - d0 nb_days = delta.days+1 var_data=np.zeros(nb_days) We define a function that will calculate the distance between each grid point of the Netcdf file and our reference latitude / longitude. We then deduce the minimum distance.\nThe other function allows us to increment our months when reading Netcdf files.\ndef getclosest_ij(lats,lons,latpt,lonpt): # find squared distance of every point on grid dist_sq = (lats-latpt)**2 + (lons-lonpt)**2 # 1D index of minimum dist_sq element minindex_flattened = dist_sq.argmin() # Get 2D index for latvals and lonvals arrays from 1D index return np.unravel_index(minindex_flattened, lats.shape) def add_month(now): try: then = (now + relativedelta(months=1)).replace(day=now.day) except ValueError: then = (now + relativedelta(months=2)).replace(day=1) return then We can know apply our function over each grid point, month by month and save the value into a DataFrame.\ni=0 IND=[] # Début de notre boucle temporelle incr=start while incr \u0026lt;= end: filename= rep1 + model + \u0026#39;_\u0026#39; + variable + \u0026#39;_\u0026#39; + str(incr.year) + \u0026#39;{:02d}\u0026#39;.format(incr.month) + \u0026#39;.nc\u0026#39; f = netCDF4.Dataset(filename) # print(f.variables.keys()) # get all variable names var = f.variables[variable] # temperature variable #print(temp)  #temp.dimensions #temp.shape lat, lon = f.variables[\u0026#39;lat\u0026#39;], f.variables[\u0026#39;lon\u0026#39;] #print(lat) #print(lon) #print(lat[:]) # extract lat/lon values (in degrees) to numpy arrays latvals = lat[:]; lonvals = lon[:] # a function to find the index of the point closest pt # (in squared distance) to give lat/lon value.  iy_min, ix_min = getclosest_ij(latvals, lonvals, lati, loni) #print(iy_min) #print(ix_min) IND.append(var[:,iy_min,ix_min]) incr=add_month(incr) flattened_list = [y for x in IND for y in x] start=datetime.datetime(yeari,monthi,day_start) TIME=[] for i in range(0,nb_days,1): # TIME.append((start+timedelta(days=i)).strftime(\u0026#34;%Y-%m-%d\u0026#34;)) TIME.append((start+timedelta(days=i))) dataFrame_NARR = pd.DataFrame({\u0026#39;Date\u0026#39;: TIME, variable_name: flattened_list}, columns = [\u0026#39;Date\u0026#39;,variable_name]) dataFrame_NARR = dataFrame_NARR.set_index(\u0026#39;Date\u0026#39;) dataFrame_NARR.head()    Date Temperature maximale     2017-01-01 00:00:00 -0.893957   2017-01-02 00:00:00 -1.53144   2017-01-03 00:00:00 -1.73783   2017-01-04 00:00:00 -0.240698   2017-01-05 00:00:00 -0.791754    We save our DataFrame in csv.\ndataFrame_NARR.to_csv(\u0026#39;./DATA/NARR/NARR_\u0026#39;+station+\u0026#39;_1pt_\u0026#39;+str(variable)+\u0026#39;_\u0026#39;+ \u0026#39;{:02d}\u0026#39;.format(monthi) + str(yeari)+\u0026#39;_\u0026#39;+ \u0026#39;{:02d}\u0026#39;.format(monthf) + str(yearf)+\u0026#39;.csv\u0026#39;)  Observation vs NARR Now we have extracted our gridpoint near Montreal, we can compare these values with observations.  We will extract the data from the Montreal McTavish station and put them in the NARR dataFrame.\ndataframe_station = pd.read_csv(\u0026#39;./DATA/station/MONTREAL_TAVISH_tasmax_1948_2017.csv\u0026#39;, header=None, names=[\u0026#39;Maximum temperature: OBS\u0026#39;]) start = date(1948, 1, 1) end = date(2017, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) dataframe_station[\u0026#39;datetime\u0026#39;] = rng dataframe_station.index = dataframe_station[\u0026#39;datetime\u0026#39;] dataframe_station = dataframe_station.drop([\u0026#34;datetime\u0026#34;], axis=1) dataframe_station.head()    datetime Maximum temperature: OBS     1948-01-01 00:00:00 -10   1948-01-02 00:00:00 -3.9   1948-01-03 00:00:00 -0.6   1948-01-04 00:00:00 -2.8   1948-01-05 00:00:00 -2.2    dataFrame_NARR = dataFrame_NARR.rename(columns={\u0026#34;Temperature maximale\u0026#34;: \u0026#34;Maximum temperature: NARR\u0026#34;}) df_NARR_Station = pd.concat([dataframe_station,dataFrame_NARR],axis=1) df_NARR_Station.tail()     Maximum temperature: OBS Maximum temperature: NARR     2017-12-27 00:00:00 -17.6 -18.819   2017-12-28 00:00:00 -20.5 -19.3721   2017-12-29 00:00:00 -18.4 -16.4532   2017-12-30 00:00:00 -17.3 -16.2088   2017-12-31 00:00:00 -18.3 -16.7728    We want to plot valeurs for 2016 and 2017 years.\ndf=df_NARR_Station.loc[\u0026#39;2016\u0026#39; : \u0026#39;2017\u0026#39;] df.head()     Maximum temperature: OBS Maximum temperature: NARR     2016-01-01 00:00:00 -0.1 nan   2016-01-02 00:00:00 0.4 nan   2016-01-03 00:00:00 0.2 nan   2016-01-04 00:00:00 -13.5 nan   2016-01-05 00:00:00 -7.5 nan    import matplotlib.pyplot as plt color = [\u0026#39;black\u0026#39;, \u0026#39;red\u0026#39;] fig = plt.figure(figsize=(16,8)) plt.plot(df.index, df[\u0026#39;Maximum temperature: NARR\u0026#39;][:], label=\u0026#39;NARR Temperature\u0026#39;, linewidth=2, c=color[0]) plt.plot(df.index, df[\u0026#39;Maximum temperature: OBS\u0026#39;][:], label=\u0026#39;Observation Temperature\u0026#39;, linewidth=2, c=color[1]) # Autre méthode pour tracer avec Pandas #df_NARR_Station[\u0026#39;2017\u0026#39;].plot(figsize=(10,5)) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Temperature\u0026#34;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 10}) plt.title(\u0026#34;Time serie: Station MTL vs NARR\u0026#34;, y=1.05) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 1, 1, 0),fontsize =10) plt.savefig(\u0026#34;figures/NARR_time_Serie_temperature.png\u0026#34;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # bbox_inches= : option qui permet de propostionner le graphique lors de l\u0026#39;enregistrement plt.show()  Example of using the .rolling () \u0026lt;/ b\u0026gt; function to calculate a moving average on a signal.  import matplotlib.pyplot as plt df_NARR_Station[\u0026#39;rollingmean5 Station\u0026#39;]= df_NARR_Station[\u0026#39;Maximum temperature: OBS\u0026#39;].rolling(window=5).mean() df_NARR_Station[\u0026#39;rollingmean5 NARR\u0026#39;]= df_NARR_Station[\u0026#39;Maximum temperature: NARR\u0026#39;].rolling(window=5).mean() df_NARR_Station.tail()     Maximum temperature: OBS Maximum temperature: NARR rollingmean5 Station rollingmean5 NARR     2017-12-27 00:00:00 -17.6 -18.819 -8.04 -8.31893   2017-12-28 00:00:00 -20.5 -19.3721 -11.12 -11.0995   2017-12-29 00:00:00 -18.4 -16.4532 -13.88 -13.6478   2017-12-30 00:00:00 -17.3 -16.2088 -16.12 -15.6138   2017-12-31 00:00:00 -18.3 -16.7728 -18.42 -17.5252    df_NARR_Station[\u0026#39;2017-01\u0026#39;:\u0026#39;2017-12\u0026#39;].plot(figsize=(16,8)) plt.xlabel(\u0026#34;Temps\u0026#34;) plt.ylabel(\u0026#34;Température\u0026#34;) plt.title(\u0026#34;Maximum temperature time serie: Station MTL vs NARR\u0026#34;, y=1.05) plt.savefig(\u0026#34;figures/NARR_time_Serie_temperature2.png\u0026#34;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # bbox_inches= : option qui permet de propostionner le graphique lors de l\u0026#39;enregistrement plt.show() ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"a457cdd35b9332a6dccc91e35455b160","permalink":"/courses/tutorial_python_netcdf/1-netcdf_tutorial/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python_netcdf/1-netcdf_tutorial/","section":"courses","summary":"Network Common Data Form (Netcdf) is a way to create, access, and share scientific data in a format that is self-documenting and transparent for many types of machines.\nThe Netcdf file itself has information describing the data it contains.\nA NetCDF file has dimensions, variables, and attributes.\nHere is a small example of a NetCDF file, to illustrate these concepts of dimensions, variables and attributes.\nThe notation used to describe this NetCDF file is called Network Common Data Language (CDL).","tags":null,"title":"1 Netcdf Part 1","type":"docs"},{"authors":null,"categories":null,"content":"Instead of extracting a grid point, we may want to extract a domain / box delimited by latitudes and longitudes.\nIn this example we will work with the annual mean of daily maximum temperature data from several regional models of CORDEX-NAM44.\nWe will extract all the grid points from the region between 47degN and 51degN of latitude and between -72degW and -64degW of longitude .\nWe will then plot the inter-annual variability of the mean annual temperature anomalies.\n# we first inmport librairies import netCDF4 import numpy as np import pandas as pd from datetime import datetime import matplotlib.pylab as plt import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) import seaborn as sns from matplotlib import gridspec # path and variable name rep=\u0026#39;./DATA/Inter_annual_anomaly/\u0026#39; variable_in = \u0026#39;Mean_tasmax\u0026#39; # list of periods we want to use list_period = [\u0026#39;2011-2040\u0026#39;,\u0026#39;2041-2070\u0026#39;,\u0026#39;2071-2100\u0026#39;] # list of models list_rcp45 = [\u0026#39;CRCM5-v1_NAM-44_ll_CCCma-CanESM2_rcp45\u0026#39;, \u0026#39;CRCM5-v1_NAM-44_ll_MPI-M-MPI-ESM-LR_rcp45\u0026#39; ] # Area to extract latbounds = [ 47 , 51 ] lonbounds = [ -72 , -64 ] We will make a loop over each model and period to make the extraction:\n#  df_rcp45 = [] matrix_45 = [] for period in list_period: globals()[\u0026#39;flattened_list_\u0026#39;+period] = [] # we define a global variable  for i in range(0,len(list_rcp45)): filename= rep +\u0026#39;anomalie_\u0026#39; + list_rcp45[i] + \u0026#39;_\u0026#39; + variable_in + \u0026#39;_\u0026#39; + period + \u0026#39;_1971-2000.nc\u0026#39; nc = netCDF4.Dataset(filename) # we here read netcdf values var = nc.variables[variable_in][:] lats = nc.variables[\u0026#39;lat\u0026#39;][:]; lons = nc.variables[\u0026#39;lon\u0026#39;][:] # in this part, we extract our domain  subset = ((lats \u0026gt; latbounds[0]) \u0026amp; (lats \u0026lt; latbounds[1]) \u0026amp; (lons \u0026gt; lonbounds[0]) \u0026amp; (lons \u0026lt; lonbounds[1])) data=pd.DataFrame(var[:,subset], dtype=\u0026#39;float\u0026#39;) globals()[\u0026#39;flattened_list_\u0026#39;+period].append(data.mean(axis=1)) df_rcp45.append(pd.DataFrame(globals()[\u0026#39;flattened_list_\u0026#39;+period]).T) df_rcp45 = pd.concat(df_rcp45) df_rcp45.head()     0 1     0 1.72084 -0.288691   1 1.4762 0.885357   2 0.581686 0.541863   3 0.390671 -0.0141989   4 1.14842 1.29417    We can add datetime index in our DataFrame.\nTIME=[] for y in range(int(list_period[0].split(\u0026#39;-\u0026#39;)[0]),int(list_period[-1].split(\u0026#39;-\u0026#39;)[-1])+1,1): TIME.append(datetime.strptime(str(y), \u0026#39;%Y\u0026#39;)) df_rcp45[\u0026#39;Date\u0026#39;] = TIME df_rcp45.index = df_rcp45[\u0026#39;Date\u0026#39;] df_rcp45 = df_rcp45.drop([\u0026#34;Date\u0026#34;], axis=1) df_rcp45.head()    Date 0 1     2011-01-01 00:00:00 1.72084 -0.288691   2012-01-01 00:00:00 1.4762 0.885357   2013-01-01 00:00:00 0.581686 0.541863   2014-01-01 00:00:00 0.390671 -0.0141989   2015-01-01 00:00:00 1.14842 1.29417    We then want, for each year, the intra-model variability. For this, we will calculate the minimum and maximum values by applying the .apply method.\ndf_rcp45[\u0026#39;min\u0026#39;] = df_rcp45.apply(np.min, axis=1) df_rcp45[\u0026#39;max\u0026#39;] = df_rcp45.apply(np.max, axis=1) df_rcp45[\u0026#39;mean\u0026#39;] = df_rcp45.apply(np.mean, axis=1) df_rcp45.head()    Date 0 1 min max mean     2011-01-01 00:00:00 1.72084 -0.288691 -0.288691 1.72084 0.716076   2012-01-01 00:00:00 1.4762 0.885357 0.885357 1.4762 1.18078   2013-01-01 00:00:00 0.581686 0.541863 0.541863 0.581686 0.561774   2014-01-01 00:00:00 0.390671 -0.0141989 -0.0141989 0.390671 0.188236   2015-01-01 00:00:00 1.14842 1.29417 1.14842 1.29417 1.2213    We will make the same work with RCMs in future condition with rcp8.5 scenario and RCMs in historical conditions.\nlist_rcp85 = [\u0026#39;CRCM5-v1_NAM-44_ll_CCCma-CanESM2_rcp85\u0026#39;,\u0026#39;CRCM5-v1_NAM-44_ll_MPI-M-MPI-ESM-MR_rcp85\u0026#39;] df_rcp85 = [] matrix_85 = [] for period in list_period: globals()[\u0026#39;flattened_list_\u0026#39;+period] = [] for i in range(0,len(list_rcp85)): filename= rep + \u0026#39;anomalie_\u0026#39; + list_rcp85[i] + \u0026#39;_\u0026#39; + variable_in + \u0026#39;_\u0026#39; + period + \u0026#39;_1971-2000.nc\u0026#39; nc = netCDF4.Dataset(filename) var = nc.variables[variable_in][:] lats = nc.variables[\u0026#39;lat\u0026#39;][:]; lons = nc.variables[\u0026#39;lon\u0026#39;][:] subset = ((lats \u0026gt; latbounds[0]) \u0026amp; (lats \u0026lt; latbounds[1]) \u0026amp; (lons \u0026gt; lonbounds[0]) \u0026amp; (lons \u0026lt; lonbounds[1])) #mask = np.where(subset) data=pd.DataFrame(var[:,subset], dtype=\u0026#39;float\u0026#39;) globals()[\u0026#39;flattened_list_\u0026#39;+period].append(data.mean(axis=1)) df_rcp85.append(pd.DataFrame(globals()[\u0026#39;flattened_list_\u0026#39;+period]).T) df_rcp85 = pd.concat(df_rcp85) df_rcp85[\u0026#39;Date\u0026#39;] = TIME df_rcp85.index = df_rcp85[\u0026#39;Date\u0026#39;] df_rcp85 = df_rcp85.drop([\u0026#34;Date\u0026#34;], axis=1) df_rcp85[\u0026#39;min\u0026#39;] = df_rcp85.apply(np.min, axis=1) df_rcp85[\u0026#39;max\u0026#39;] = df_rcp85.apply(np.max, axis=1) df_rcp85[\u0026#39;mean\u0026#39;] = df_rcp85.apply(np.mean, axis=1) ### historical RCMs list_histo = [\u0026#39;CRCM5-v1_NAM-44_ll_CCCma-CanESM2_historical\u0026#39;, \u0026#39;CRCM5-v1_NAM-44_ll_MPI-M-MPI-ESM-LR_historical\u0026#39;] df_histo = [] globals()[\u0026#39;flattened_list_\u0026#39;+period] = [] for i in range(0,len(list_histo)): filename= rep + \u0026#39;anomalie_\u0026#39; + list_histo[i] + \u0026#39;_\u0026#39; + variable_in + \u0026#39;_1971-2000_1971-2000.nc\u0026#39; nc = netCDF4.Dataset(filename) var = nc.variables[variable_in][:] lats = nc.variables[\u0026#39;lat\u0026#39;][:]; lons = nc.variables[\u0026#39;lon\u0026#39;][:] subset = ((lats \u0026gt; latbounds[0]) \u0026amp; (lats \u0026lt; latbounds[1]) \u0026amp; (lons \u0026gt; lonbounds[0]) \u0026amp; (lons \u0026lt; lonbounds[1])) #mask = np.where(subset) data=pd.DataFrame(var[:,subset], dtype=\u0026#39;float\u0026#39;) globals()[\u0026#39;flattened_list_\u0026#39;+period].append(data.mean(axis=1)) df_histo.append(pd.DataFrame(globals()[\u0026#39;flattened_list_\u0026#39;+period]).T) TIME=[] for y in range(1971,2001,1): TIME.append(datetime.strptime(str(y), \u0026#39;%Y\u0026#39;)) df_histo = pd.concat(df_histo) df_histo[\u0026#39;Date\u0026#39;] = TIME df_histo.index = df_histo[\u0026#39;Date\u0026#39;] df_histo = df_histo.drop([\u0026#34;Date\u0026#34;], axis=1) df_histo[\u0026#39;min\u0026#39;] = df_histo.apply(np.min, axis=1) df_histo[\u0026#39;max\u0026#39;] = df_histo.apply(np.max, axis=1) df_histo[\u0026#39;mean\u0026#39;] = df_histo.apply(np.mean, axis=1) We put all results in the same DataFrame:\nresult = [] result = pd.DataFrame({\u0026#39;min_rcp45\u0026#39;: df_rcp45[\u0026#39;min\u0026#39;], \u0026#39;max_rcp45\u0026#39;: df_rcp45[\u0026#39;max\u0026#39;],\u0026#39;mean_rcp45\u0026#39;: df_rcp45[\u0026#39;mean\u0026#39;], \u0026#39;min_rcp85\u0026#39;: df_rcp85[\u0026#39;min\u0026#39;], \u0026#39;max_rcp85\u0026#39;: df_rcp85[\u0026#39;max\u0026#39;],\u0026#39;mean_rcp85\u0026#39;: df_rcp85[\u0026#39;mean\u0026#39;], \u0026#39;min_histo\u0026#39;: df_histo[\u0026#39;min\u0026#39;], \u0026#39;max_histo\u0026#39;: df_histo[\u0026#39;max\u0026#39;],\u0026#39;mean_histo\u0026#39;: df_histo[\u0026#39;mean\u0026#39;]}, columns = [\u0026#39;min_rcp45\u0026#39;,\u0026#39;max_rcp45\u0026#39;,\u0026#39;mean_rcp45\u0026#39;,\u0026#39;min_rcp85\u0026#39;,\u0026#39;max_rcp85\u0026#39;,\u0026#39;mean_rcp85\u0026#39;,\u0026#39;min_histo\u0026#39;,\u0026#39;max_histo\u0026#39;,\u0026#39;mean_histo\u0026#39;]) result.tail()    Date min_rcp45 max_rcp45 mean_rcp45 min_rcp85 max_rcp85 mean_rcp85 min_histo max_histo mean_histo     2096-01-01 00:00:00 1.89008 4.49628 3.19318 5.44184 8.03633 6.73908 nan nan nan   2097-01-01 00:00:00 3.86389 5.06262 4.46325 6.10506 7.35563 6.73034 nan nan nan   2098-01-01 00:00:00 2.28996 4.60365 3.4468 6.58306 7.14778 6.86542 nan nan nan   2099-01-01 00:00:00 2.94984 4.26892 3.60938 6.40518 7.04316 6.72417 nan nan nan   2100-01-01 00:00:00 2.99867 3.27685 3.13776 6.16794 6.96608 6.56701 nan nan nan    We can now plot our inter-annual variability:\ncolor = [\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;] fig = plt.figure(figsize=(10, 6)) gs = gridspec.GridSpec(1, 2, width_ratios=[6, 1]) gs.update( wspace=0.04) ax1 = plt.subplot(gs[0]) plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] #  plt.plot(result.index.year, result[\u0026#39;mean_histo\u0026#39;][:], label=\u0026#39;RCMs historical\u0026#39;, linewidth=2, c=color[0]) plt.plot(result.index.year, result[\u0026#39;mean_rcp45\u0026#39;][:], label=\u0026#39;RCMs scenario rcp 4.5\u0026#39;, linewidth=2, c=color[1]) plt.plot(result.index.year, result[\u0026#39;mean_rcp85\u0026#39;][:], label=\u0026#39;RCMs scenario rcp 8.5\u0026#39;, linewidth=2, c=color[2]) plt.fill_between(result.index.year,result[\u0026#39;min_histo\u0026#39;],result[\u0026#39;max_histo\u0026#39;], color = color[0], alpha=.2) plt.fill_between(result.index.year,result[\u0026#39;min_rcp45\u0026#39;],result[\u0026#39;max_rcp45\u0026#39;], color = color[1], alpha=.2) plt.fill_between(result.index.year,result[\u0026#39;min_rcp85\u0026#39;],result[\u0026#39;max_rcp85\u0026#39;], color = color[2], alpha=.2) plt.legend(loc=\u0026#34;upper left\u0026#34;, markerscale=1., scatterpoints=1, fontsize=20) plt.xticks(range(result.index.year[0]-1, result.index.year[-1]+1, 10), fontsize=14) plt.yticks( fontsize=14) ax1.grid(axis = \u0026#34;x\u0026#34;, linestyle = \u0026#34;--\u0026#34;, color=\u0026#39;black\u0026#39;, linewidth=0.25, alpha=0.5) ax1.grid(axis = \u0026#34;y\u0026#34;, linestyle = \u0026#34;--\u0026#34;, color=\u0026#39;black\u0026#39;, linewidth=0.25, alpha=0.5) plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\u0026#34;right\u0026#34;) plt.xlabel(\u0026#39;Date\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.ylabel(\u0026#39;°C\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.title(\u0026#39;Annual change in daily maximum temperature: (1971-2100) compared with normal (1971-2000)\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) ax1.set_facecolor(\u0026#39;white\u0026#39;) plt.yticks( fontsize=14) plt.show() We would add next to our chart a boxplot on all models for the period 1971-2000 and 2071-2100 only. We will extract these periods from our matrix result = []\nlist_to_remove = [\u0026#39;min\u0026#39;,\u0026#39;max\u0026#39;,\u0026#39;mean\u0026#39;] df_histo = df_histo.drop(list_to_remove, axis=1) df_rcp45 = df_rcp45.drop(list_to_remove, axis=1) df_rcp85 = df_rcp85.drop(list_to_remove, axis=1) df_histo = df_histo.loc[\u0026#39;1971\u0026#39; : \u0026#39;2010\u0026#39;].stack() df_rcp45 = df_rcp45.loc[\u0026#39;2071\u0026#39; : \u0026#39;2100\u0026#39;].stack() df_rcp85 = df_rcp85.loc[\u0026#39;2071\u0026#39; : \u0026#39;2100\u0026#39;].stack() matrix_box = pd.DataFrame({\u0026#39;RCMs_histo\u0026#39;: df_histo, \u0026#39;RCMs_rcp45\u0026#39;: df_rcp45,\u0026#39;RCMs_rcp85\u0026#39;: df_rcp85}, columns = [\u0026#39;RCMs_histo\u0026#39;,\u0026#39;RCMs_rcp45\u0026#39;,\u0026#39;RCMs_rcp85\u0026#39;]) matrix_box.head()     RCMs_histo RCMs_rcp45 RCMs_rcp85     (Timestamp(\u0026lsquo;1971-01-01 00:00:00\u0026rsquo;), 0) -1.34157 nan nan   (Timestamp(\u0026lsquo;1971-01-01 00:00:00\u0026rsquo;), 1) -0.12412 nan nan   (Timestamp(\u0026lsquo;1972-01-01 00:00:00\u0026rsquo;), 0) -0.641267 nan nan   (Timestamp(\u0026lsquo;1972-01-01 00:00:00\u0026rsquo;), 1) -0.0350673 nan nan   (Timestamp(\u0026lsquo;1973-01-01 00:00:00\u0026rsquo;), 0) 0.0541643 nan nan    color = [\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;] fig = plt.figure(figsize=(22, 12)) gs = gridspec.GridSpec(1, 2, width_ratios=[6, 1]) gs.update( wspace=0.04) ax1 = plt.subplot(gs[0]) plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] #  plt.plot(result.index.year, result[\u0026#39;mean_histo\u0026#39;][:], label=\u0026#39;RCMs historical\u0026#39;, linewidth=2, c=color[0]) plt.plot(result.index.year, result[\u0026#39;mean_rcp45\u0026#39;][:], label=\u0026#39;RCMs scenario rcp 4.5\u0026#39;, linewidth=2, c=color[1]) plt.plot(result.index.year, result[\u0026#39;mean_rcp85\u0026#39;][:], label=\u0026#39;RCMs scenario rcp 8.5\u0026#39;, linewidth=2, c=color[2]) plt.fill_between(result.index.year,result[\u0026#39;min_histo\u0026#39;],result[\u0026#39;max_histo\u0026#39;], color = color[0], alpha=.2) plt.fill_between(result.index.year,result[\u0026#39;min_rcp45\u0026#39;],result[\u0026#39;max_rcp45\u0026#39;], color = color[1], alpha=.2) plt.fill_between(result.index.year,result[\u0026#39;min_rcp85\u0026#39;],result[\u0026#39;max_rcp85\u0026#39;], color = color[2], alpha=.2) plt.legend(loc=\u0026#34;upper left\u0026#34;, markerscale=1., scatterpoints=1, fontsize=20) #ax.set_xlim(result.index.year[0], result.index.year[-1]) plt.xticks(range(result.index.year[0]-1, result.index.year[-1]+1, 10), fontsize=14) plt.yticks( fontsize=14) ax1.grid(axis = \u0026#34;x\u0026#34;, linestyle = \u0026#34;--\u0026#34;, color=\u0026#39;black\u0026#39;, linewidth=0.25, alpha=0.5) ax1.grid(axis = \u0026#34;y\u0026#34;, linestyle = \u0026#34;--\u0026#34;, color=\u0026#39;black\u0026#39;, linewidth=0.25, alpha=0.5) plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\u0026#34;right\u0026#34;) plt.xlabel(\u0026#39;Date\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.ylabel(\u0026#39;°C\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.title(\u0026#39;Annual change in daily maximum temperature: (1971-2100) compared with normal (1971-2000)\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) my_pal = {\u0026#34;RCMs_histo\u0026#34;: \u0026#34;grey\u0026#34;, \u0026#34;RCMs_rcp45\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;RCMs_rcp85\u0026#34;:\u0026#34;red\u0026#34;} ax2 = plt.subplot(gs[1]) #ax2 = matrix_box.boxplot(column=[\u0026#39;RCMs_histo\u0026#39;, \u0026#39;RCMs_rcp45\u0026#39;, \u0026#39;RCMs_rcp85\u0026#39;]) ax2 = sns.boxplot(data=matrix_box, palette=my_pal) # Add transparency to colors for patch in ax2.artists: r, g, b, a = patch.get_facecolor() patch.set_facecolor((r, g, b, .2)) plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\u0026#34;right\u0026#34;) ax1.set_facecolor(\u0026#39;white\u0026#39;) ax2.set_facecolor(\u0026#39;white\u0026#39;) ax2.spines[\u0026#39;top\u0026#39;].set_visible(False) ax2.spines[\u0026#39;bottom\u0026#39;].set_visible(False) ax2.spines[\u0026#39;right\u0026#39;].set_visible(False) ax2.spines[\u0026#39;left\u0026#39;].set_visible(False) medians = matrix_box.median().values median_labels = [str(np.round(s, 2))+\u0026#39;°C\u0026#39; for s in medians] pos = range(len(medians)) i=0 for tick,label in zip(pos,ax2.get_xticklabels()): ax2.text(pos[tick], medians[tick] + 0.1, median_labels[tick], horizontalalignment=\u0026#39;center\u0026#39;, size=\u0026#39;medium\u0026#39;, color = color[i], weight=\u0026#39;semibold\u0026#39;) i+=1 x1, x2, x3 = 0, 1, 2 ax2.text(x1, matrix_box.min().min().round()-0.15 , \u0026#34;1971-2000\u0026#34;, ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;, size=\u0026#39;medium\u0026#39;, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) ax2.text((x2+x3)*.5, matrix_box.min().median()-0.4 , \u0026#34;2071-2100\u0026#34;, ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;, size=\u0026#39;medium\u0026#39;, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.yticks( fontsize=14) plt.savefig(\u0026#39;./figures/VI_YEAR_Mean_tasmax.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, format=\u0026#39;png\u0026#39;, dpi=1000) plt.show() ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"fec61d30f4b6458db6e7354598fb7734","permalink":"/courses/tutorial_python_netcdf/2-netcdf_tutorial_2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python_netcdf/2-netcdf_tutorial_2/","section":"courses","summary":"Instead of extracting a grid point, we may want to extract a domain / box delimited by latitudes and longitudes.\nIn this example we will work with the annual mean of daily maximum temperature data from several regional models of CORDEX-NAM44.\nWe will extract all the grid points from the region between 47degN and 51degN of latitude and between -72degW and -64degW of longitude .\nWe will then plot the inter-annual variability of the mean annual temperature anomalies.","tags":null,"title":"2 Netcdf Part 2","type":"docs"},{"authors":null,"categories":null,"content":"The Cartopy python library allows you to analyze, process and plot georeferenced data with the help of Matplotlib.\nhttps://scitools.org.uk/cartopy/docs/latest/#\nTo install the library under the prompt of anaconda: conda install -c conda-forge cartopy\n1 - Example of drawing a 2D Netcdf field  We will work climatology (1981-2010) monthly minimum daily temperature At first we will import the Python libraries and open the Netcdf file  The Dataset class of the netCDF4 module is used to open and read Netcdf files. Here we will store the Netcdf file in the variable nc_fid.\nfrom netCDF4 import Dataset import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) filename=\u0026#39;./DATA/ANUSPLIN/ANUSPLIN_NLDAS_10km_CLIMATO_TASMIN_1981_2010_06.nc\u0026#39; nc_fid=Dataset(filename,\u0026#39;r\u0026#39;) nc_fid.variables OrderedDict([('lon', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 lon(y, x) units: degrees_east long_name: Longitude CoordinateAxisType: Lon unlimited dimensions: current shape = (1068, 420) filling on, default _FillValue of 9.969209968386869e+36 used), ('lat', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 lat(y, x) units: degrees_north long_name: Latitude CoordinateAxisType: Lat unlimited dimensions: current shape = (1068, 420) filling on, default _FillValue of 9.969209968386869e+36 used), ('time', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float64 time(time) long_name: Time delta_t: units: Days unlimited dimensions: current shape = (1,) filling on, default _FillValue of 9.969209968386869e+36 used), ('TASMIN', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 TASMIN(time, y, x) long_name: Daily minimum temperature units: Celcius missing_value: -999.0 coordinates: lon lat unlimited dimensions: current shape = (1, 1068, 420) filling on, default _FillValue of 9.969209968386869e+36 used)])  We will read TASMIN, lat and lon variables.\nlats = nc_fid.variables[\u0026#39;lat\u0026#39;][:] lons = nc_fid.variables[\u0026#39;lon\u0026#39;][:] time = nc_fid.variables[\u0026#39;time\u0026#39;][:] Vals = nc_fid.variables[\u0026#39;TASMIN\u0026#39;][:].squeeze() print( Vals.shape) (1068, 420)  We will now call the cartopy and matplotlib libraries from Python to create a graphical instance. The carto library brings functions to visually enrich maps made with cartopy: adding a scale \u0026hellip;\nimport matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np import matplotlib as mpl from carto import scale_bar We create an instance of Cartopy to create a map. We choose here our projection, grid orientation and total coverage.\nHere is a very useful link to choose a projection type: https://scitools.org.uk/cartopy/docs/latest/crs/projections.html\nprojections = [ccrs.PlateCarree(), ccrs.Robinson(), ccrs.Mercator(), ccrs.Orthographic(), ccrs.InterruptedGoodeHomolosine(), ccrs.LambertConformal(), ] for proj in projections: plt.figure() ax = plt.axes(projection=proj) ax.stock_img() ax.coastlines() ax.set_title(f\u0026#39;{type(proj)}\u0026#39;) In this example, we will work with a so-called \u0026ldquo;LambertConformal\u0026rdquo; projection. To create a regional map, we use the GeoAxis set_extent method to limit the size of our region.\nfig=plt.figure(figsize=(5,3), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-100,-60,18,62]) We will add some products to our map with Cartopy's cartopy.feature method: https://scitools.org.uk/cartopy/docs/v0.14/matplotlib/feature_interface.html\n         cartopy.feature.BORDERS Borders   cartopy.feature.COASTLINE Coast   cartopy.feature.LAKES Lakes   cartopy.feature.LAND Continents   cartopy.feature.OCEAN Ocean   cartopy.feature.RIVERS Rivers   cartopy.feature.STATES States    We will also add the provinces with Cartopy's NaturalEarthFeature class: https://scitools.org.uk/cartopy/docs/v0.16/matplotlib/feature_interface.html#cartopy.feature.NaturalEarthFeature\nWe work here by adding layers.\n# adding caost: ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); plt.show() # adding land and oceanss: ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) plt.show() # adding lakes and rivers ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) plt.show() # Adding states ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) plt.show() We can now fill our map with our georeferenced field with the method pcolormesh of matplotlib: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pcolormesh.html\nHere Lat Lon coordinates are in 2D, so in each grid point we know the latitude and longitude of our fields.\nIt is possible that for a netcdf file Lat and Lon are in 1D. We must then write them in 2D with the following command:\n-lon, lat = np.meshgrid (lons, lats)\nWe will also produce our own color palette. Here is a useful link to select our colors and create our palette: http://colorbrewer2.org/#type=diverging\u0026amp;scheme=RdYlBu\u0026amp;n=8\nYou can also use one of the color palettes predefined by Matplotlip: https://matplotlib.org/examples/color/colormaps_reference.html\nWe will use to draw our map type pcolormesh matplotib. Other types of plots are available: https://matplotlib.org/basemap/users/examples.html\nTo improve the reading of our map, we can at this level add:\n a legend under the map: cbar.set_label add a color bar with a display interval: plt.colorbar  Y=np.array([[77,0,111],[115,14,181],[160,17,222],[195,14,240],\\ [0,0,93],[21,38,177],[33,95,227],[32,162,247],[59,224,248],[202,255,250],\\ [4,255,179],[37,181,139],[32,132,81],[72,162,60],[157,240,96],[213,255,166],\\ [241,247,132],[248,185,68],[255,124,4],[235,78,14],[215,32,24],[189,24,40],[162,16,56],[135,16,65],[107,15,73]])/255. colbar=mpl.colors.ListedColormap(Y) fig=plt.figure(figsize=(10,6), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) mm = ax.pcolormesh(lons,\\ lats,\\ Vals,\\ vmin=-28,\\ vmax=28, \\ transform=ccrs.PlateCarree(),\\ cmap=colbar ) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); plt.show() If the pallet is not suitable, it is possible to use a palette of Matplotlib.\nfig=plt.figure(figsize=(10,6), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) mm = ax.pcolormesh(lons,\\ lats,\\ Vals,\\ vmin=-28,\\ vmax=28, \\ transform=ccrs.PlateCarree(),\\ cmap=\u0026#39;jet\u0026#39; ) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); plt.show() We can finally add: - a title to our chart: plt.title - a color bar plt.colorbar - a title to our color bar: plt.xlabel - latitudes and longitudes with the function: gridlines () - a scale bar: scale_bar ()\nThen save our chart with the command: plt.savefig\nY=np.array([[77,0,111],[115,14,181],[160,17,222],[195,14,240],\\ [0,0,93],[21,38,177],[33,95,227],[32,162,247],[59,224,248],[202,255,250],\\ [4,255,179],[37,181,139],[32,132,81],[72,162,60],[157,240,96],[213,255,166],\\ [241,247,132],[248,185,68],[255,124,4],[235,78,14],[215,32,24],[189,24,40],[162,16,56],[135,16,65],[107,15,73]])/255. colbar=mpl.colors.ListedColormap(Y) fig=plt.figure(figsize=(22,12), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) mm = ax.pcolormesh(lons,\\ lats,\\ Vals,\\ vmin=-28,\\ vmax=28, \\ transform=ccrs.PlateCarree(),\\ cmap=colbar ) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) ax.gridlines() # Standard 6,000 km scale bar. scale_bar(ax, (0.85, 0.2), 500 ,plot_kwargs = dict(linestyle=\u0026#39;dashed\u0026#39;, color=\u0026#39;black\u0026#39;)) cbar = plt.colorbar(mm, orientation=\u0026#39;horizontal\u0026#39;, shrink=0.75, drawedges=\u0026#39;True\u0026#39;, ticks=np.arange(-28, 28.1, 4),extend=\u0026#39;both\u0026#39;) cbar.set_label(u\u0026#39;\\nProjection = Lambert Conformal Conic / Lambert Conique Conforme\\nResolution: 5 Arcs-Minutes (10 km)\\nData provided by Natural Resources Canada and the University of Washington/ Données fournies par Ressources Naturelles Canada et l Université de Washington\u0026#39;, size=\u0026#39;medium\u0026#39;) # Affichage de la légende de la barre de couleur cbar.ax.tick_params(labelsize=17) plt.xlabel(u\u0026#39;\\n\\n\\nTemperature / Température (°C)\u0026#39;,size=\u0026#39;x-large\u0026#39;) string_title=u\u0026#39;Daily Minimum Temperature / Température Minimale Quotidienne\\nin June / en Juin \\nMonthly Mean Climatology / Climatologie Moyenne Mensuelle (1981 - 2010)\\n\\n\u0026#39; plt.title(string_title, size=\u0026#39;xx-large\u0026#39;) plt.savefig(\u0026#39;./figures/ANUSPLIN_NLDAS_10km_CLIM_TASMIN_06_1981-2010.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() 2 - Another example 2D Netcdf  We will work with the climatology of the total monthly precipitation accumulation (1981-2010) for the month of December.  from netCDF4 import Dataset import matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np from carto import scale_bar rep_data=\u0026#39;./DATA/ANUSPLIN/\u0026#39; fic=rep_data+\u0026#39;ANUSPLIN_NLDAS_10km_CLIMATO_PrecTOT_1981_2010_12.nc\u0026#39; dset=Dataset(fic) precip=dset.variables[\u0026#39;PrecTOT\u0026#39;][:].squeeze() lon=dset.variables[\u0026#39;lon\u0026#39;][:].squeeze() lat=dset.variables[\u0026#39;lat\u0026#39;][:].squeeze() ## Interrogeons un peu chaque variable print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;dimension de precipitation = \u0026#39;,precip.shape) print(\u0026#39;Min de precip est = \u0026#39;, np.nanmin(precip)) print(\u0026#39;Max de precip est = \u0026#39;, np.nanmax(precip)) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;dimension de latitude = \u0026#39;,lat.shape) print(\u0026#39;Min de lat est = \u0026#39;, np.min(lat)) print(\u0026#39;Max de lat est = \u0026#39;, np.max(lat)) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;dimension de longitude = \u0026#39;,lon.shape) print(\u0026#39;Min de lon est = \u0026#39;, np.min(lon)) print(\u0026#39;Max de lon est = \u0026#39;, np.max(lon)) print(\u0026#39;-----------------------------------------\u0026#39;) ----------------------------------------- dimension de precipitation = (1068, 420) Min de precip est = 2.2327309 Max de precip est = 809.1944 ----------------------------------------- ----------------------------------------- dimension de latitude = (1068, 420) Min de lat est = 25.125 Max de lat est = 60.041664 ----------------------------------------- ----------------------------------------- dimension de longitude = (1068, 420) Min de lon est = -140.95833 Max de lon est = -52.04167 -----------------------------------------  fig = plt.figure(figsize=(22,12)) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ## Choisissons une colormap cmap0=plt.cm.jet_r cmap0.set_under(\u0026#39;w\u0026#39;) ## on met en blanc les valeurs inferieures au min de clev cmap0.set_over(\u0026#39;darkblue\u0026#39;) ## bleu fonce pour les valeurs extremes de pluie mm = ax.pcolormesh(lon,\\ lat,\\ precip,\\ vmin=0,\\ vmax=150, \\ transform=ccrs.PlateCarree(),\\ cmap=cmap0 ) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); ax.gridlines() # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) fig.canvas.draw() # Standard 6,000 km scale bar. scale_bar(ax, (0.85, 0.2), 500 ,plot_kwargs = dict(linestyle=\u0026#39;dashed\u0026#39;, color=\u0026#39;black\u0026#39;)) cbar = plt.colorbar(mm, shrink=0.75, drawedges=\u0026#39;True\u0026#39;, ticks=np.arange(0, 150.1, 20),extend=\u0026#39;both\u0026#39;) cbar.ax.tick_params(labelsize=17) plt.xlabel(u\u0026#39;\\Precipitation (mm)\u0026#39;,size=\u0026#39;x-large\u0026#39;) string_title=u\u0026#39;Precipitation climatology ANUSPLIN: December 1981-2010\u0026#39; plt.title(string_title, size=\u0026#39;xx-large\u0026#39;) plt.savefig(\u0026#39;./figures/My_2Dlalon_plot.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1, dpi=150) plt.show() plt.close() 3 - Cartopy: example with overplot Sometimes we want to have many plots on the same picture.\n We will work the climatologies of the total monthly accumulation of precipitation (1981-2010) for all the months of the year.  We will produce the same previous operation but for each month using a loop for.\n## we import librairies that we need from netCDF4 import Dataset import matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np from carto import scale_bar month_name=[\u0026#34;Jan\u0026#34;,\u0026#34;Feb\u0026#34;,\u0026#34;Mar\u0026#34;,\u0026#34;Apr\u0026#34;,\u0026#34;May\u0026#34;,\u0026#34;June\u0026#34;,\u0026#34;Jul\u0026#34;,\u0026#34;Aug\u0026#34;,\u0026#34;Sep\u0026#34;,\u0026#34;Oct\u0026#34;,\u0026#34;Nov\u0026#34;,\u0026#34;Dec\u0026#34;] rep_data=\u0026#39;./DATA/ANUSPLIN/\u0026#39; fig=plt.figure(figsize=(12,15)) clevs=np.arange(5,150.1,5) ## Colormap we will use cmap0=plt.cm.jet_r cmap0.set_under(\u0026#39;w\u0026#39;) cmap0.set_over(\u0026#39;darkblue\u0026#39;) for imonth in np.arange(1,13): # loop over 12 months ax=fig.add_subplot(4,3,imonth, projection=ccrs.LambertConformal()) fic=rep_data+\u0026#39;ANUSPLIN_NLDAS_10km_CLIMATO_PrecTOT_1981_2010_\u0026#39;+\u0026#39;{:02d}\u0026#39;.format(imonth)+\u0026#39;.nc\u0026#39; dset=Dataset(fic) precip=dset.variables[\u0026#39;PrecTOT\u0026#39;][:].squeeze() lon=dset.variables[\u0026#39;lon\u0026#39;][:].squeeze() lat=dset.variables[\u0026#39;lat\u0026#39;][:].squeeze() ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) mm = ax.pcolormesh(lon,\\ lat,\\ precip,\\ vmin=0,\\ vmax=150, \\ transform=ccrs.PlateCarree(),\\ cmap=cmap0 ) plt.title(\u0026#39;Normale \u0026#39;+ month_name[imonth-1],fontsize=12) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); ax.gridlines(); # Standard 6,000 km scale bar. scale_bar(ax, (0.85, 0.2), 500 ,plot_kwargs = dict(linestyle=\u0026#39;dashed\u0026#39;, color=\u0026#39;black\u0026#39;)) plt.savefig(\u0026#39;./figures/My_2Dlalon_multipanel_plot.png\u0026#39;) plt.show() 4 - Cartopy: MODIS https://lance-modis.eosdis.nasa.gov/imagery/gallery/2012270-0926/Miriam.A2012270.2050.2km.jpg\nhttps://lance-modis.eosdis.nasa.gov/imagery/gallery/2012270-0926/Miriam.A2012270.2050.txt\nfig = plt.figure(figsize=(12, 20)) fname = \u0026#39;./figures/Miriam.A2012270.2050.1km.jpg\u0026#39; img_extent = (-120.67660000000001, -106.32104523100001, 13.2301484511245, 30.766899999999502) img = plt.imread(fname) ax = plt.axes(projection=ccrs.PlateCarree()) # set a margin around the data ax.set_xmargin(0.05) ax.set_ymargin(0.10) # ajout de l\u0026#39;image ax.imshow(img, origin=\u0026#39;upper\u0026#39;, extent=img_extent, transform=ccrs.PlateCarree()) ax.coastlines(resolution=\u0026#39;50m\u0026#39;, color=\u0026#39;black\u0026#39;, linewidth=1) # ajout d\u0026#39;une ville ax.plot(-117.1625, 32.715, \u0026#39;bo\u0026#39;, markersize=7, transform=ccrs.Geodetic()) ax.text(-117, 33, \u0026#39;San Diego\u0026#39;, transform=ccrs.Geodetic()) Text(-117, 33, 'San Diego')  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"e3391e1e386ab479d03a830a4007e130","permalink":"/courses/tutorial_python_netcdf/3-cartopy_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python_netcdf/3-cartopy_library/","section":"courses","summary":"The Cartopy python library allows you to analyze, process and plot georeferenced data with the help of Matplotlib.\nhttps://scitools.org.uk/cartopy/docs/latest/#\nTo install the library under the prompt of anaconda: conda install -c conda-forge cartopy\n1 - Example of drawing a 2D Netcdf field  We will work climatology (1981-2010) monthly minimum daily temperature At first we will import the Python libraries and open the Netcdf file  The Dataset class of the netCDF4 module is used to open and read Netcdf files.","tags":null,"title":"3 Cartopy","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, we will use the features of the Python xarray library to process and analyze Netcdf files.\nTo install the library under anaconda:\n$ conda install xarray\nHere is an example of structure of a Netcdf file under xarray:\nDataArray xarray.DataArray is xarray’s implementation of a labeled, multi-dimensional array. It has several key properties: | | | |\u0026ndash;|\u0026ndash;| | values| a numpy.ndarray holding the array’s values | | dims| dimension names for each axis (e.g., (\u0026lsquo;x\u0026rsquo;, \u0026lsquo;y\u0026rsquo;, \u0026lsquo;z\u0026rsquo;,\u0026lsquo;time\u0026rsquo;)) | | coords| a dict-like container of arrays (coordinates) that label each point (e.g., 1-dimensional arrays of numbers, datetime objects or strings) | | attrs| an OrderedDict to hold arbitrary metadata (attributes) |\nDataSet xarray.Dataset is xarray’s multi-dimensional equivalent of a DataFrame. It is a dict-like container of labeled arrays (DataArray objects) with aligned dimensions. It is designed as an in-memory representation of the data model from the netCDF file format.\nxarray.DataSet is a collection of DataArrays. Each NetCDF file contains a DataSet.\n1- Open a Netcdf file We will open and store the data of a Netcdf file in a Dataset.\nFirst we need to import librairies and create aliases.\nimport xarray as xr import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) from matplotlib import pyplot as plt %matplotlib inline plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8,5)  To import and store as dataset only one Netcdf file:  We will work with temperature fields from cera20c reanalysis.\nunique_dataDIR = \u0026#39;./DATA/CERA20C/cera20c_member0_TAS_197101_day.nc\u0026#39; TAS = xr.open_dataset(unique_dataDIR) TAS \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 31) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-01-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] ... t2m (time, latitude, longitude) float32 ... Attributes: CDI: Climate Data Interface version 1.6.9 (http://mpimet.mpg.de/... Conventions: CF-1.6 history: Thu Oct 25 14:29:40 2018: cdo daymean ./TAS/cera20c_member0... CDO: Climate Data Operators version 1.6.9 (http://mpimet.mpg.de/...   If we want to import several Netcdf files:  Here, we want to store all files\u0026rsquo; names starting with \u0026lsquo;cera20c_member0_TAS_\u0026rsquo; and located in \u0026lsquo;./data/cera20c/\u0026rsquo; path.\nmulti_dataDIR = \u0026#39;./DATA/CERA20C/cera20c_member0_TAS_*.nc\u0026#39; TAS2 = xr.open_mfdataset(multi_dataDIR) TAS2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; Attributes: CDI: Climate Data Interface version 1.6.9 (http://mpimet.mpg.de/... Conventions: CF-1.6 history: Thu Oct 25 14:29:40 2018: cdo daymean ./TAS/cera20c_member0... CDO: Climate Data Operators version 1.6.9 (http://mpimet.mpg.de/...   Combine Netcdf:  To combine variables and coordinates between multiple DataArray and/or Dataset objects, use merge(). It can merge a list of Dataset, DataArray or dictionaries of objects convertible to DataArray objects:\nmulti_dataDIR = \u0026#39;./DATA/CERA20C/cera20c_member0_TAS_*.nc\u0026#39; TAS2 = xr.open_mfdataset(multi_dataDIR) multi_dataDIR2 = \u0026#39;./DATA/CERA20C/cera20c_member0_SIC_*.nc\u0026#39; SIC2 = xr.open_mfdataset(multi_dataDIR2) DS_new = xr.merge([TAS2,SIC2]) DS_new \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt;  2- Exploring the data We can quickly explore our datasets by using some methods of the xarray library:\n DS.var DS.dims DS.coords DS.attrs  DS_new.var \u0026lt;bound method ImplementsDatasetReduce._reduce_method.\u0026lt;locals\u0026gt;.wrapped_func of \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt;\u0026gt;  DS_new.dims Frozen(SortedKeysDict({'longitude': 288, 'latitude': 145, 'time': 365, 'bnds': 2}))  DS_new.coords Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00  DS_new.attrs OrderedDict()  3- Basic operations with Xarray:  Select a date:  We can use .sel() method to select one timestamp from our Dataset.\nDS_date = DS_new.sel(time=\u0026#39;1971-01-01\u0026#39;) DS_date \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 1) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(1, 2), chunksize=(1, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(1, 145, 288), chunksize=(1, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(1, 145, 288), chunksize=(1, 145, 288)\u0026gt;   Select time range  We can select a time range with slicing\nDS_date_range = DS_new.sel(time=slice(\u0026#39;1971-06-01\u0026#39;, \u0026#39;1971-08-31\u0026#39;)) DS_date_range \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 92) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-06-01T10:30:00 ... 1971-08-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(92, 2), chunksize=(30, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(92, 145, 288), chunksize=(30, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(92, 145, 288), chunksize=(30, 145, 288)\u0026gt;   Export a dataset  We can export our dataset into dataframe and then use Pandas library to make analysis.\ndf = DS_date_range.to_dataframe() df.head()     time_bnds t2m siconc     (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-01 10:30:00\u0026rsquo;)) 1971-06-01 00:00:00 266.296 0.990234   (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-02 10:30:00\u0026rsquo;)) 1971-06-02 00:00:00 264.76 0.988525   (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-03 10:30:00\u0026rsquo;)) 1971-06-03 00:00:00 265.866 0.987502   (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-04 10:30:00\u0026rsquo;)) 1971-06-04 00:00:00 265.947 0.98738   (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-05 10:30:00\u0026rsquo;)) 1971-06-05 00:00:00 266.126 0.987152    df.describe()     t2m siconc     count 7.68384e+06 7.68384e+06   mean 267.002 0.141752   std 25.852 0.312001   min 192.562 0   25% 273.221 0   50% 285.734 0   75% 296.388 0   max 314.804 1     Time mean  Mean_array = DS_date_range.mean(dim=\u0026#39;time\u0026#39;) Mean_array.values \u0026lt;bound method Mapping.values of \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 Data variables: t2m (latitude, longitude) float32 dask.array\u0026lt;shape=(145, 288), chunksize=(145, 288)\u0026gt; siconc (latitude, longitude) float32 dask.array\u0026lt;shape=(145, 288), chunksize=(145, 288)\u0026gt;\u0026gt;   To save our results into csv:  Mean_array.t2m.to_dataframe().to_csv(\u0026#39;./DATA/CERA20C_T2m_mean.csv\u0026#39;)  Mean over all latitudes and longitudes grid points:  DS_date_range.t2m.mean(dim=(\u0026#39;latitude\u0026#39;, \u0026#39;longitude\u0026#39;)) \u0026lt;xarray.DataArray 't2m' (time: 92)\u0026gt; dask.array\u0026lt;shape=(92,), dtype=float32, chunksize=(30,)\u0026gt; Coordinates: * time (time) datetime64[ns] 1971-06-01T10:30:00 ... 1971-08-31T10:30:00  DS_date_range.t2m.mean(dim=(\u0026#39;latitude\u0026#39;, \u0026#39;longitude\u0026#39;)).plot() [\u0026lt;matplotlib.lines.Line2D at 0xabe86a0\u0026gt;]   To save into csv:  DS_date_range.t2m.mean(dim=(\u0026#39;time\u0026#39;, \u0026#39;longitude\u0026#39;)).to_dataframe().to_csv(\u0026#39;./DATA/CERA20C_T2m_2Dmean.csv\u0026#39;)  Quick plot with Xarray  import cartopy.crs as ccrs fig=plt.figure(figsize=(10,10), frameon=True) ax = plt.axes(projection=ccrs.Orthographic(-80, 35)) Mean_array.t2m.plot.contourf(ax=ax, transform=ccrs.PlateCarree()); ax.set_global(); ax.coastlines();  Basic operations with Xarray:  In this example, we will mean DS_date_range over time and apply a substration to change units.\ncentigrade = DS_date_range.t2m.mean(dim=\u0026#39;time\u0026#39;) - 273.16 centigrade.values array([[ -1.2229004, -1.2229004, -1.2229004, ..., -1.2229004, -1.2229004, -1.2229004], [ -1.3348389, -1.3311157, -1.3274231, ..., -1.3488159, -1.3442383, -1.3395386], [ -1.6027222, -1.5914001, -1.5804138, ..., -1.6451721, -1.631012 , -1.6168518], ..., [-58.547928 , -58.5663 , -58.58455 , ..., -58.314026 , -58.391983 , -58.46997 ], [-59.556473 , -59.577957 , -59.59955 , ..., -59.478424 , -59.50441 , -59.53041 ], [-60.739105 , -60.739105 , -60.739105 , ..., -60.739105 , -60.739105 , -60.739105 ]], dtype=float32)  fig=plt.figure(figsize=(10,10), frameon=True) ax = plt.axes(projection=ccrs.Orthographic(-80, 35)) centigrade.plot.contourf(ax=ax, transform=ccrs.PlateCarree()); ax.set_global(); ax.coastlines();  Groupby() method:  groupdby() method with Xarray is very usefull to group our datasets by month, season, year\u0026hellip; and then apply function to compute indices.\nDS_new \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt;  # monthly mean: DS_month = DS_new.groupby(\u0026#39;time.month\u0026#39;).mean(\u0026#39;time\u0026#39;) DS_month #DS_month.to_dataframe() \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288, month: 12) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * month (month) int64 1 2 3 4 5 6 7 8 9 10 11 12 Data variables: t2m (month, latitude, longitude) float32 dask.array\u0026lt;shape=(12, 145, 288), chunksize=(1, 145, 288)\u0026gt; siconc (month, latitude, longitude) float32 dask.array\u0026lt;shape=(12, 145, 288), chunksize=(1, 145, 288)\u0026gt;  We can use this method to compute climatology and then anomalies.\nclimatology = DS_new.groupby(\u0026#39;time.month\u0026#39;).mean(\u0026#39;time\u0026#39;) anomalies = DS_new.groupby(\u0026#39;time.month\u0026#39;) - climatology anomalies \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 month (time) int64 1 1 1 1 1 1 1 1 1 1 ... 12 12 12 12 12 12 12 12 12 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt;  # seaon mean: DS_season = DS_new.groupby(\u0026#39;time.season\u0026#39;).mean(\u0026#39;time\u0026#39;) DS_season \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288, season: 4) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * season (season) object 'DJF' 'JJA' 'MAM' 'SON' Data variables: t2m (season, latitude, longitude) float32 dask.array\u0026lt;shape=(4, 145, 288), chunksize=(1, 145, 288)\u0026gt; siconc (season, latitude, longitude) float32 dask.array\u0026lt;shape=(4, 145, 288), chunksize=(1, 145, 288)\u0026gt;  # year mean: DS_year = DS_new.groupby(\u0026#39;time.year\u0026#39;).mean(\u0026#39;time\u0026#39;) DS_year \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288, year: 1) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * year (year) int64 1971 Data variables: t2m (year, latitude, longitude) float32 dask.array\u0026lt;shape=(1, 145, 288), chunksize=(1, 145, 288)\u0026gt; siconc (year, latitude, longitude) float32 dask.array\u0026lt;shape=(1, 145, 288), chunksize=(1, 145, 288)\u0026gt;   to select a specific season:  DS_winter = DS_season.sel(season=\u0026#39;DJF\u0026#39;) DS_winter \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 season \u0026lt;U3 'DJF' Data variables: t2m (latitude, longitude) float32 dask.array\u0026lt;shape=(145, 288), chunksize=(145, 288)\u0026gt; siconc (latitude, longitude) float32 dask.array\u0026lt;shape=(145, 288), chunksize=(145, 288)\u0026gt;  In the example below, we will group the xarray.DataArray data by season, calculate the average, apply a simple arrhythmic operation and plot the resulting fields for each season.\nDS_Season = DS_new.t2m.groupby(\u0026#39;time.season\u0026#39;).mean(\u0026#39;time\u0026#39;)- 273.15 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(9,5)) j = 0 for i, season in enumerate((\u0026#39;DJF\u0026#39;, \u0026#39;MAM\u0026#39;, \u0026#39;JJA\u0026#39;, \u0026#39;SON\u0026#39;)): if season ==\u0026#39;JJA\u0026#39;: j += 1 i = 0 elif season ==\u0026#39;SON\u0026#39;: i = 1 DS_Season.sel(season=season).plot.pcolormesh( ax=axes[i, j], vmin=-30, vmax=30, cmap=\u0026#39;Spectral_r\u0026#39;, add_colorbar=True, extend=\u0026#39;both\u0026#39;) for ax in axes.flat: ax.axes.get_xaxis().set_ticklabels([]) ax.axes.get_yaxis().set_ticklabels([]) ax.axes.axis(\u0026#39;tight\u0026#39;) plt.tight_layout() fig.suptitle(\u0026#39;Seasonal Surface Air Temperature\u0026#39;, fontsize=16, y=1.02) Text(0.5, 1.02, 'Seasonal Surface Air Temperature')  lat_bnd = [80, 50] lon_bnd = [250, 310] DS_Season = DS_new.sel(longitude=slice(*lon_bnd), latitude=slice(*lat_bnd),).siconc.groupby(\u0026#39;time.season\u0026#39;).mean(\u0026#39;time\u0026#39;) *100 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16,10)) j = 0 for i, season in enumerate((\u0026#39;DJF\u0026#39;, \u0026#39;MAM\u0026#39;, \u0026#39;JJA\u0026#39;, \u0026#39;SON\u0026#39;)): if season ==\u0026#39;JJA\u0026#39;: j += 1 i = 0 elif season ==\u0026#39;SON\u0026#39;: i = 1 DS_Season.sel(season=season).plot.pcolormesh( ax=axes[i, j], vmin=0, vmax=100, cmap=\u0026#39;Spectral_r\u0026#39;, add_colorbar=True, extend=\u0026#39;both\u0026#39;) for ax in axes.flat: ax.axes.get_xaxis().set_ticklabels([]) ax.axes.get_yaxis().set_ticklabels([]) ax.axes.axis(\u0026#39;tight\u0026#39;) plt.tight_layout() fig.suptitle(\u0026#39;Seasonal Sea Ice Concentration\u0026#39;, fontsize=16, y=1.02) Text(0.5, 1.02, 'Seasonal Sea Ice Concentration')  To save our result into Netcdf:\nDS_season = DS_new.groupby(\u0026#39;time.season\u0026#39;).mean(\u0026#39;time\u0026#39;) dataDIR = \u0026#39;./DATA/CERA20C_season.nc\u0026#39; DS_Season.to_netcdf(dataDIR) 4- Select grid points from Netcdf file using Xarray In the previous section we applied the .sel () method to work on the time dimension. This method can be used on spatial dimensions to extract points or study areas from our netcdf file.\nGridpoint: to extract the closest grid point of a latitude / longitude: lati = 45.5 loni = 269.2 data = DS_new.sel(longitude=loni , latitude=lati , method=\u0026#39;nearest\u0026#39;) data \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, time: 365) Coordinates: longitude float32 268.75 latitude float32 45.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time) float32 dask.array\u0026lt;shape=(365,), chunksize=(31,)\u0026gt; siconc (time) float32 dask.array\u0026lt;shape=(365,), chunksize=(31,)\u0026gt;  data[\u0026#39;t2m\u0026#39;] = data[\u0026#39;t2m\u0026#39;] - 273.15 data.t2m \u0026lt;xarray.DataArray 't2m' (time: 365)\u0026gt; dask.array\u0026lt;shape=(365,), dtype=float32, chunksize=(31,)\u0026gt; Coordinates: longitude float32 268.75 latitude float32 45.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00  We can convert our selection into a DataFrame and then use Pandas to analyse our results.\ndf = data.t2m.to_dataframe() fig = plt.figure(figsize=(16,8)) df[\u0026#39;t2m\u0026#39;].plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x13660400\u0026gt;  Gridpoints: to extract a list of points lats = [20.0,50.0,90.0] lons = [60.0,80.0,120.0] data = DS_new.sel(longitude=lons , latitude=lats , method=\u0026#39;nearest\u0026#39;) data[\u0026#39;t2m\u0026#39;] = data[\u0026#39;t2m\u0026#39;]-273.15 data \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 3, longitude: 3, time: 365) Coordinates: * longitude (longitude) float32 60.0 80.0 120.0 * latitude (latitude) float32 20.0 50.0 90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 3, 3), chunksize=(31, 3, 3)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 3, 3), chunksize=(31, 3, 3)\u0026gt;  fig = plt.figure(figsize=(16,8)) data.t2m.sel(longitude=60.0, latitude=[20.0,50.0,90.0]).plot.line(x=\u0026#39;time\u0026#39;) [\u0026lt;matplotlib.lines.Line2D at 0xdf31ac8\u0026gt;, \u0026lt;matplotlib.lines.Line2D at 0xdd0bf28\u0026gt;, \u0026lt;matplotlib.lines.Line2D at 0xdd0b4a8\u0026gt;]  To extract an area or subdomain delimited by latitude and longitude values: .slicing() lat_bnd = [80, 50] lon_bnd = [250, 310] area = DS_new.sel(longitude=slice(*lon_bnd), latitude=slice(*lat_bnd),) area \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 25, longitude: 49, time: 365) Coordinates: * longitude (longitude) float32 250.0 251.25 252.5 ... 307.5 308.75 310.0 * latitude (latitude) float32 80.0 78.75 77.5 76.25 ... 52.5 51.25 50.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 25, 49), chunksize=(31, 25, 49)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 25, 49), chunksize=(31, 25, 49)\u0026gt;  area.longitude.values array([250. , 251.25, 252.5 , 253.75, 255. , 256.25, 257.5 , 258.75, 260. , 261.25, 262.5 , 263.75, 265. , 266.25, 267.5 , 268.75, 270. , 271.25, 272.5 , 273.75, 275. , 276.25, 277.5 , 278.75, 280. , 281.25, 282.5 , 283.75, 285. , 286.25, 287.5 , 288.75, 290. , 291.25, 292.5 , 293.75, 295. , 296.25, 297.5 , 298.75, 300. , 301.25, 302.5 , 303.75, 305. , 306.25, 307.5 , 308.75, 310. ], dtype=float32)  To visualize our area::\nimport cartopy.crs as ccrs import cartopy.feature as cfeat def make_figure(): fig = plt.figure(figsize=(22, 12)) ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree()) # generate a basemap with country borders, oceans and coastlines ax.add_feature(cfeat.LAND) ax.add_feature(cfeat.OCEAN) ax.add_feature(cfeat.COASTLINE) ax.add_feature(cfeat.BORDERS, linestyle=\u0026#39;dotted\u0026#39;) return fig, ax make_figure(); _, ax = make_figure() # plot the temperature field area.t2m[0].plot() \u0026lt;matplotlib.collections.QuadMesh at 0xb2e9240\u0026gt;  To mask an area delimited by a Shapefile: To do this, we need to import two more librairies:\n Geopandas: conda install -c conda-forge geopandas osgeo: conda install -c conda-forge gdal  The next function will open a shapefile, read the polygons and make a mask from each grid points of our Netcdf inside the polygons.\nfrom osgeo import ogr import geopandas as gpd import numpy as np def get_mask(lons2d, lats2d, shp_path=\u0026#34;\u0026#34;, polygon_name=None): \u0026#34;\u0026#34;\u0026#34;Assumes that the shape file contains polygons in lat lon coordinates:param lons2d::param lats2d::param shp_path::rtype : np.ndarrayThe mask is 1 for the points inside of the polygons\u0026#34;\u0026#34;\u0026#34; ds = ogr.Open(shp_path) \u0026#34;\u0026#34;\u0026#34;:type : ogr.DataSource\u0026#34;\u0026#34;\u0026#34; xx = lons2d.copy() yy = lats2d # set longitudes to be from -180 to 180 xx[xx \u0026gt; 180] -= 360 mask = np.zeros(lons2d.shape, dtype=int) nx, ny = mask.shape pt = ogr.Geometry(ogr.wkbPoint) for i in range(ds.GetLayerCount()): layer = ds.GetLayer(i) \u0026#34;\u0026#34;\u0026#34;:type : ogr.Layer\u0026#34;\u0026#34;\u0026#34; for j in range(layer.GetFeatureCount()): feat = layer.GetFeature(j) \u0026#34;\u0026#34;\u0026#34;:type : ogr.Feature\u0026#34;\u0026#34;\u0026#34; # Select polygons by the name property if polygon_name is not None: if not feat.GetFieldAsString(\u0026#34;NAME\u0026#34;) == polygon_name: continue g = feat.GetGeometryRef() \u0026#34;\u0026#34;\u0026#34;:type : ogr.Geometry\u0026#34;\u0026#34;\u0026#34; assert isinstance(g, ogr.Geometry) for pi in range(nx): for pj in range(ny): pt.SetPoint_2D(0, float(xx[pi, pj]), float(yy[pi, pj])) mask[pi, pj] += int(g.Contains(pt)) return mask We first read the Netcdf file and store informations in a Xarray.dataset.\nds = xr.open_dataset(\u0026#39;./DATA/CERA20C/cera20c_member0_TAS_197101_day.nc\u0026#39;) We then need to extract latitudes and longitudes values and compute a 2D matrix.\nImp_Lats = ds[\u0026#39;latitude\u0026#39;].values Imp_Lons = ds[\u0026#39;longitude\u0026#39;].values lon2d, lat2d = np.meshgrid(Imp_Lons, Imp_Lats) ds_mean = ds.mean(\u0026#39;time\u0026#39;) - 273.15 ds_mean \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288) Coordinates: * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 Data variables: t2m (latitude, longitude) float32 -34.067184 ... -25.249802  We open the shape file with Geopandas library.\nshapes = gpd.read_file(\u0026#34;./DATA/Shapefiles/Countries_Final-polygon.shp\u0026#34;) list(shapes.columns.values) ['FIPS', 'ISO2', 'ISO3', 'UN', 'NAME', 'AREA', 'POP2005', 'REGION', 'SUBREGION', 'LON', 'LAT', 'layer', 'path', 'geometry']  shapes.head() shapes.loc[27, \u0026#39;geometry\u0026#39;] shapes.plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xd8b4b70\u0026gt;  We want in our study to extract information inside Mexico shapefile.\nmask=get_mask(lon2d,lat2d,shp_path=\u0026#34;./DATA/Shapefiles/Countries_Final-polygon.shp\u0026#34;, polygon_name=\u0026#39;Mexico\u0026#39;) np.max(mask) 1  We will convert our mask into numpy 2D array. We'll be later able to apply this matrix to mask our Netcdf file.\nnp.save(\u0026#39;DATA/Mexico.npy\u0026#39;,mask) # saving our mask in numpy.array We will mask our area using .where() method.\nds_mask = ds_mean.where(mask == 1) ds_mask.to_netcdf(\u0026#39;DATA/Mexico.nc\u0026#39;) # we want to save our shapefile mask in Netcdf format ds_mask \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288) Coordinates: * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 Data variables: t2m (latitude, longitude) float32 nan nan nan nan ... nan nan nan nan  np.max(ds_mask.t2m) \u0026lt;xarray.DataArray 't2m' ()\u0026gt; array(24.010315)  _, ax = make_figure() # plot the temperature field lat_bnd = [35, 0] lon_bnd = [240, 280] ds_mask.t2m.sel(longitude=slice(*lon_bnd), latitude=slice(*lat_bnd),).plot.pcolormesh(vmin=0, vmax=30, cmap=\u0026#39;Spectral\u0026#39;,add_colorbar=True, extend=\u0026#39;both\u0026#39;) \u0026lt;matplotlib.collections.QuadMesh at 0xdc65400\u0026gt;  5- Last example using Xarray: In this section, We will calculate the seasonal accumulation of the precipitation, extract a region, plot the domain and record our result in Netcdf:\n# Let\u0026#39;s open cera20c_enda_ep_PR_*.nc netcdf files  multi_dataDIR = \u0026#39;./DATA/CERA20C/cera20c_enda_ep_PR_*.nc\u0026#39; array = xr.open_mfdataset(multi_dataDIR) array.tp \u0026lt;xarray.DataArray 'tp' (time: 365, latitude: 181, longitude: 360)\u0026gt; dask.array\u0026lt;shape=(365, 181, 360), dtype=float32, chunksize=(31, 181, 360)\u0026gt; Coordinates: * longitude (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0 * latitude (latitude) float32 90.0 89.0 88.0 87.0 ... -88.0 -89.0 -90.0 * time (time) datetime64[ns] 1971-01-02T18:00:00 ... 1972-01-01T18:00:00 Attributes: units: m long_name: Total precipitation  All Netcdf files are stored in DataArray container, we can now group our Datasets by season, apply a simple sum() method over time and then change units from meters to mm.\narray_season = array.groupby(\u0026#39;time.season\u0026#39;).sum(\u0026#39;time\u0026#39;)*1000 array_season \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 181, longitude: 360, season: 4) Coordinates: * latitude (latitude) float32 90.0 89.0 88.0 87.0 ... -88.0 -89.0 -90.0 * season (season) object 'DJF' 'JJA' 'MAM' 'SON' * longitude (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0 Data variables: tp (season, latitude, longitude) float32 dask.array\u0026lt;shape=(4, 181, 360), chunksize=(1, 181, 360)\u0026gt;  We want to extract a specific domain delimited: - latitude boundaries: 50N to 70N - longitude boudaries: 250E to 310E\nWe finally want to extract winter season.\nlat_bnd = [70, 50] lon_bnd = [250, 310] subset_season_DJF = array_season.sel(season = \u0026#39;DJF\u0026#39;, longitude=slice(*lon_bnd), latitude=slice(*lat_bnd),) subset_season_DJF \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 21, longitude: 61) Coordinates: * latitude (latitude) float32 70.0 69.0 68.0 67.0 ... 53.0 52.0 51.0 50.0 season \u0026lt;U3 'DJF' * longitude (longitude) float32 250.0 251.0 252.0 253.0 ... 308.0 309.0 310.0 Data variables: tp (latitude, longitude) float32 dask.array\u0026lt;shape=(21, 61), chunksize=(21, 61)\u0026gt;  Let's save the Dataset to Netcdf.\ndataDIR = \u0026#39;./DATA/subset_season.nc\u0026#39; subset_season_DJF.to_netcdf(dataDIR) We can call our make_figure() function to quick plot our Dataset.\n_, ax = make_figure() # plot the temperature field subset_season_DJF.tp.plot.pcolormesh(vmin=0, vmax=200, cmap=\u0026#39;Spectral\u0026#39;,add_colorbar=True, extend=\u0026#39;both\u0026#39;) \u0026lt;matplotlib.collections.QuadMesh at 0x130aedd8\u0026gt;  array_season \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 181, longitude: 360, season: 4) Coordinates: * latitude (latitude) float32 90.0 89.0 88.0 87.0 ... -88.0 -89.0 -90.0 * season (season) object 'DJF' 'JJA' 'MAM' 'SON' * longitude (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0 Data variables: tp (season, latitude, longitude) float32 dask.array\u0026lt;shape=(4, 181, 360), chunksize=(1, 181, 360)\u0026gt;  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"987eed7f83bed8904091e698a9a5a7b7","permalink":"/courses/tutorial_python_netcdf/4-xarray_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python_netcdf/4-xarray_library/","section":"courses","summary":"In this tutorial, we will use the features of the Python xarray library to process and analyze Netcdf files.\nTo install the library under anaconda:\n$ conda install xarray\nHere is an example of structure of a Netcdf file under xarray:\nDataArray xarray.DataArray is xarray’s implementation of a labeled, multi-dimensional array. It has several key properties: | | | |\u0026ndash;|\u0026ndash;| | values| a numpy.ndarray holding the array’s values | | dims| dimension names for each axis (e.","tags":null,"title":"4 Xarray","type":"docs"},{"authors":[],"categories":[],"content":"From country shapefiles to Netcdf Mask In this tutorial, we will use shapefiles to create mask over specific countries.\nIn order to work with the whole globe, we will use gridded dataset ERA5 meteorological data.\nI will present a simple solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - regionmask: to mask a gridded file according to a shapefile - numpy: for simple array manipulations - geopandas: to open shapefiles - matplotlib: for plotting  1- Import librairies and create aliases. import xarray as xr import numpy as np import regionmask import geopandas as gpd import pandas as pd import matplotlib.pyplot as plt import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) %matplotlib inline Working over countries In this example we will focus on countries in Africa. We will use shapefiles developped in ESRI format from this link:\nhttps://community.esri.com/external-link.jspa?url=http%3A%2F%2Fwww.maplibrary.org%2Flibrary%2Fstacks%2FAfrica%2Findex.htm\nAfter downloaded the shapefile, we must load it using geopandas library:\nPATH_TO_SHAPEFILE = \u0026#39;./Countries/Africa_Countries.shp\u0026#39; countries = gpd.read_file(PATH_TO_SHAPEFILE) countries.head()     ID CODE COUNTRY geometry     0 1 ALG Algeria POLYGON ((-5.7636199999979 25.58624999999302, \u0026hellip;   1 2 ANG Angola POLYGON ((13.36632442474365 -8.32172966003418,\u0026hellip;    countries.shape[0] 762  my_list = list(countries[\u0026#39;CODE\u0026#39;]) my_list_unique = set(list(countries[\u0026#39;CODE\u0026#39;])) indexes = [my_list.index(x) for x in my_list_unique] Shapes are here a GeoDataFrame containing all polygons illustrating the countries boundaries.\nNow we can load the ERA5 gridded data. The parameter chunks is very important, it defines how big are the “pieces” of data moved from the disk to the memory. With this value the entire computation on a workstation with 32 GB takes a couple of minutes.\nWe will load all the temperature files for the year 2018 using Xarray library.\nmodel=\u0026#39;ERA5_T2m_1h\u0026#39; t_in = \u0026#39;J:/REANALYSES/ERA5/T2m_1h/\u0026#39; data = t_in + model + \u0026#39;_2018*_sfc.nc\u0026#39; ds = xr.open_mfdataset(data, chunks = {\u0026#39;time\u0026#39;: 10}) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 0.0 0.25 0.5 0.75 ... 359.25 359.5 359.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  The next function assign_coordswill convert the longitude from the 0-360 range to -180,180\nds = ds.assign_coords(longitude=(((ds.longitude + 180) % 360) - 180)).sortby(\u0026#39;longitude\u0026#39;) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  len(list(countries.geometry.values[i] for i in indexes)) len(list(countries.COUNTRY[indexes])) len(indexes) 52  a = range(0,countries.shape[0]) np.shape(a) (762,)  Our xarray Dataset contains a single variable (t2m) which is stored as a dask.array. This is the result of loading files with open_mfdataset.\nNow we will use regionmask module to create a gridded mask with the function regions_cls documented here: https://regionmask.readthedocs.io/en/stable/generated/regionmask.Regions_cls.html#regionmask.Regions_cls\nWith this function we will create an object able to mask ERA5 gridded data.\ncountries_mask_poly = regionmask.Regions_cls(name = \u0026#39;COUNTRY\u0026#39;, numbers = indexes, names = countries.COUNTRY[indexes], abbrevs = countries.COUNTRY[indexes], outlines = list(countries.geometry.values[i] for i in range(0,countries.shape[0]))) countries_mask_poly 52 'COUNTRY' Regions () Burkina Faso Senegal Botswana Liberia Chad Equatorial Guinea Djibouti Ghana Nigeria Sao Tome and Principe Swaziland Uganda Tanzania Comoros Guinea Algeria Niger Madagascar Burundi Cameroon Mali Zimbabwe Cote d`Ivoire Tunisia Sierra Leone Libya Rwanda Benin Malawi Gabon South Africa Western Sahara Zambia Central African Republic Togo Namibia Gambia Congo-Brazzaville Democratic Republic of Congo Morocco Eritrea Cape Verde Angola Ethiopia Lesotho Egypt Guinea-Bissau Kenya Mozambique Sudan Mauritania Somalia  Now we are ready to apply the mask on the gridded dataset xarray ERA5.\nWe select only the first timestep to speed up the process.\nThis step could take few minutes because of ERA5 resolution and grid : Dimensions: (latitude: 721, longitude: 1440)\nmask = countries_mask_poly.mask(ds.isel(time = 0), lat_name=\u0026#39;latitude\u0026#39;, lon_name=\u0026#39;longitude\u0026#39;) mask \u0026lt;xarray.DataArray 'region' (latitude: 721, longitude: 1440)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75  Mask can be saved (for example as a NetCDF) for a later use.\nmask.to_netcdf(\u0026#39;./mask_Africa_by_countries.nc\u0026#39;) We can use Panoply free software to plot our netcdf file.\nhttps://www.giss.nasa.gov/tools/panoply/\nHere's a quick visualisation using Matplotlib:\nplt.figure(figsize=(16,8)) ax = plt.axes() mask.plot(ax = ax) countries.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;, lw = 1) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xbdf0d68\u0026gt;  Let's now extract one specific country from our mask. We will for example extract informations only over Algeria. Remember, index for Algeria is: 0 .\nmask_algeria = mask.where(mask == 0 ) mask_algeria.to_netcdf(\u0026#39;./mask_Algeria.nc\u0026#39;) plt.figure(figsize=(16,8)) ax = plt.axes() mask_algeria.plot(ax = ax) countries.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;, lw = 1) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7528278\u0026gt;  2- Extract time-series We can now for each country aggregate the grid cells in the national borders. We will first develop two methods to mask our area. Then, we will extract the time series.\n- Method 1: I will focus on Algeria.\nID_COUNTRY = 0 print(countries.COUNTRY[ID_COUNTRY]) Algeria  As first step, I will save the latitude and longitude vectors because I will use it later. Then, I select the mask points where the value is equal to target value (the ID_COUNTRY code). In the numpy array sel_mask all the values are nan except for the selected ones.\nlat = mask.latitude.values lon = mask.longitude.values sel_mask = mask.where(mask == ID_COUNTRY).values sel_mask array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]])  To speed-up the process I want to crop the xarray Dataset selecting the smallest box containing the entire mask. To do this I store in id_lon and id_lat the coordinate points where the mask has at least a non-nan value.\nid_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))] id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))] id_lat array([37. , 36.75, 36.5 , 36.25, 36. , 35.75, 35.5 , 35.25, 35. , 34.75, 34.5 , 34.25, 34. , 33.75, 33.5 , 33.25, 33. , 32.75, 32.5 , 32.25, 32. , 31.75, 31.5 , 31.25, 31. , 30.75, 30.5 , 30.25, 30. , 29.75, 29.5 , 29.25, 29. , 28.75, 28.5 , 28.25, 28. , 27.75, 27.5 , 27.25, 27. , 26.75, 26.5 , 26.25, 26. , 25.75, 25.5 , 25.25, 25. , 24.75, 24.5 , 24.25, 24. , 23.75, 23.5 , 23.25, 23. , 22.75, 22.5 , 22.25, 22. , 21.75, 21.5 , 21.25, 21. , 20.75, 20.5 , 20.25, 20. , 19.75, 19.5 , 19.25, 19. ], dtype=float32)  The Xarray dataset is reduced selecting only the target year and the coordinates containing the target region. Then the dataset is load from the dask array using compute and then filtered using the mask.\nout_sel1 = ds.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == ID_COUNTRY) out_sel1 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 73, longitude: 82, time: 8760) Coordinates: * latitude (latitude) float64 37.0 36.75 36.5 36.25 ... 19.5 19.25 19.0 * longitude (longitude) float64 -8.5 -8.25 -8.0 -7.75 ... 11.25 11.5 11.75 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  - Method 2: We can directly use xarray library to apply netcdf mask with using .where() method and DataArray mask:\nout_sel2 = ds.where(mask == 0) out_sel2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  Quick visualisation, we will display the first step of our DataArray masked.\nFor out_sel2 array :\nplt.figure(figsize=(12,8)) ax = plt.axes() out_sel2.t2m.isel(time = 0).plot(ax = ax) countries.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xe73e550\u0026gt;  For out_sel1 array :\nplt.figure(figsize=(12,8)) ax = plt.axes() out_sel1.t2m.isel(time = 0).plot(ax = ax) countries.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xe8024a8\u0026gt;  Finally we can aggregate by the arithmetic mean using the groupby function to obtain a time-series of national average temperatures.\nx = out_sel1.groupby(\u0026#39;time\u0026#39;).mean() x \u0026lt;xarray.Dataset\u0026gt; Dimensions: (time: 8760) Coordinates: * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time) float32 285.01346 284.65833 283.95526 ... 282.03668 281.461  Then we plot the time-series…\nx.t2m.plot() [\u0026lt;matplotlib.lines.Line2D at 0xe9dc518\u0026gt;]   Let's resample our dataset by day and then compute a daily mean.  x = out_sel1.resample(time = \u0026#39;1D\u0026#39;).mean()-273.15 x \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 73, longitude: 82, time: 365) Coordinates: * longitude (longitude) float64 -8.5 -8.25 -8.0 -7.75 ... 11.25 11.5 11.75 * latitude (latitude) float64 37.0 36.75 36.5 36.25 ... 19.5 19.25 19.0 * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan  daily_mean = x.t2m.mean(dim=(\u0026#39;longitude\u0026#39;,\u0026#39;latitude\u0026#39;)) daily_mean \u0026lt;xarray.DataArray 't2m' (time: 365)\u0026gt; array([14.550091, 13.812102, 13.700798, ..., 10.975014, 11.182918, 10.956429], dtype=float32) Coordinates: * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31  daily_mean.plot() [\u0026lt;matplotlib.lines.Line2D at 0xe243f98\u0026gt;]  And we save it as a csv\ndaily_mean.to_pandas().to_csv(\u0026#39;average-temperature-algeria.csv\u0026#39;, header = [\u0026#39;t2m\u0026#39;]) 2- Extract time-series for one specific localisation In this example, we eant to extract time-series for Alger:\nWith: - longitude = 3.04 - latitude = 36.75\nlati = 36.75 loni = 3.04 data = out_sel1.sel(longitude=loni , latitude=lati , method=\u0026#39;nearest\u0026#39;) data.t2m \u0026lt;xarray.DataArray 't2m' (time: 8760)\u0026gt; array([289.91342, 289.63348, 289.2745 , ..., 285.62613, 284.9182 , 284.80624], dtype=float32) Coordinates: latitude float64 36.75 longitude float64 3.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Attributes: units: K long_name: 2 metre temperature  data[\u0026#39;t2m\u0026#39;] = data[\u0026#39;t2m\u0026#39;] - 273.15 df = data.t2m.to_dataframe() fig = plt.figure(figsize=(16,8)) df[\u0026#39;t2m\u0026#39;].plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x20773fd0\u0026gt;  Let's plot montlhy temperature distribution for Alger:\ndf[\u0026#39;month\u0026#39;] = df.index.strftime(\u0026#34;%b\u0026#34;) df.head()    time latitude longitude t2m month     2018-01-01 00:00:00 36.75 3 16.7634 Jan   2018-01-01 01:00:00 36.75 3 16.4835 Jan   2018-01-01 02:00:00 36.75 3 16.1245 Jan   2018-01-01 03:00:00 36.75 3 16.197 Jan   2018-01-01 04:00:00 36.75 3 16.0389 Jan    ax = plt.axes() sns.boxplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;t2m\u0026#34;, data=df, palette=\u0026#34;Set1\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show() ","date":1580841589,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580841589,"objectID":"1992a6a058bf3b12af01d19be757ed10","permalink":"/post/shapefiles_country/","publishdate":"2020-02-04T10:39:49-08:00","relpermalink":"/post/shapefiles_country/","section":"post","summary":"From country shapefiles to Netcdf Mask In this tutorial, we will use shapefiles to create mask over specific countries.\nIn order to work with the whole globe, we will use gridded dataset ERA5 meteorological data.\nI will present a simple solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - regionmask: to mask a gridded file according to a shapefile - numpy: for simple array manipulations - geopandas: to open shapefiles - matplotlib: for plotting  1- Import librairies and create aliases.","tags":[],"title":"Shapefiles_Country","type":"post"},{"authors":[],"categories":[],"content":"﻿\nFrom shapefiles to Netcdf Mask Many times we need to create Netcdf mask files over continents or maybe countries. In this tutorial, we will use shapefiles to create those masks.\nIn order to work with the whole globe, we will use gridded dataset ERA5 meteorological data.\nI will present a simple solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - regionmask: to mask a gridded file according to a shapefile - numpy: for simple array manipulations - geopandas: to open shapefiles - matplotlib: for plotting  1- First we need to import librairies and create aliases. import xarray as xr import numpy as np import regionmask import geopandas as gpd import pandas as pd import matplotlib.pyplot as plt import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) %matplotlib inline Working over continents In this example we will use the classification developed by ARCGIS.\nShapefiles are available on this website: https://www.arcgis.com/home/item.html?id=5cf4f223c4a642eb9aa7ae1216a04372\nAfter downloaded the shapefile, we must load it using geopandas library:\nPATH_TO_SHAPEFILE = \u0026#39;./continent_shapefile/continent.shp\u0026#39; continents = gpd.read_file(PATH_TO_SHAPEFILE) continents 0 Asia 1 North America 2 Europe 3 Africa 4 South America 5 Oceania 6 Australia 7 Antarctica Name: CONTINENT, dtype: GeoDataFrame\nShapes are here a GeoDataFrame containing all polygons illustrating the continent boundaries.\nNow we can load the ERA5 gridded data. The parameter chunks is very important, it defines how big are the “pieces” of data moved from the disk to the memory. With this value the entire computation on a workstation with 32 GB takes a couple of minutes.\nWe will load all the temperature files for the year 2018 using Xarray library.\nmodel=\u0026#39;ERA5_T2m_1h\u0026#39; t_in = \u0026#39;J:/REANALYSES/ERA5/T2m_1h/\u0026#39; data = t_in + model + \u0026#39;_2018*_sfc.nc\u0026#39; ds = xr.open_mfdataset(data, chunks = {\u0026#39;time\u0026#39;: 10}) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 0.0 0.25 0.5 0.75 ... 359.25 359.5 359.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  The next function assign_coordswill convert the longitude from the 0-360 range to -180,180\nds = ds.assign_coords(longitude=(((ds.longitude + 180) % 360) - 180)).sortby(\u0026#39;longitude\u0026#39;) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  Our xarray Dataset contains a single variable (t2m) which is stored as a dask.array. This is the result of loading files with open_mfdataset.\nNow we will use regionmask module to create a gridded mask with the function regions_cls documented here: https://regionmask.readthedocs.io/en/stable/generated/regionmask.Regions_cls.html#regionmask.Regions_cls\nWith this function we will create an object able to mask ERA5 gridded data.\ncontinents_mask_poly = regionmask.Regions_cls(name = \u0026#39;CONTINENT\u0026#39;, numbers = list(range(0,8)), names = list(continents.CONTINENT), abbrevs = list(continents.CONTINENT), outlines = list(continents.geometry.values[i] for i in range(0,8))) continents_mask_poly 8 'CONTINENT' Regions () Asia North America Europe Africa South America Oceania Australia Antarctica  Now we are ready to apply the mask on the gridded dataset xarray ERA5.\nWe select only the first timestep to speed up the process.\nThis step could take few minutes because of ERA5 resolution and grid : Dimensions: (latitude: 721, longitude: 1440)\nmask = continents_mask_poly.mask(ds.isel(time = 0), lat_name=\u0026#39;latitude\u0026#39;, lon_name=\u0026#39;longitude\u0026#39;) mask \u0026lt;xarray.DataArray 'region' (latitude: 721, longitude: 1440)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, 7., 7., ..., 7., 7., 7.], [nan, 7., 7., ..., 7., 7., 7.], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75  Mask can be saved (for example as a NetCDF) for a later use.\nmask.to_netcdf(\u0026#39;./mask_all_continents.nc\u0026#39;) A quick visualisation:\nplt.figure(figsize=(15,8)) ax = plt.axes() mask.plot(ax = ax) continents.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;, lw = 1) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x787df98\u0026gt;  2- Extract time-series We can now for each contient aggregate the grid cells.\nWe will first develop two methods to mask our area. Then, we will extract the time series.\n- Method 1: We will do this work over Africa.\nID_CONTINENT = 3 print(continents.CONTINENT[ID_CONTINENT]) Africa  As first step, I will save the latitude and longitude vectors because I will use it later. Then, I select the mask points where the value is equal to target value (the ID_CONTINENT code define before).\nIn the numpy array sel_mask all the values are nan except for the selected ones.\nlat = mask.latitude.values lon = mask.longitude.values sel_mask = mask.where(mask == ID_CONTINENT).values sel_mask array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]])  To speed-up the process I want to crop the xarray Dataset selecting the smallest box containing the entire mask. To do this I store in id_lon and id_lat the coordinate points where the mask has at least a non-nan value.\nid_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))] id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))] id_lat array([ 37.25, 37. , 36.75, 36.5 , 36.25, 36. , 35.75, 35.5 , 35.25, 35. , 34.75, 34.5 , 34.25, 34. , 33.75, 33.5 , 33.25, 33. , 32.75, 32.5 , 32.25, 32. , 31.75, 31.5 , 31.25, 31. , 30.75, 30.5 , 30.25, 30. , 29.75, 29.5 , 29.25, 29. , 28.75, 28.5 , 28.25, 28. , 27.75, 27.5 , 27.25, 27. , 26.75, 26.5 , 26.25, 26. , 25.75, 25.5 , 25.25, 25. , 24.75, 24.5 , 24.25, 24. , 23.75, 23.5 , 23.25, 23. , 22.75, 22.5 , 22.25, 22. , 21.75, 21.5 , 21.25, 21. , 20.75, 20.5 , 20.25, 20. , 19.75, 19.5 , 19.25, 19. , 18.75, 18.5 , 18.25, 18. , 17.75, 17.5 , 17.25, 17. , 16.75, 16.5 , 16.25, 16. , 15.75, 15.5 , 15.25, 15. , 14.75, 14.5 , 14.25, 14. , 13.75, 13.5 , 13.25, 13. , 12.75, 12.5 , 12.25, 12. , 11.75, 11.5 , 11.25, 11. , 10.75, 10.5 , 10.25, 10. , 9.75, 9.5 , 9.25, 9. , 8.75, 8.5 , 8.25, 8. , 7.75, 7.5 , 7.25, 7. , 6.75, 6.5 , 6.25, 6. , 5.75, 5.5 , 5.25, 5. , 4.75, 4.5 , 4.25, 4. , 3.75, 3.5 , 3.25, 3. , 2.75, 2.5 , 2.25, 2. , 1.75, 1.5 , 1.25, 1. , 0.75, 0.5 , 0.25, 0. , -0.25, -0.5 , -0.75, -1. , -1.25, -1.5 , -1.75, -2. , -2.25, -2.5 , -2.75, -3. , -3.25, -3.5 , -3.75, -4. , -4.25, -4.5 , -4.75, -5. , -5.25, -5.5 , -5.75, -6. , -6.25, -6.5 , -6.75, -7. , -7.25, -7.5 , -7.75, -8. , -8.25, -8.5 , -8.75, -9. , -9.25, -9.5 , -9.75, -10. , -10.25, -10.5 , -10.75, -11. , -11.25, -11.5 , -11.75, -12. , -12.25, -12.5 , -12.75, -13. , -13.25, -13.5 , -13.75, -14. , -14.25, -14.5 , -14.75, -15. , -15.25, -15.5 , -15.75, -16. , -16.25, -16.5 , -16.75, -17. , -17.25, -17.5 , -17.75, -18. , -18.25, -18.5 , -18.75, -19. , -19.25, -19.5 , -19.75, -20. , -20.25, -20.5 , -20.75, -21. , -21.25, -21.5 , -21.75, -22. , -22.25, -22.5 , -22.75, -23. , -23.25, -23.5 , -23.75, -24. , -24.25, -24.5 , -24.75, -25. , -25.25, -25.5 , -25.75, -26. , -26.25, -26.5 , -26.75, -27. , -27.25, -27.5 , -27.75, -28. , -28.25, -28.5 , -28.75, -29. , -29.25, -29.5 , -29.75, -30. , -30.25, -30.5 , -30.75, -31. , -31.25, -31.5 , -31.75, -32. , -32.25, -32.5 , -32.75, -33. , -33.25, -33.5 , -33.75, -34. , -34.25, -34.5 , -34.75], dtype=float32)  The Xarray dataset is reduced selecting only the target year and the coordinates containing the target region. Then the dataset is load from the dask array using compute and then filtered using the mask.\nout_sel1 = ds.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == ID_CONTINENT) out_sel1 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 289, longitude: 333, time: 8760) Coordinates: * latitude (latitude) float64 37.25 37.0 36.75 36.5 ... -34.25 -34.5 -34.75 * longitude (longitude) float64 -25.25 -25.0 -24.75 ... 57.25 57.5 57.75 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  - Method 2: We can directly use xarray library to apply netcdf mask with using .where() method and DataArray mask:\nmask \u0026lt;xarray.DataArray 'region' (latitude: 721, longitude: 1440)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, 7., 7., ..., 7., 7., 7.], [nan, 7., 7., ..., 7., 7., 7.], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75  out_sel2 = ds.where(mask == 3) out_sel1 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 289, longitude: 333, time: 8760) Coordinates: * latitude (latitude) float64 37.25 37.0 36.75 36.5 ... -34.25 -34.5 -34.75 * longitude (longitude) float64 -25.25 -25.0 -24.75 ... 57.25 57.5 57.75 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  out_sel2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  Quick visualisation, we will display the first step of our DataArray masked.\nFor out_sel1 array :\nplt.figure(figsize=(12,8)) ax = plt.axes() out_sel1.t2m.isel(time = 0).plot(ax = ax) continents.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xba03908\u0026gt;  For out_sel2 array :\nplt.figure(figsize=(15,8)) ax = plt.axes() out_sel2.t2m.isel(time = 0).plot(ax = ax) continents.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x759f9e8\u0026gt;  ! Finally we can resample our dataset by day and then compute a daily mean.  With out_sel1:\ndaily_mean = out_sel1.resample(time = \u0026#39;1D\u0026#39;).mean()-273.15 daily_mean \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 289, longitude: 333, time: 365) Coordinates: * latitude (latitude) float64 37.25 37.0 36.75 36.5 ... -34.25 -34.5 -34.75 * longitude (longitude) float64 -25.25 -25.0 -24.75 ... 57.25 57.5 57.75 * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan  To compute spatial mean over Africa:\ndaily_mean = daily_mean.t2m.mean(dim=(\u0026#39;longitude\u0026#39;,\u0026#39;latitude\u0026#39;)) daily_mean \u0026lt;xarray.DataArray 't2m' (time: 365)\u0026gt; array([20.868923, 20.559137, 20.618475, ..., 20.510223, 20.604036, 20.943094], dtype=float32) Coordinates: * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31  Then we plot the time-series…\ndaily_mean.plot() [\u0026lt;matplotlib.lines.Line2D at 0xc2d4cf8\u0026gt;]  And we save it as a csv.\ndaily_mean.to_pandas().to_csv(\u0026#39;average-daily-temperature.csv\u0026#39;, header = [\u0026#39;t2m\u0026#39;]) Just to see if we find same results with masking method 2. We will plot the same time serie with out_sel2 Xarray:\ndaily_mean2 = out_sel2.resample(time = \u0026#39;1D\u0026#39;).mean()-273.15 daily_mean2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 365) Coordinates: * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 721, 1440), chunksize=(1, 721, 1440)\u0026gt;  daily_mean2 = daily_mean2.t2m.mean(dim=(\u0026#39;longitude\u0026#39;,\u0026#39;latitude\u0026#39;)) daily_mean2 \u0026lt;xarray.DataArray 't2m' (time: 365)\u0026gt; dask.array\u0026lt;shape=(365,), dtype=float32, chunksize=(1,)\u0026gt; Coordinates: * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31  daily_mean2.plot() [\u0026lt;matplotlib.lines.Line2D at 0xc2d1f28\u0026gt;]  ","date":1580484028,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580484028,"objectID":"9f2f3f0d0b9b48b7115ba3816a5d3628","permalink":"/post/shapefile_netcdf/","publishdate":"2020-01-31T07:20:28-08:00","relpermalink":"/post/shapefile_netcdf/","section":"post","summary":"﻿\nFrom shapefiles to Netcdf Mask Many times we need to create Netcdf mask files over continents or maybe countries. In this tutorial, we will use shapefiles to create those masks.\nIn order to work with the whole globe, we will use gridded dataset ERA5 meteorological data.\nI will present a simple solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - regionmask: to mask a gridded file according to a shapefile - numpy: for simple array manipulations - geopandas: to open shapefiles - matplotlib: for plotting  1- First we need to import librairies and create aliases.","tags":[],"title":"Shapefile_netcdf","type":"post"},{"authors":[],"categories":[],"content":"Plot of a radial temperature chart In this post, we will plot a radial temperature chart using daily temperature from Environment Canada.\nThe objective of this product is to obtain an analysis of the daily evolution of the temperature and its positioning in relation to the norm and the historical records of the Julian day.\nIn this work, we will extract data from Environment and Climate Change Canada for the RIGAUD station in Quebec. Here is the information about this station:\nName: RIGAUD Username: 5252 Latitude: 45.5 degN Longitude: -74.37 degW Period covered: 1963 - today  A list of stations is available on this site: ftp://ftp.tor.ec.gc.ca/Pub/Get_More_Data_More_data/Repository%20of%20stations%20FR.csv\nThe data will be uploaded directly to Environment and Climate Change Canada's website http://climate.weather.gc.ca/\nThe data is in XML format. XML or eXtensible Markup Language is a generic markup language. To read this format, we will call the \u0026lsquo;xml.etree.ElementTree\u0026rsquo; library: https://docs.python.org/2/library/xml.etree.elementtree.html#module-xml.etree.ElementTree\nFirst, we import the necessary libraries:\n- matplotlib: module to plot our graph - datetime: python module for manipulating dates - wget: module to extract data on a url - pandas: module for working with data structures - os: \u0026quot;system\u0026quot; module to create, delete ... files from our environment  import matplotlib.pyplot as plt import numpy as np import matplotlib.ticker as ticker from datetime import date import os import wget import pandas as pd import xml.etree.ElementTree as ET import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) For example, we will extract the year 2000 daily records of the station RIGAUD (ID: 5252).\nid_stat = 5252 year = 2000 tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=2\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() stationsdata = root.findall(\u0026#39;.//stationdata\u0026#39;) -1 / unknown  Notre fichier XML a la structure suivant:\nOur XML file got this structure:\nWe need to target the maxtemp and mintemp tags for our work, so the stationdata[0] and station[1] fields.\nresultmax = [] resultmin = [] for stationdata in stationsdata: champs1=stationdata.find(\u0026#39;maxtemp\u0026#39;) resultmax.append(champs1.text) # we append data for each day in a year  data_max=np.array(resultmax,\u0026#34;float\u0026#34;) for stationdata in stationsdata: champs2=stationdata.find(\u0026#39;mintemp\u0026#39;) resultmin.append(champs2.text) # we append data for each day in a year  data_min=np.array(resultmin,\u0026#34;float\u0026#34;) We then have two python lists with daily minimum and maximum temperatures.\nprint(resultmax[0:5]) ['-1.0', '5.5', '2.0', '4.0', '-3.0']  We can do the same over the whole recording period of the station (1963-2019) by applying a for loop over the years.\nFor each year and each variable to be extracted we will increment a list in the variables min_array = [] and max_array = [].\nyi = 1963 yf = 2019 id_stat = 5252 station = \u0026#39;RIGAUD\u0026#39; max_array = [] min_array = [] for year in range(yi,yf+1): ### loop over yars  tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=2\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() stationsdata = root.findall(\u0026#39;.//stationdata\u0026#39;) resultmax = [] resultmin = [] for stationdata in stationsdata: champs1=stationdata.find(\u0026#39;maxtemp\u0026#39;) resultmax.append(champs1.text) # loop over days for tmax  data_max=np.array(resultmax,\u0026#34;float\u0026#34;) for stationdata in stationsdata: champs2=stationdata.find(\u0026#39;mintemp\u0026#39;) resultmin.append(champs2.text) # loop over days for tmin  data_min=np.array(resultmin,\u0026#34;float\u0026#34;) max_array.append(data_max) min_array.append(data_min) len(max_array) -1 / unknown 57  For each min_array and max_array fields, we get a list of lists. We will flatten thoses lists:\ndef flatten(input): new_list = [] for i in input: for j in i: new_list.append(j) return new_list min_array=flatten(min_array) max_array=flatten(max_array) len(min_array) 20819  So we now have two lists of 20454 days for each min_array and max_array fields.\nKnowing that the period extends from January 1, 1963 to December 31, 2018, we can add a temporal dimension to our dataframe with the datetime module of python.\nstart = date(1963, 1, 1) end = date(2019, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) tmin_dataset = pd.Series(min_array, index=rng) tmax_dataset = pd.Series(max_array, index=rng) tmax_dataset.head() 1963-01-01 -11.1 1963-01-02 -6.7 1963-01-03 -3.9 1963-01-04 -2.2 1963-01-05 -3.9 Freq: D, dtype: float64  One of the big advantages of the pandas dataframe is that we can very easily work with time series. Here we will calculate the daily climatologies of the tmin_dataset and tmax_dataset variables over the 1981-2010 normal period.\nAlso, with the pandas groupby tools, we can calculate the daily minimum and maximum of tmin_dataset and tmax_dataset over the full period of the recording.\nmintmin = [] maxtmax = [] climtmin = [] climtmax = [] clim_min_dataset=tmin_dataset[\u0026#39;1981\u0026#39;:\u0026#39;2010\u0026#39;] clim_max_dataset=tmax_dataset[\u0026#39;1981\u0026#39;:\u0026#39;2010\u0026#39;] climtmin = clim_min_dataset.groupby([clim_min_dataset.index.month, clim_min_dataset.index.day]).mean() climtmax = clim_max_dataset.groupby([clim_max_dataset.index.month, clim_max_dataset.index.day]).mean() mintmin = tmin_dataset.groupby([tmin_dataset.index.month, tmin_dataset.index.day]).min() maxtmax = tmax_dataset.groupby([tmax_dataset.index.month, tmax_dataset.index.day]).max() data_min_tmin=np.array(mintmin,\u0026#34;float\u0026#34;) data_max_tmax=np.array(maxtmax,\u0026#34;float\u0026#34;) data_clim_tmax=np.array(climtmax,\u0026#34;float\u0026#34;) data_clim_tmin=np.array(climtmin,\u0026#34;float\u0026#34;) print(len(data_clim_tmax)) print(len(maxtmax)) 366 366  We can know visualize our timeries with a radial chart.\nx = [] ndays=366 Azs=np.arange(0,ndays) angle = Azs * 2.0 * np.pi / ndays fig=plt.figure(figsize=(12,12)) ax = fig.add_subplot(111, polar=True) ax.plot([angle[0],angle[0]], [data_min[0],data_max[0]],\u0026#39;red\u0026#39;, alpha=1.0, linewidth=3.0, label=str(yf)) ax.plot([angle[0],angle[0]], [data_clim_tmin[0],data_clim_tmax[0]],\u0026#39;blue\u0026#39;, alpha=0.3, linewidth=3.0, label=\u0026#39;Climatology (1981-2010)\u0026#39;) ax.plot([angle[0],angle[0]], [data_min_tmin[0],data_max_tmax[0]],\u0026#39;grey\u0026#39;, alpha=0.4, linewidth=3.0, label=\u0026#39;Extreme (\u0026#39;+str(yi)+\u0026#39;-\u0026#39;+str(yf)+\u0026#39;)\u0026#39;) leg=plt.legend(bbox_to_anchor=(0.15, 1.0),fontsize=10) leg.get_frame().set_linewidth(0.0) leg.set_title(str(station)+\u0026#39;\u0026#39;+str(yf), prop={\u0026#39;size\u0026#39;: 10, \u0026#39;weight\u0026#39;: \u0026#39;heavy\u0026#39;}) ax.plot([angle,angle], [data_min,data_max],\u0026#39;red\u0026#39;, alpha=1.0, linewidth=3.0) ax.plot([angle,angle], [data_clim_tmin,data_clim_tmax],\u0026#39;blue\u0026#39;, alpha=0.3, linewidth=3.0) ax.plot([angle,angle], [data_min_tmin,data_max_tmax],\u0026#39;grey\u0026#39;, alpha=0.4, linewidth=3.0) ax.set_rmin(-40) ax.set_rmax(+40) ax.grid(True) ax.set_theta_direction(-1) ax.set_theta_offset(np.pi / 2) ax.set_rticks([-30,-20, -10, 0, 10, 20, 30]) # less radial ticks ax.set_rlabel_position(-45.) # get radial labels away from plotted line ax.set_yticklabels([\u0026#39;$-30^{\\circ}$\u0026#39;, \u0026#39;$-20^{\\circ}$\u0026#39;, \u0026#39;$-10^{\\circ}$\u0026#39;, \u0026#39;$0^{\\circ}$\u0026#39;, \u0026#39;$10^{\\circ}$\u0026#39;, \u0026#39;$20^{\\circ}$\u0026#39;, \u0026#39;$30^{\\circ}$\u0026#39; ], fontsize=10) # Set the major and minor tick locations ax.xaxis.set_major_locator(ticker.MultipleLocator(np.pi/6)) ax.xaxis.set_minor_locator(ticker.MultipleLocator(np.pi/12)) ax.grid(linewidth=1,color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;) # Turn off major tick labels ax.xaxis.set_major_formatter(ticker.NullFormatter()) ax.set_frame_on(False) # Set the minor tick width to 0 so you don\u0026#39;t see them for tick in ax.xaxis.get_minor_ticks(): tick.tick1line.set_markersize(0) tick.tick2line.set_markersize(0) tick.label1.set_horizontalalignment(\u0026#39;center\u0026#39;) # Set the names of your ticks, with blank spaces for the major ticks ax.set_xticklabels([\u0026#39;\u0026#39;,\u0026#39;Jan\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Feb\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Mar\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Apr\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;May\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Jun\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Jul\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Aug\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Sep\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Oct\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Nov\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Dec\u0026#39;],minor=True) plt.show() ","date":1575691203,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575691203,"objectID":"8fc9d803e13f0f530f5c49b99bb31c78","permalink":"/post/temperature_radial_chart/","publishdate":"2019-12-06T20:00:03-08:00","relpermalink":"/post/temperature_radial_chart/","section":"post","summary":"Plot of a radial temperature chart In this post, we will plot a radial temperature chart using daily temperature from Environment Canada.\nThe objective of this product is to obtain an analysis of the daily evolution of the temperature and its positioning in relation to the norm and the historical records of the Julian day.\nIn this work, we will extract data from Environment and Climate Change Canada for the RIGAUD station in Quebec.","tags":[],"title":"Temperature_Radial_Chart","type":"post"},{"authors":[],"categories":[],"content":"Extract daily temperature from Environment Canada using Python The objective of this product is to retrieve daily temperature data from the second generation homogenized dataset of Environment and Climate Change Canada developed by Vincent et al. 2012.\nAdjusted and homogenized Canadian climate dataset (DCCAH) were prepared to provide a better spatial and temporal representation of the climate trends in Canada.\nIn the Second Generation of Homogenized Temperature, new adjustments were applied to the daily minimum temperatures at synoptic stations (mainly airports) to address the bias due to the change in observing time in July 1961 (Vincent et al. 2009).\nDaily homogenized temperatures (minimum, maximum and mean) can be dowloaded on this link: ftp://ccrp.tor.ec.gc.ca/pub/EC_data/AHCCD_daily/Raw dataset can be downloaded here: http://climate.weather.gc.ca/historical_data/search_historic_data_f.html\nIn this post, we will work on a specific province in Canada (using filters). To do this, we will use Temperature_Stations.xls available on ftp site. This file provide us a list of all stations available.\nWe first need to import our librairies:\nimport pandas as pd import os from datetime import date import calendar import numpy as np import pathlib import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) from itertools import islice We will work with daily minimum temperature data only for the Northwest Territories of Canada.\nReferring to the document Temperature_Stations.xls, we see that the acronym for this province is: NWT.\ndataframe = pd.read_excel(\u0026#34;./Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.head()     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     0 BC AGASSIZ 1100120 1893 1 2018 9 49.25 -121.77 15 N   1 BC ATLIN 1200560 1905 8 2018 12 59.57 -133.7 674 N   2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   4 BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    Using this Dataframe we can define some input parameters to filter our data.\nvarin = \u0026#39;dn\u0026#39; # variable acronym  path = \u0026#39;Homog_daily_min_temp_v2018\u0026#39; # path to get data  varout = \u0026#39;Tasmin\u0026#39; province = \u0026#39;NWT\u0026#39; # Province to work with We can now filter our dataset.\nglobals()[\u0026#39;dataframe_\u0026#39;+province] = dataframe.loc[(dataframe[\u0026#34;Prov\u0026#34;] == province),:] globals()[\u0026#39;dataframe_\u0026#39;+province]     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     65 NWT CAPE PARRY 2200675 1957 5 2018 12 70.17 -124.72 87 N   66 NWT FORT GOOD HOPE 2201450 1944 8 2018 12 66.23 -128.65 82 Y   68 NWT FORT RELIANCE 2201903 1948 10 2018 12 62.72 -109.17 168 Y   69 NWT FORT SIMPSON 2202103 1895 11 2018 12 61.77 -121.23 169 Y   70 NWT FORT SMITH 2202201 1913 7 2018 12 60.02 -111.97 205 Y   71 NWT HAY RIVER 2202401 1893 9 2018 12 60.83 -115.78 166 Y   72 NWT INUVIK 2202578 1957 3 2018 12 68.3 -133.48 103 Y   73 NWT MOULD BAY 250M001 1948 5 2018 12 76.23 -119.35 2 Y   74 NWT NORMAN WELLS 2202801 1943 5 2018 12 65.28 -126.8 73 Y   75 NWT SACHS HARBOUR 2503648 1955 11 2018 8 72 -125.27 86 Y   76 NWT TUKTOYAKTUK 2203914 1957 6 2018 12 69.45 -133 18 Y   77 NWT YELLOWKNIFE 2204101 1942 7 2018 12 62.47 -114.43 206 Y    We found 13 stations for this province.\nWe want to work with YELLOWKNIFE station: stnid = 2204101.\nstnid = \u0026#39;2204101\u0026#39; f1 = open(\u0026#39;./\u0026#39;+path+\u0026#39;/\u0026#39;+str(varin)+str(stnid)+\u0026#39;.txt\u0026#39;, \u0026#39;r\u0026#39;) for line in islice(f1, 7): print(line) 2204101, YELLOWKNIFE , NWT, station joined , Homogenized daily minimum temperature , Deg Celcius, Updated to December 2018 2204101, YELLOWKNIFE , NWT, station jointe , Temperature quotidienne minimale homogeneisee, Deg Celcius, Mise a jour jusqu a decembre 2018 Year Mo Day 01 Day 02 Day 03 Day 04 Day 05 Day 06 Day 07 Day 08 Day 09 Day 10 Day 11 Day 12 Day 13 Day 14 Day 15 Day 16 Day 17 Day 18 Day 19 Day 20 Day 21 Day 22 Day 23 Day 24 Day 25 Day 26 Day 27 Day 28 Day 29 Day 30 Day 31 Annee Mo Jour 01 Jour 02 Jour 03 Jour 04 Jour 05 Jour 06 Jour 07 Jour 08 Jour 09 Jour 10 Jour 11 Jour 12 Jour 13 Jour 14 Jour 15 Jour 16 Jour 17 Jour 18 Jour 19 Jour 20 Jour 21 Jour 22 Jour 23 Jour 24 Jour 25 Jour 26 Jour 27 Jour 28 Jour 29 Jour 30 Jour 31 1942 7 12.2 13.3 11.7 10.0 9.4 11.7 14.4 14.4 13.3 12.2 10.6 11.7 11.1 12.8 14.4 15.0 13.9 14.4 13.9 14.4 13.9 13.3 11.7 12.2 11.1 12.2 13.3 10.0 10.6 11.7 8.3 1942 8 7.8 5.0 9.4 12.8 9.4 9.4 10.0 10.6 12.8 10.6 12.2 12.2 9.4 15.0 12.8 11.7 14.4 14.4 11.7 8.3 9.4 7.8 12.2 8.9 3.9 7.2 10.6 11.1 7.2 5.0 3.3 1942 9 3.9 6.1 6.1 8.3 9.4 11.1 11.1 6.7 6.1 9.4 5.0 9.4 7.2 4.4 5.0 4.4 3.3 1.1 2.2 -0.6 -2.2 0.0 -1.7 1.1 -4.4 -0.6 0.0 -0.6 1.7 1.1 -9999.9M  Cleaning data: We see that in our dataset we have for each line the daily data by year and by month according to the structure:\nThere is a 4 rows header. We will delete this header and also delete the alphanumeric characters, clean the missing values and create a dataframe.\nf1 = open(\u0026#39;./\u0026#39;+path+\u0026#39;/\u0026#39;+str(varin)+str(stnid)+\u0026#39;.txt\u0026#39;, \u0026#39;r\u0026#39;) f2 = open(\u0026#39;./tmp.txt\u0026#39;, \u0026#39;w\u0026#39;) for line in f1: for word in line: if word == \u0026#39;M\u0026#39;: f2.write(word.replace(\u0026#39;M\u0026#39;, \u0026#39;\u0026#39;)) elif word == \u0026#39;a\u0026#39;: f2.write(word.replace(\u0026#39;a\u0026#39;, \u0026#39;\u0026#39;)) else: f2.write(word) f1.close() f2.close() df_station = pd.read_csv(\u0026#39;./tmp.txt\u0026#39;, delim_whitespace=True, skiprows = range(0, 4)) df_station.head()     1942 7 12.2 13.3 11.7 10.0 9.4 11.7.1 14.4 14.4.1 13.3.1 12.2.1 10.6 11.7.2 11.1 12.8 14.4.2 15.0 13.9 14.4.3 13.9.1 14.4.4 13.9.2 13.3.2 11.7.3 12.2.2 11.1.1 12.2.3 13.3.3 10.0.1 10.6.1 11.7.4 8.3     0 1942 8 7.8 5 9.4 12.8 9.4 9.4 10 10.6 12.8 10.6 12.2 12.2 9.4 15 12.8 11.7 14.4 14.4 11.7 8.3 9.4 7.8 12.2 8.9 3.9 7.2 10.6 11.1 7.2 5 3.3   1 1942 9 3.9 6.1 6.1 8.3 9.4 11.1 11.1 6.7 6.1 9.4 5 9.4 7.2 4.4 5 4.4 3.3 1.1 2.2 -0.6 -2.2 0 -1.7 1.1 -4.4 -0.6 0 -0.6 1.7 1.1 -9999.9   2 1942 10 0 4.4 1.7 5 3.3 -1.1 4.4 2.2 3.9 1.7 5 0.6 -2.8 -1.1 2.8 -1.1 -1.1 2.8 0.6 -2.8 -2.8 -6.7 -10.6 -11.1 -7.2 -2.2 -1.7 -1.1 -3.9 -5 -8.3   3 1942 11 -11.7 -13.9 -13.3 -12.8 -12.8 -12.8 -12.2 -17.8 -12.2 -21.1 -16.1 -16.1 -17.8 -11.1 -22.2 -25 -25 -22.8 -17.8 -23.9 -26.7 -13.9 -18.3 -26.7 -32.2 -31.1 -34.4 -26.7 -30 -27.2 -9999.9   4 1942 12 -23.9 -20.6 -20.6 -25 -20.6 -20.6 -26.1 -27.8 -30 -31.1 -28.9 -23.9 -25 -31.1 -33.9 -37.2 -37.8 -38.9 -40.6 -39.4 -33.3 -35 -35.6 -33.9 -26.1 -31.7 -34.4 -28.9 -24.4 -33.9 -40.6    That's better but we still have some missing values. We will also change column names.\ndf_station.columns = [\u0026#39;Year\u0026#39;, \u0026#39;Month\u0026#39;, \u0026#39;D1\u0026#39;,\u0026#39;D2\u0026#39;,\u0026#39;D3\u0026#39;,\u0026#39;D4\u0026#39;,\u0026#39;D5\u0026#39;,\u0026#39;D6\u0026#39;,\u0026#39;D7\u0026#39;,\u0026#39;D8\u0026#39;,\u0026#39;D9\u0026#39;,\u0026#39;D10\u0026#39;, \u0026#39;D11\u0026#39;,\u0026#39;D12\u0026#39;,\u0026#39;D13\u0026#39;,\u0026#39;D14\u0026#39;,\u0026#39;D15\u0026#39;,\u0026#39;D16\u0026#39;,\u0026#39;D17\u0026#39;,\u0026#39;D18\u0026#39;,\u0026#39;D19\u0026#39;,\u0026#39;D20\u0026#39;, \u0026#39;D21\u0026#39;,\u0026#39;D22\u0026#39;,\u0026#39;D23\u0026#39;,\u0026#39;D24\u0026#39;,\u0026#39;D25\u0026#39;,\u0026#39;D26\u0026#39;,\u0026#39;D27\u0026#39;,\u0026#39;D28\u0026#39;,\u0026#39;D29\u0026#39;,\u0026#39;D30\u0026#39;,\u0026#39;D31\u0026#39;] os.remove(\u0026#34;./tmp.txt\u0026#34;) # nettoyage des valeurs manquantes  try: df_station = df_station.replace({\u0026#39;E\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: df_station = df_station.replace({\u0026#39;a\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: df_station = df_station.replace({\u0026#39;-9999.9\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: df_station = df_station.replace({-9999.9:\u0026#39;\u0026#39;}, regex=True) except: pass for col in df_station.columns[2:]: df_station[col] = pd.to_numeric(df_station[col], errors=\u0026#39;coerce\u0026#39;) df_station.head()     Year Month D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 D11 D12 D13 D14 D15 D16 D17 D18 D19 D20 D21 D22 D23 D24 D25 D26 D27 D28 D29 D30 D31     0 1942 8 7.8 5 9.4 12.8 9.4 9.4 10 10.6 12.8 10.6 12.2 12.2 9.4 15 12.8 11.7 14.4 14.4 11.7 8.3 9.4 7.8 12.2 8.9 3.9 7.2 10.6 11.1 7.2 5 3.3   1 1942 9 3.9 6.1 6.1 8.3 9.4 11.1 11.1 6.7 6.1 9.4 5 9.4 7.2 4.4 5 4.4 3.3 1.1 2.2 -0.6 -2.2 0 -1.7 1.1 -4.4 -0.6 0 -0.6 1.7 1.1 -9999.9   2 1942 10 0 4.4 1.7 5 3.3 -1.1 4.4 2.2 3.9 1.7 5 0.6 -2.8 -1.1 2.8 -1.1 -1.1 2.8 0.6 -2.8 -2.8 -6.7 -10.6 -11.1 -7.2 -2.2 -1.7 -1.1 -3.9 -5 -8.3   3 1942 11 -11.7 -13.9 -13.3 -12.8 -12.8 -12.8 -12.2 -17.8 -12.2 -21.1 -16.1 -16.1 -17.8 -11.1 -22.2 -25 -25 -22.8 -17.8 -23.9 -26.7 -13.9 -18.3 -26.7 -32.2 -31.1 -34.4 -26.7 -30 -27.2 -9999.9   4 1942 12 -23.9 -20.6 -20.6 -25 -20.6 -20.6 -26.1 -27.8 -30 -31.1 -28.9 -23.9 -25 -31.1 -33.9 -37.2 -37.8 -38.9 -40.6 -39.4 -33.3 -35 -35.6 -33.9 -26.1 -31.7 -34.4 -28.9 -24.4 -33.9 -40.6    We can now detect the minimum and maximum recording years and write the daily data on a single column.\nyearmin = df_station[\u0026#39;Year\u0026#39;].min() yearmax = df_station[\u0026#39;Year\u0026#39;].max() m_start = df_station[\u0026#39;Month\u0026#39;].loc[(df_station[\u0026#39;Year\u0026#39;] == yearmin)].min() m_end = df_station[\u0026#39;Month\u0026#39;].loc[(df_station[\u0026#39;Year\u0026#39;] == yearmax)].max() d_end = calendar.monthrange(yearmax, m_end)[1] tmp_tmin = [ ] for year in range(yearmin,yearmax+1): ### Loop over years for month in range(1,13): df = [] last_day = calendar.monthrange(year, month)[1] tmin = df_station.loc[(df_station[\u0026#34;Year\u0026#34;] == year) \u0026amp; (df_station[\u0026#34;Month\u0026#34;] == month)].iloc[:,2:last_day+2].values if len(tmin) == 0: a = np.empty((calendar.monthrange(year,month)[1])) a[:] = np.nan df=pd.DataFrame(a) else: df=pd.DataFrame(tmin.T) start = date(year, month, 1) end = date(year, month, last_day) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) df[\u0026#39;datetime\u0026#39;] = rng df.index = df[\u0026#39;datetime\u0026#39;] tmp_tmin.append(df) tmp_tmin = pd.concat(tmp_tmin) df = pd.DataFrame({\u0026#39;datetime\u0026#39;: tmp_tmin[\u0026#39;datetime\u0026#39;], \u0026#39;Var\u0026#39;: tmp_tmin.iloc[:,0]}, columns = [\u0026#39;datetime\u0026#39;,\u0026#39;Tmin\u0026#39;]) df.index = df[\u0026#39;datetime\u0026#39;] tmp_tmin = tmp_tmin.drop([\u0026#34;datetime\u0026#34;], axis=1) tmp_tmin.tail()    datetime 0     2018-12-27 00:00:00 -32.6   2018-12-28 00:00:00 -33.3   2018-12-29 00:00:00 -27.7   2018-12-30 00:00:00 -35.2   2018-12-31 00:00:00 -33.8    visualization: Quick visualization of the monthly average temperatures for the month of January. We will group the data by month and calculate the average.\nimport matplotlib.pylab as plt import datetime month_tmin = tmp_tmin.resample(\u0026#39;M\u0026#39;).mean() month_tmin.tail()    datetime 0     2018-08-31 00:00:00 8.9871   2018-09-30 00:00:00 -0.673333   2018-10-31 00:00:00 -4.99032   2018-11-30 00:00:00 -15.5233   2018-12-31 00:00:00 -21.8129    tmin_janvier = month_tmin[month_tmin.index.month==1] tmin_janvier.head()    datetime 0     1942-01-31 00:00:00 nan   1943-01-31 00:00:00 -32.6871   1944-01-31 00:00:00 -25.471   1945-01-31 00:00:00 -28.6355   1946-01-31 00:00:00 -33.4032    plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[10,6] plt.plot(tmin_janvier.index, tmin_janvier[:], label=\u0026#39;Tmin Station\u0026#39;, linewidth=2, c=\u0026#39;r\u0026#39;) plt.title(\u0026#39;Monthly mean of daily minimum temperature: January from \u0026#39; + datetime.date(yearmin, 1, 1).strftime(\u0026#39;%Y\u0026#39;)+ \u0026#39;et \u0026#39; + datetime.date(yearmax, 1, 1).strftime(\u0026#39;%Y\u0026#39;), fontsize=15, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize=15, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.ylabel(\u0026#39;°C\u0026#39;, fontsize=15, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.show() Final code The following code retrieves all stations for a specific province but for a common period.\nFor example, we wish to extract all daily temperature data for the province of the Northwest Territories but only for the common period 1989-2018.\nWe wish to have one file per station.\nimport pandas as pd import os from datetime import date import calendar import numpy as np import pathlib ################################################ # varin = \u0026#39;dn\u0026#39; path = \u0026#39;Homog_daily_min_temp_v2018\u0026#39; varout = \u0026#39;Tasmoy\u0026#39; province = \u0026#39;NWT\u0026#39; yearmin = 1989 yearmax = 2018 ############################################################################### dataframe = pd.read_excel(\u0026#34;./Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) globals()[\u0026#39;dataframe_\u0026#39;+province] = dataframe.loc[(dataframe[\u0026#34;Prov\u0026#34;] == province) \u0026amp; (dataframe[\u0026#34;année déb.\u0026#34;] \u0026lt;= yearmin) \u0026amp; (dataframe[\u0026#34;année fin.\u0026#34;] \u0026gt;= yearmax),:] names = [] for i, row in globals()[\u0026#39;dataframe_\u0026#39;+province].iterrows(): stnid = row[\u0026#39;stnid\u0026#39;] f1 = open(\u0026#39;./\u0026#39;+path+\u0026#39;/\u0026#39;+str(varin)+str(stnid)+\u0026#39;.txt\u0026#39;, \u0026#39;r\u0026#39;) f2 = open(\u0026#39;./tmp.txt\u0026#39;, \u0026#39;w\u0026#39;) for line in f1: for word in line: if word == \u0026#39;M\u0026#39;: f2.write(word.replace(\u0026#39;M\u0026#39;, \u0026#39;\u0026#39;)) elif word == \u0026#39;a\u0026#39;: f2.write(word.replace(\u0026#39;a\u0026#39;, \u0026#39;\u0026#39;)) else: f2.write(word) f1.close() f2.close() station = pd.read_csv(\u0026#39;./tmp.txt\u0026#39;, delim_whitespace=True, skiprows = range(0, 4)) station.columns = [\u0026#39;Annee\u0026#39;, \u0026#39;Mois\u0026#39;, \u0026#39;D1\u0026#39;,\u0026#39;D2\u0026#39;,\u0026#39;D3\u0026#39;,\u0026#39;D4\u0026#39;,\u0026#39;D5\u0026#39;,\u0026#39;D6\u0026#39;,\u0026#39;D7\u0026#39;,\u0026#39;D8\u0026#39;,\u0026#39;D9\u0026#39;,\u0026#39;D10\u0026#39;, \u0026#39;D11\u0026#39;,\u0026#39;D12\u0026#39;,\u0026#39;D13\u0026#39;,\u0026#39;D14\u0026#39;,\u0026#39;D15\u0026#39;,\u0026#39;D16\u0026#39;,\u0026#39;D17\u0026#39;,\u0026#39;D18\u0026#39;,\u0026#39;D19\u0026#39;,\u0026#39;D20\u0026#39;, \u0026#39;D21\u0026#39;,\u0026#39;D22\u0026#39;,\u0026#39;D23\u0026#39;,\u0026#39;D24\u0026#39;,\u0026#39;D25\u0026#39;,\u0026#39;D26\u0026#39;,\u0026#39;D27\u0026#39;,\u0026#39;D28\u0026#39;,\u0026#39;D29\u0026#39;,\u0026#39;D30\u0026#39;,\u0026#39;D31\u0026#39;] os.remove(\u0026#34;./tmp.txt\u0026#34;) try: station = station.replace({\u0026#39;E\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: station = station.replace({\u0026#39;a\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: station = station.replace({\u0026#39;-9999.9\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: station = station.replace({-9999.9:\u0026#39;\u0026#39;}, regex=True) except: pass for col in station.columns[2:]: station[col] = pd.to_numeric(station[col], errors=\u0026#39;coerce\u0026#39;) m_start = station[\u0026#39;Mois\u0026#39;].loc[(station[\u0026#39;Annee\u0026#39;] == yearmin)].min() m_end = station[\u0026#39;Mois\u0026#39;].loc[(station[\u0026#39;Annee\u0026#39;] == yearmax)].max() d_end = calendar.monthrange(yearmax, m_end)[1] tmp_tmin = [ ] for year in range(yearmin,yearmax+1): ### Boucle sur les annees for month in range(1,13): df = [] last_day = calendar.monthrange(year, month)[1] tmin = station.loc[(station[\u0026#34;Annee\u0026#34;] == year) \u0026amp; (station[\u0026#34;Mois\u0026#34;] == month)].iloc[:,2:last_day+2].values if len(tmin) == 0: a = np.empty((calendar.monthrange(year,month)[1])) a[:] = np.nan df=pd.DataFrame(a) else: df=pd.DataFrame(tmin.T) start = date(year, month, 1) end = date(year, month, last_day) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) df[\u0026#39;datetime\u0026#39;] = rng df.index = df[\u0026#39;datetime\u0026#39;] tmp_tmin.append(df) tmp_tmin = pd.concat(tmp_tmin) df = pd.DataFrame({\u0026#39;datetime\u0026#39;: tmp_tmin[\u0026#39;datetime\u0026#39;], \u0026#39;Var\u0026#39;: tmp_tmin.iloc[:,0]}, columns = [\u0026#39;datetime\u0026#39;,\u0026#39;Tmin\u0026#39;]) df.index = df[\u0026#39;datetime\u0026#39;] tmp_tmin = tmp_tmin.drop([\u0026#34;datetime\u0026#34;], axis=1) name = row[\u0026#39;Nom de station\u0026#39;].replace(\u0026#39;\u0026#39;,\u0026#39;_\u0026#39;) name = name.replace(\u0026#34;\u0026#39;\u0026#34;,\u0026#39;\u0026#39;) names.append(name) mypath=\u0026#39;./Daily_data_by_Province/\u0026#39;+varout+\u0026#39;/\u0026#39; pathlib.Path(mypath).mkdir(parents=True, exist_ok=True) tmp_tmin.to_csv(mypath+name+\u0026#39;_daily_\u0026#39;+varout+\u0026#39;_\u0026#39;+str(yearmin)+\u0026#39;-\u0026#39;+str(yearmax)+\u0026#39;.csv\u0026#39;) latlon = pd.DataFrame({\u0026#39;Latitude\u0026#39;: globals()[\u0026#39;dataframe_\u0026#39;+province][\u0026#34;lat (deg)\u0026#34;], \u0026#39;Longitude\u0026#39;: globals()[\u0026#39;dataframe_\u0026#39;+province][\u0026#34;long (deg)\u0026#34;] }, columns = [\u0026#39;Latitude\u0026#39;,\u0026#39;Longitude\u0026#39;]) latlon.to_csv(\u0026#39;./Daily_data_by_Province/stations_latlon_\u0026#39;+province+\u0026#39;.csv\u0026#39;) names = pd.DataFrame(names) names.to_csv(\u0026#39;./Daily_data_by_Province/stations_noms_\u0026#39;+province+\u0026#39;.csv\u0026#39;) base_filename = \u0026#39;./Daily_data_by_Province/stations_noms_\u0026#39;+province+\u0026#39;.txt\u0026#39; names[0].to_csv(base_filename, sep=\u0026#39;\\t\u0026#39;, index = False) ","date":1575690639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575690639,"objectID":"c7175df8435a025284248b2e831c63ce","permalink":"/post/eccc_temp/","publishdate":"2019-12-06T19:50:39-08:00","relpermalink":"/post/eccc_temp/","section":"post","summary":"Extract daily temperature from Environment Canada using Python The objective of this product is to retrieve daily temperature data from the second generation homogenized dataset of Environment and Climate Change Canada developed by Vincent et al. 2012.\nAdjusted and homogenized Canadian climate dataset (DCCAH) were prepared to provide a better spatial and temporal representation of the climate trends in Canada.\nIn the Second Generation of Homogenized Temperature, new adjustments were applied to the daily minimum temperatures at synoptic stations (mainly airports) to address the bias due to the change in observing time in July 1961 (Vincent et al.","tags":[],"title":"ECCC_Temp","type":"post"},{"authors":[],"categories":[],"content":"ANUSPLIN climatology using Cartopy Agriculture and Agri-Food Canada have produced daily precipitation, minimum and maximum temperature across Canada (south of 60°N) for climate related application purpose using thin-plate smoothing splines, as implemented in the ANUSPLIN climate modeling software (Hutchinson et al., 2009; McKenney et al., 2011).\nThe so-called ANUSPLIN data uses ground-based observations and generates daily gridded data from 1951 to 2017 on a Lambert conformal conic projection with 5’ arc minutes spacing (equivalent to a resolution of about 10 km). The key strength of this spatial interpolation method is its global dependence on all data, permitting robust and stable determination of spatially varying dependences on elevation. Hutchinson et al. (2009) have shown that while ANUSPLIN fall month’s absolute errors were remarkably small, those of winter months were quite large due to rather difficult observation and measurement conditions.\nData are available on:\nftp://ftp.nrcan.gc.ca/pub/outgoing/canada_daily_grids\nIn this post, we will see how to use Cartopy with Netcdf in order to display a nice climatology.\nWe will display a climatoly of ANUSPLIN from 1981 to 2010.\nThis dataset is only availabe with ascii grid format, I'll here show how to convert it into Netcdf using Python in an other post.\nThe Cartopy python library allows you to analyze, process and plot georeferenced data with the help of Matplotlib.\nhttps://scitools.org.uk/cartopy/docs/latest/#\nWe first import our librairies:\nfrom netCDF4 import Dataset, num2date import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import datetime import xarray as xr import pandas as pd filename=\u0026#39;K:/DATA/ANUSPLIN_10km/Netcdf/all_domaine/YEAR/Mean_tasmoy/ANUSPLIN_10km_MEAN_YEAR_Mean_tasmoy_1950-2017.nc\u0026#39; nc_fid=Dataset(filename,\u0026#39;r\u0026#39;) nc_fid.variables OrderedDict([('lon', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 lon(y, x) units: degrees_east long_name: Longitude CoordinateAxisType: Lon unlimited dimensions: current shape = (1068, 510) filling on, default _FillValue of 9.969209968386869e+36 used), ('lat', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 lat(y, x) units: degrees_north long_name: Latitude CoordinateAxisType: Lat unlimited dimensions: current shape = (1068, 510) filling on, default _FillValue of 9.969209968386869e+36 used), ('time', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float64 time(time) long_name: Time delta_t: unlimited dimensions: current shape = (68,) filling on, default _FillValue of 9.969209968386869e+36 used), ('Mean_tasmoy', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 Mean_tasmoy(time, y, x) long_name: Mean_tasmoy units: Celcius missing_value: -999.0 coordinates: lon lat unlimited dimensions: current shape = (68, 1068, 510) filling on, default _FillValue of 9.969209968386869e+36 used)])  data=nc_fid.variables[\u0026#39;Mean_tasmoy\u0026#39;][:].squeeze() lons=nc_fid.variables[\u0026#39;lon\u0026#39;][:].squeeze() lats=nc_fid.variables[\u0026#39;lat\u0026#39;][:].squeeze() time = nc_fid.variables[\u0026#39;time\u0026#39;] data.shape (68, 1068, 510)  data_m = data[32:62,:,:] clim_81_2010=data_m.mean(axis=0) clim_81_2010.shape (1068, 510)  We will add some cities to display on our map. We first need to create a dataframe with names of stations and localisations.\nnames=[\u0026#39;Whitehorse\u0026#39;,\u0026#39;Yellowknife\u0026#39;,\u0026#39;Iqaluit\u0026#39;,\u0026#39;Victoria\u0026#39;,\u0026#39;Edmonton\u0026#39;,\u0026#39;Regina\u0026#39;,\u0026#39;Winnipeg\u0026#39;,\u0026#39;Toronto\u0026#39;,\u0026#39;Ottawa\u0026#39;,\u0026#39;Quebec\u0026#39;,\u0026#39;Halifax\u0026#39;,\u0026#39;Charlottetown\u0026#39;,\u0026#39;St-John s\u0026#39;] latitudes=[60.721188,62.453972,63.748611,48.407326,53.631611,50.445210, 49.895077,43.651070,45.424721,46.829853,44.651070,46.238888,47.560539] longitudes=[-135.056839,-114.371788,-68.519722,-123.329773,-113.323975,-104.618896, -97.138451,-79.347015,-75.695000,-71.254028,-63.582687,-63.129166,-52.712830] df = pd.DataFrame(list(zip(names, latitudes, longitudes)), columns =[\u0026#39;Names\u0026#39;, \u0026#39;latitudes\u0026#39;, \u0026#39;longitudes\u0026#39;]) df     Names latitudes longitudes     0 Whitehorse 60.7212 -135.057   1 Yellowknife 62.454 -114.372   2 Iqaluit 63.7486 -68.5197   3 Victoria 48.4073 -123.33   4 Edmonton 53.6316 -113.324   5 Regina 50.4452 -104.619   6 Winnipeg 49.8951 -97.1385   7 Toronto 43.6511 -79.347   8 Ottawa 45.4247 -75.695   9 Quebec 46.8299 -71.254   10 Halifax 44.6511 -63.5827   11 Charlottetown 46.2389 -63.1292   12 St-John s 47.5605 -52.7128    import matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np import matplotlib as mpl from carto import scale_bar Y=np.array([[50,136,189],[102,194,165],[171,221,164],[230,245,152],\\ [255,255,191],[254,224,139],[253,174,97],[244,109,67],[213,62,79]])/255. colbar=mpl.colors.ListedColormap(Y) fig=plt.figure(figsize=(28,16), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-55,35,80]) #ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.plot(df[\u0026#39;longitudes\u0026#39;], df[\u0026#39;latitudes\u0026#39;], \u0026#39;ko\u0026#39;, ms=5, transform=ccrs.Geodetic()) for lg, lt, name in zip(df[\u0026#39;longitudes\u0026#39;], df[\u0026#39;latitudes\u0026#39;], df[\u0026#39;Names\u0026#39;]): if name in [u\u0026#39;Nazaré\u0026#39;, \u0026#39;Marinha Grande\u0026#39;]: ax.text(lg - .05, lt + .05, name, va=\u0026#39;center\u0026#39;, ha=\u0026#39;right\u0026#39;, transform=ccrs.Geodetic(), fontweight=\u0026#39;bold\u0026#39;) else: ax.text(lg + .5, lt + .5, name, va=\u0026#39;center\u0026#39;, ha=\u0026#39;left\u0026#39;, transform=ccrs.Geodetic(), fontweight=\u0026#39;bold\u0026#39;) mm = ax.pcolormesh(lons,\\ lats,\\ clim_81_2010,\\ vmin=-30,\\ vmax=15, \\ transform=ccrs.PlateCarree(),\\ cmap=colbar ) ax.gridlines() # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) # Standard 6,000 km scale bar. scale_bar(ax, (0.85, 0.05), 500 ,plot_kwargs = dict(linestyle=\u0026#39;dashed\u0026#39;, color=\u0026#39;black\u0026#39;)) cbar = plt.colorbar(mm, orientation=\u0026#39;horizontal\u0026#39;, shrink=0.5, drawedges=\u0026#39;True\u0026#39;, ticks=np.arange(-30, 15.1, 5),extend=\u0026#39;both\u0026#39;) cbar.set_label(u\u0026#39;\\nProjection = Lambert Conformal Conic \\nResolution: 5 Arcs-Minutes (10 km)\\nData provided by Natural Resources Canada / Created by Guillaume Dueymes\u0026#39;, size=\u0026#39;medium\u0026#39;) # Affichage de la légende de la barre de couleur cbar.ax.tick_params(labelsize=17) plt.xlabel(u\u0026#39;\\n\\n\\nTemperature / Température (°C)\u0026#39;,size=\u0026#39;x-large\u0026#39;) string_title=u\u0026#39;Climate normals of mean annual temperature (°C)\\nreference period 1981-2010\\n\u0026#39; plt.title(string_title, size=\u0026#39;xx-large\u0026#39;) plt.savefig(\u0026#39;./ANUSPLIN_NLDAS_10km_YEAR_CLIM_1981-2010.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() ","date":1575338174,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575338174,"objectID":"d5630635c7a6159d33fcc5d736b661de","permalink":"/post/cartopy_climatology/","publishdate":"2019-12-02T17:56:14-08:00","relpermalink":"/post/cartopy_climatology/","section":"post","summary":"ANUSPLIN climatology using Cartopy Agriculture and Agri-Food Canada have produced daily precipitation, minimum and maximum temperature across Canada (south of 60°N) for climate related application purpose using thin-plate smoothing splines, as implemented in the ANUSPLIN climate modeling software (Hutchinson et al., 2009; McKenney et al., 2011).\nThe so-called ANUSPLIN data uses ground-based observations and generates daily gridded data from 1951 to 2017 on a Lambert conformal conic projection with 5’ arc minutes spacing (equivalent to a resolution of about 10 km).","tags":[],"title":"Cartopy_climatology","type":"post"},{"authors":null,"categories":null,"content":"","date":1575244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575244800,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2019-12-02T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"Réseau Inondations InterSectoriel du Québec.","tags":["Inondations"],"title":"","type":"project"},{"authors":["Irina Sagurova","Antoinette Ludwig","Nicholas H. Ogden","Yann Pelcat","Guillaume Dueymes","Philippe Gachon"],"categories":["2"],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"f93e4e42d5544ce86272d249f286781a","permalink":"/publication/article9/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/article9/","section":"publication","summary":"Background: The geographic range of the tick Amblyomma americanum, a vector of diseases of public health significance such as ehrlichiosis, has expanded from the southeast of the United States northward during the 20th century. Recently, populations of this tick have been reported to be present close to the Canadian border in Michigan and New York states, but established populations are not known in Canada. Previous research suggests that changing temperature patterns with climate change may influence tick life cycles and permit northward range expansion of ticks in the northern hemisphere. Objectives: We aimed to estimate minimal temperature conditions for survival of A. americanum populations at the northern edge of the tick's range and to investigate the possibility of range expansion of A. americanum into northern U.S. states and southern Canada in the coming decades. Methods: A simulation model of the tick A. americanum was used, via simulations using climate data from meteorological stations in the United States and Canada, to estimate minimal temperature conditions for survival of A. americanum populations at the northern edge of the tick's range. Results: The predicted geographic scope of temperature suitability [ ≥ 3,285 annual cumulative degree days (DD)  0 ° C ] included most of the central and eastern U.S. states east of longitude 110°W, which is consistent with current surveillance data for the presence of the tick in this region, as well as parts of southern Quebec and Ontario in Canada. Regional climate model output raises the possibility of northward range expansion into all provinces of Canada from Alberta to Newfoundland and Labrador during the coming decades, with the greatest northward range expansion (up to 1,000 km by the year 2100) occurring under the greenhouse gas (GHG) emissions of Representative Concentration Pathway (RCP) 8.5. Predicted northward range expansion was reduced by approximately half under the reduced GHG emissions of RCP4.5. Discussion: Our results raise the possibility of range expansion of A. americanum into northern U.S. states and southern Canada in the coming decades, and conclude that surveillance for this tick, and the diseases it transmits, would be prudent.","tags":[],"title":"Predicted Northward Expansion of the Geographic Range of the Tick Vector Amblyomma americanum in North America under Future Climate Conditions","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\"data.csv\") data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file and adding markup: mmark to your page front matter.\nTo render inline or block math, wrap your LaTeX math with $$...$$.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ renders as\n\\[\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}\\]\nExample inline math $$\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2$$ renders as \\(\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2\\) .\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$ renders as\n\\[f(k;p_0^*) = \\begin{cases} p_0^* \u0026 \\text{if }k=1, \\\\ 1-p_0^* \u0026 \\text {if }k=0.\\end{cases}\\]\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD; A--B; A--C; B--D; C--D; ``` renders as\ngraph TD; A--B; A--C; B--D; C--D; An example sequence diagram:\n```mermaid sequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good! ``` renders as\nsequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good! An example Gantt diagram:\n```mermaid gantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d ``` renders as\ngantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a Markdown extension for asides, also referred to as notices or hints. By prefixing a paragraph with A\u0026gt;, it will render as an aside. You can enable this feature by adding markup: mmark to your page front matter, or alternatively using the Alert shortcode.\nA\u0026gt; A Markdown aside is useful for displaying notices, hints, or definitions to your readers. renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.\n Did you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":null,"categories":null,"content":"Python includes several built-in container types, but the most common ones are lists and tuples, which we would see in this tutorial.\nThere are certain things you can do with all container types. These operations include indexing, slicing, adding, multiplying. In addition, Python has built-in functions like finding the length of a sequence, finding its largest and smallest elements\u0026hellip;\nEach element of a sequence is assigned by a number - its position or index. The first index is zero, the second index is one\u0026hellip;\nCreating a list is as simple as putting different comma-separated values between square brackets :\n list1 = [ a, b, c, d, e] list2 = [ 1, 2, 3, 4, 5]  Python is an object-oriented language, lists are associated with methods: object.method() Functions can be applied to lists.\n2.1 Create a list To create a list, we use comma-separated values between square brackets.\nmy_list = [1,2,3,4,5,6,7,8,\u0026#34;hello\u0026#34;,10.5] print(my_list) # fonction to print elements in list  [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5]  We can use range()function to generate a sequence of numbers over time.\nmy_list = list(range(10)) my_list [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  my_list = list(range(1981,2011,2)) my_list [1981, 1983, 1985, 1987, 1989, 1991, 1993, 1995, 1997, 1999, 2001, 2003, 2005, 2007, 2009]  When we work with Python object, dir() command always shows us the tasks we can do with this object: dir(my_list)\ndir(my_list) ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']  2.2 Accessing Values in Lists : To access values in a list, use the square brackets for slicing along with the index or indices to obtain value available at that index.\nThe first index is zero, the second index is one\u0026hellip; to read first element, we use index 0. Then to read last element, we use -1 index.\nIt is also possible to modify a value with its index.\nmy_list = [1,2,3,4,5,6,7,8,\u0026#34;hello\u0026#34;,10.5] my_list[:] # To use all elements [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5]  my_list[1] # to access an item from the list: here we access the second element.  2  my_list[2:4] # slicing: to access the elements between the 3rd position and the 4th [3, 4]  my_list[:4] # slicing: to access all elements up to index 4 or 4th position [1, 2, 3, 4]  my_list[3:] # slicing: to access all elements from index 3 [4, 5, 6, 7, 8, 'hello', 10.5]  my_list[-2] # to get the 2nd value from the end, with use negative indexes, we do not start from 0 anymore. 'hello'  my_list[1:-4] # we start from the index 1, with slicing, we stop at the 4th index from the end [2, 3, 4, 5, 6]  my_list[::2] # to extract the elements with an increment [1, 3, 5, 7, 'hello']  my_list[1::2] # to extract the elements with an increment [2, 4, 6, 8, 10.5]  2.3 Updating Lists https://docs.python.org/2/tutorial/datastructures.html#more-on-lists\n2.3.1 Add elements To addan element: we use .append()method\nmy_list = [1,2,3,4,5,6,7,8,\u0026#34;hello\u0026#34;,10.5] my_list [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5]  my_list.append(2) print(my_list) [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5, 2]  my_list [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5, 2]  2.3.2 Insert elements To insertan element to our list using index: insert()method\nmy_list.insert(5,\u0026#34;new\u0026#34;) print(my_list) [1, 2, 3, 4, 5, 'new', 6, 7, 8, 'hello', 10.5, 2]  2.3.3 Update elements To changean element in our list using index\nmy_list[0]=\u0026#34;a\u0026#34; print(my_list) ['a', 2, 3, 4, 5, 'new', 6, 7, 8, 'hello', 10.5, 2]  To changeseveral elements in our list: slicing\nmy_list[3:5]=[8,10,11,22] print(my_list) ['a', 2, 3, 8, 10, 11, 22, 'new', 6, 7, 8, 'hello', 10.5, 2]  2.3.4 Remove elements To removein our list: 2 methods\nmy_list[0:2]=[] # remove using slicing  print(my_list) [3, 8, 10, 11, 22, 'new', 6, 7, 8, 'hello', 10.5, 2]  my_list [3, 8, 10, 11, 22, 'new', 6, 7, 8, 'hello', 10.5, 2]  del my_list[3] # to remove using keywords print(my_list) [3, 8, 10, 22, 'new', 6, 7, 8, 'hello', 10.5, 2]  https://www.programiz.com/python-programming/keyword-list\n2.4 Some useful operations on lists  Concatenate two lists  my_list1=[1,2,3,4,5] my_list2=[6,7,8,9,10] my_list3=my_list1+my_list2 my_list3 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]   Duplicate a list  my_list1=[1,2,3,4,5] my_list1*3 [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]   Reverse elements in list  .reverse()method is used to reverse elements in list\nmy_list1=[1,2,3,4,5] print(my_list1) my_list1.reverse() print(my_list1) [1, 2, 3, 4, 5] [5, 4, 3, 2, 1]   Count elements in list  .count()method is used to return count of how many times obj occurs in list.\nmy_list2= [\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;c\u0026#34;] my_list2.count(\u0026#34;a\u0026#34;) 3   .index()method returns the lowest index in list that obj appears  my_list2=[\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;c\u0026#34;] my_list2.index(\u0026#34;c\u0026#34;) 4   function sum()  my_list1=[1,2,3,4,5] sum(my_list1) 15   function .len()gives the total length of the list.  len(my_list1) 5   .sort()function to sort objects of list  my_list = [90,3,8,4,1,10,25,99] my_list.sort() my_list [1, 3, 4, 8, 10, 25, 90, 99]  my_list = [90,3,8,4,1,10,25,99] my_list [90, 3, 8, 4, 1, 10, 25, 99]  my_list.sort() my_list.reverse() my_list [99, 90, 25, 10, 8, 4, 3, 1]   min()and max()functions to return the minimumand the maximumfrom list  my_list [99, 90, 25, 10, 8, 4, 3, 1]  min(my_list) 1  max(my_list) 99   example applying a loop forand print()function to return values from a list  list_month = [\u0026#34;jan\u0026#34;,\u0026#34;feb\u0026#34;,\u0026#34;mar\u0026#34;] for month in list_month: print(month) jan feb mar   enumerate()function to return values and index  list_month = [\u0026#34;jan\u0026#34;,\u0026#34;feb\u0026#34;,\u0026#34;mar\u0026#34;] for month in enumerate(list_month): print(month) (0, 'jan') (1, 'feb') (2, 'mar')   .split()method to transform string into list  my_string = \u0026#34;January-February-March\u0026#34; my_string.split(\u0026#34;-\u0026#34;) ['January', 'February', 'March']   .join()method to transform list into string  list1 = [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;] \u0026#34;:\u0026#34;.join(list1) 'January:February:March'  2.5 Loop over lists You can loop over the elements of a list like this:\nlist1 = [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;] for month in list1: print(month) January February March  If you want access to the index of each element within the body of a loop, use the built-in enumerate function:\nlist1 = [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;] for idx, month in enumerate(list1): print(\u0026#39;#%d: %s\u0026#39; % (idx + 1, month)) #1: January #2: February #3: March  2.6 List comprehensions When programming, frequently we want to transform one type of data into another. As a simple example, consider the following code that computes square numbers:\nnums = [0, 1, 2, 3, 4] squares = [] for x in nums: squares.append(x ** 2) print(squares) [0, 1, 4, 9, 16]  But, with You can make this code simpler using a list comprehension:\nnums = [0, 1, 2, 3, 4] squares = [x ** 2 for x in nums] print(squares) [0, 1, 4, 9, 16]  List comprehensions can also contain conditions:\nnums = [0, 1, 2, 3, 4] even_squares = [x ** 2 for x in nums if x % 2 == 0] print(even_squares) [0, 4, 16]  2.7 Tuples   1 tuple is a list, a set of stored values, but with the difference that a tuple can not be modified as in a list.\n  the interest of a tuple is what is stored in a tuple will never be modifiable\n  We write all the elements of a tuple by separating them with commas and all surrounded by parentheses: - my_tuple = (,,,,)\n  We will use a tuple to define some kinds of constants that are not intended to change\ntuple1=(1,2,3,4,5) tuple1 (1, 2, 3, 4, 5)  tuple1[0]=0 # we can\u0026#39;t change a tuple --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-60-c1b0272b0229\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 tuple1[0]=0 # we can't change a tuple TypeError: 'tuple' object does not support item assignment ----------------------------------------------------------------------------  tuple1[1] 2  - Exercice on lists:  1- Create a list named \u0026quot;my_notes\u0026quot; which contains the following numbers: 19,7,15,9,10,6,18,10,16,14,13,10,2,20,17,8,12,10,11,4 2- Calculate the overall average of this class of 20 students and put the result in a variable called \u0026quot;general_note\u0026quot; 3- Find the lowest score and the highest score 4- Sort the list of notes from largest to smallest 5- Replace note 2 by 6 6- Count the number of notes equal to 10 in the class 7- Count the number of notes equal to 10 in the class 8- Find the number of students with a grade\u0026gt; 10  1- Create a list named \u0026ldquo;my_notes\u0026rdquo; which contains the following numbers\nmes_notes=[19,7,15,9,10,6,18,10,16,14,13,10,2,20,17,8,12,10,11,4] mes_notes [19, 7, 15, 9, 10, 6, 18, 10, 16, 14, 13, 10, 2, 20, 17, 8, 12, 10, 11, 4]  2 - Calculate the overall average of this class of 20 students and put the result in a variable called \u0026ldquo;general_note\u0026rdquo;\nimport numpy moyenne_generale=numpy.mean(mes_notes) moyenne_generale 11.55  3- Find the lowest score and the highest score\nmin(mes_notes) 2  max(mes_notes) 20  4- Sort the list of notes from largest to smallest - method 1 :\nmes_notes.sort() mes_notes [2, 4, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]  mes_notes.reverse() mes_notes [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 10, 10, 10, 9, 8, 7, 6, 4, 2]   method 2: using .sort() method  help(mes_notes.sort) Help on built-in function sort: sort(*, key=None, reverse=False) method of builtins.list instance Stable sort *IN PLACE*.  mes_notes.sort(reverse=True) mes_notes [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 10, 10, 10, 9, 8, 7, 6, 4, 2]  5- Replace note 2 by 6\nmes_notes[19]=6 mes_notes [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 10, 10, 10, 9, 8, 7, 6, 4, 6]  6- Count the number of notes equal to 10 in the class? : method count()\nmes_notes.count(10) 4  7- Count the number of notes equal to 10 in the class\nmes_notes_array=numpy.asarray(mes_notes) mes_notes_array array([20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 10, 10, 10, 9, 8, 7, 6, 4, 6])  mes_notes_array[mes_notes_array\u0026gt;10] # affichage des notes supérieures à 10  array([20, 19, 18, 17, 16, 15, 14, 13, 12, 11])  mes_notes_array\u0026gt;10 # cette opération est un masque avec des booléens array([ True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False])  len(mes_notes_array[mes_notes_array\u0026gt;10]) 10  mes_notes_array[mes_notes_array\u0026gt;10] array([20, 19, 18, 17, 16, 15, 14, 13, 12, 11])  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ac42c0e67ba065a88d6a128f0baedba8","permalink":"/courses/tutorial_python/2-lists_in_python/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/2-lists_in_python/","section":"courses","summary":"Python includes several built-in container types, but the most common ones are lists and tuples, which we would see in this tutorial.\nThere are certain things you can do with all container types. These operations include indexing, slicing, adding, multiplying. In addition, Python has built-in functions like finding the length of a sequence, finding its largest and smallest elements\u0026hellip;\nEach element of a sequence is assigned by a number - its position or index.","tags":null,"title":"2 List in Python","type":"docs"},{"authors":null,"categories":null,"content":"3.1 Flow control with the conditional structure: If, else The flow control statements can be classified into Conditional Statements and Iteration Statements.\n  The Conditional Statements selects a particular set of statements for execution depending upon a specified condition. The most popular conditional control statement is ‘if else’, so let’s see how it works in python.:\n Syntax IF: if condition: indentation ---\u0026gt; action 1 elif: indentation ---\u0026gt; action 2 else: indentation ---\u0026gt; action 3    WARNING: python works in indentation   Multiple tasks can be assigned in conditional structures.\n  my_variable = 5 if (my_variable \u0026gt; 5 ): my_variable = my_variable + 1 my_variable 5  if (my_variable \u0026gt; 5 ): my_variable = my_variable + 1 else: my_variable = my_variable - 1 my_variable 4  if (my_variable \u0026gt; 5 ): my_variable = my_variable + 1 elif (my_variable == 5) : my_variable = my_variable * 10 else: my_variable = my_variable - 1 my_variable 3  3.2 Flow control: Loop for: A loop allows to repeat instructions according to your needs.\n Syntax FOR: items = [1,2,3] for i in items: indentation ---\u0026gt; print (i) # 1,2,3  Be careful once again to respect the indentation.\nmy_list=[1,2,3,4,5,6,7,8,9,10] for value in my_list: print(value*2) print(\u0026#34;---\u0026#34;) 2 --- 4 --- 6 --- 8 --- 10 --- 12 --- 14 --- 16 --- 18 --- 20 ---  for value in my_list: my_new_value=value*2 print(\u0026#34;The multiplication of 2 * %d= %d\u0026#34; % (value,my_new_value)) # to concatenate text and a variable #% d: python knows we\u0026#39;ll display an integer #% s: python knows we\u0026#39;ll display a string #% f: python knows we will display a float # we give a tuple to display The multiplication of 2 * 1 = 2 The multiplication of 2 * 2 = 4 The multiplication of 2 * 3 = 6 The multiplication of 2 * 4 = 8 The multiplication of 2 * 5 = 10 The multiplication of 2 * 6 = 12 The multiplication of 2 * 7 = 14 The multiplication of 2 * 8 = 16 The multiplication of 2 * 9 = 18 The multiplication of 2 * 10 = 20  3.3 Flow control: while() loop , same as if structure but repeated  Syntaxe WHILE: run = True while run: indentation ---\u0026gt; print('running') if \u0026lt;condition\u0026gt;: run = False  As long as the condition is met, the iteration in the loop continues. Be careful to respect the indentation.\nmy_list=[1,2,3,4,5,6,7,8,9,10] counter=0 while (counter \u0026lt; 10): print(\u0026#34;My counter = %d\u0026#34; % (counter)) print(\u0026#34;My value = %d\u0026#34; % (my_list[counter])) # in a tuple, we want the value from our list counter=counter+1 My counter = 0 My value = 1 My counter = 1 My value = 2 My counter = 2 My value = 3 My counter = 3 My value = 4 My counter = 4 My value = 5 My counter = 5 My value = 6 My counter = 6 My value = 7 My counter = 7 My value = 8 My counter = 8 My value = 9 My counter = 9 My value = 10  # we can combine several conditions in a while loop # we will for example extract only the even values # If modulo value = 0, even number counter=0 counter_true_result=0 while (counter \u0026lt; 10): if my_list[counter] % 2 == 0: print(\u0026#34;My counter = %d\u0026#34; % (counter)) print(\u0026#34;My value = %d\u0026#34; % (my_list[counter])) counter_true_result=counter_true_result+1 counter=counter+1 print(counter_true_result) My counter = 1 My value = 2 My counter = 3 My value = 4 My counter = 5 My value = 6 My counter = 7 My value = 8 My counter = 9 My value = 10 5  3.4 Loop range It's possible to create a loop with range() function:\nfor my_value in range(0,10): print(my_value*2) print(\u0026#34;---\u0026#34;) 0 --- 2 --- 4 --- 6 --- 8 --- 10 --- 12 --- 14 --- 16 --- 18 ---  3.5 To stop a loop: It's possible to stop a loop with break command.\nmy_list=[1,2,3,4,5,6,7,8,9,10] for i in my_list: if i \u0026gt; 5: print(\u0026#34;stop: the following values are greater than 5.\u0026#34;) break print(i) 1 2 3 4 5 stop: the following values are greater than 5.  # to import a library import numpy as np dir(np) The help()function provides help on a function of the library.\n - Example: help(numpy.mean)  help (np.median) # for help on a library function # we can distinguish between optional and mandatory arguments Help on function median in module numpy: median(a, axis=None, out=None, overwrite_input=False, keepdims=False) Compute the median along the specified axis. Returns the median of the array elements. Parameters ---------- a : array_like Input array or object that can be converted to an array. axis : {int, sequence of int, None}, optional Axis or axes along which the medians are computed. The default is to compute the median along a flattened version of the array. A sequence of axes is supported since version 1.9.0. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow use of memory of input array `a` for calculations. The input array will be modified by the call to `median`. This will save memory when you do not need to preserve the contents of the input array. Treat the input as undefined, but it will probably be fully or partially sorted. Default is False. If `overwrite_input` is ``True`` and `a` is not already an `ndarray`, an error will be raised. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `arr`. .. versionadded:: 1.9.0 Returns ------- median : ndarray A new array holding the result. If the input contains integers or floats smaller than ``float64``, then the output data-type is ``np.float64``. Otherwise, the data-type of the output is the same as that of the input. If `out` is specified, that array is returned instead. See Also -------- mean, percentile Notes ----- Given a vector ``V`` of length ``N``, the median of ``V`` is the middle value of a sorted copy of ``V``, ``V_sorted`` - i e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the two middle values of ``V_sorted`` when ``N`` is even. Examples -------- \u0026gt;\u0026gt;\u0026gt; a = np.array([[10, 7, 4], [3, 2, 1]]) \u0026gt;\u0026gt;\u0026gt; a array([[10, 7, 4], [ 3, 2, 1]]) \u0026gt;\u0026gt;\u0026gt; np.median(a) 3.5 \u0026gt;\u0026gt;\u0026gt; np.median(a, axis=0) array([ 6.5, 4.5, 2.5]) \u0026gt;\u0026gt;\u0026gt; np.median(a, axis=1) array([ 7., 2.]) \u0026gt;\u0026gt;\u0026gt; m = np.median(a, axis=0) \u0026gt;\u0026gt;\u0026gt; out = np.zeros_like(m) \u0026gt;\u0026gt;\u0026gt; np.median(a, axis=0, out=m) array([ 6.5, 4.5, 2.5]) \u0026gt;\u0026gt;\u0026gt; m array([ 6.5, 4.5, 2.5]) \u0026gt;\u0026gt;\u0026gt; b = a.copy() \u0026gt;\u0026gt;\u0026gt; np.median(b, axis=1, overwrite_input=True) array([ 7., 2.]) \u0026gt;\u0026gt;\u0026gt; assert not np.all(a==b) \u0026gt;\u0026gt;\u0026gt; b = a.copy() \u0026gt;\u0026gt;\u0026gt; np.median(b, axis=None, overwrite_input=True) 3.5 \u0026gt;\u0026gt;\u0026gt; assert not np.all(a==b)  - Exercise Objective: To manipulate a list containing the prices of 58 houses Creation of the list \u0026ldquo;price_of_58_houses\u0026rdquo;\nprice_of_58_houses=list(range(125000,700000,10000)) price_of_58_houses[0:20] [125000, 135000, 145000, 155000, 165000, 175000, 185000, 195000, 205000, 215000, 225000, 235000, 245000, 255000, 265000, 275000, 285000, 295000, 305000, 315000]  len(price_of_58_houses) 58 1- How many houses have a price greater than or equal to 300000 euros? 2- How many houses have a price between 250000 and 400000 euros? 3- How many houses have a price that is not higher than 600000 euros? 4- How many houses have a price lower than 150000 euros or more than 650000 euros?  Hint: Scroll through the list with a loop, use if condition statements, and a counter to count the true results.\n# 1.  nombre_maisons=0 for prix in price_of_58_houses: if prix \u0026gt;= 300000: nombre_maisons=nombre_maisons+1 print(\u0026#34;Number of house with price greater than or equal to 300000 euros : %d\u0026#34; % (nombre_maisons)) Number of house with price greater than or equal to 300000 euros : 40  # 2.  nombre_maisons=0 for prix in price_of_58_houses: if (prix \u0026gt;= 250000) and (prix \u0026lt;= 400000): compteur nombre_maisons=nombre_maisons+1 print(\u0026#34;The result is : %d\u0026#34; % (nombre_maisons)) The result is : 15  # 3. nombre_maisons=0 for prix in price_of_58_houses: if not(prix \u0026gt; 600000): nombre_maisons=nombre_maisons+1 print(\u0026#34;The result is : %d\u0026#34; % (nombre_maisons)) The result is : 48  # 4.  nombre_maisons=0 for prix in price_of_58_houses: if (prix \u0026lt; 150000) or (prix \u0026gt; 650000): nombre_maisons=nombre_maisons+1 print(\u0026#34;The result is : : %d\u0026#34; % (nombre_maisons)) The result is : 8\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"9d9c207528bfb3e6aaad7eb82af12d1a","permalink":"/courses/tutorial_python/3-flow_control/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/3-flow_control/","section":"courses","summary":"3.1 Flow control with the conditional structure: If, else The flow control statements can be classified into Conditional Statements and Iteration Statements.\n  The Conditional Statements selects a particular set of statements for execution depending upon a specified condition. The most popular conditional control statement is ‘if else’, so let’s see how it works in python.:\n Syntax IF: if condition: indentation ---\u0026gt; action 1 elif: indentation ---\u0026gt; action 2 else: indentation ---\u0026gt; action 3    WARNING: python works in indentation   Multiple tasks can be assigned in conditional structures.","tags":null,"title":"3 Flow control","type":"docs"},{"authors":null,"categories":null,"content":"4 Libraries and functions in Python 4.1 Introduction to librairies To facilitate the processing, manipulation and visualization of data, Python has many libraries.\nPython libraries allow you to import codes and functions that will make our analysis easier.\nHere are the most popular libraries:\n numpy: mathematics or scientific calculations: http://www.numpy.org/ pandas: data manipulation: http://pandas.pydata.org/ matplotlib: visualization of data: https://matplotlib.org/ scikit-learn: machine learning: https://scikit-learn.org Datetime: formatting dates: https: //docs.python.org/2/library/datetime.html  To import a library under Python, use the function: import + library_name\n# To import a library import numpy Every library imported under python has help.\nTo know what a library contains, we use the function: dir ()\n - Example: dir(numpy)  dir(numpy) The help () function provides help with a function in the library\n - Example: help(numpy.mean)  help(numpy.mean) Help on function mean in module numpy: mean(a, axis=None, dtype=None, out=None, keepdims=\u0026lt;no value\u0026gt;) Compute the arithmetic mean along the specified axis. Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. `float64` intermediate and return values are used for integer inputs. Parameters ---------- a : array_like Array containing numbers whose mean is desired. If `a` is not an array, a conversion is attempted. axis : None or int or tuple of ints, optional Axis or axes along which the means are computed. The default is to compute the mean of the flattened array. .. versionadded:: 1.7.0 If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before. dtype : data-type, optional Type to use in computing the mean. For integer inputs, the default is `float64`; for floating point inputs, it is the same as the input dtype. out : ndarray, optional Alternate output array in which to place the result. The default is ``None``; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See `doc.ufuncs` for details. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. If the default value is passed, then `keepdims` will not be passed through to the `mean` method of sub-classes of `ndarray`, however any non-default value will be. If the sub-class' method does not implement `keepdims` any exceptions will be raised. Returns ------- m : ndarray, see dtype parameter above If `out=None`, returns a new array containing the mean values, otherwise a reference to the output array is returned. See Also -------- average : Weighted average std, var, nanmean, nanstd, nanvar Notes ----- The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below). Specifying a higher-precision accumulator using the `dtype` keyword can alleviate this issue. By default, `float16` results are computed using `float32` intermediates for extra precision. Examples -------- \u0026gt;\u0026gt;\u0026gt; a = np.array([[1, 2], [3, 4]]) \u0026gt;\u0026gt;\u0026gt; np.mean(a) 2.5 \u0026gt;\u0026gt;\u0026gt; np.mean(a, axis=0) array([ 2., 3.]) \u0026gt;\u0026gt;\u0026gt; np.mean(a, axis=1) array([ 1.5, 3.5]) In single precision, `mean` can be inaccurate: \u0026gt;\u0026gt;\u0026gt; a = np.zeros((2, 512*512), dtype=np.float32) \u0026gt;\u0026gt;\u0026gt; a[0, :] = 1.0 \u0026gt;\u0026gt;\u0026gt; a[1, :] = 0.1 \u0026gt;\u0026gt;\u0026gt; np.mean(a) 0.54999924 Computing the mean in float64 is more accurate: \u0026gt;\u0026gt;\u0026gt; np.mean(a, dtype=np.float64) 0.55000000074505806  list_1=[1,2,3,4,5,6,7,8,9,10] list_1 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  numpy.mean(list_1) # name of library + name of function  5.5  numpy.max(list_1) 10  4.2 Built-in function Python gives you many built-in functions like print(), etc\nHere's a list: https://docs.python.org/3/library/functions.html\nSome examples: abs(-1) # Return the absolute value of a number  1  len([1,2,3]) # Return the length (the number of items) of an object.  3  max([1,3,2,6,99,1]) #Return max value from a list 99  round(1.2) # Return number rounded to ndigits precision  1  4.3 Create own functions. These functions are called user-defined functions.\n  a function makes our code more fluid and readable:\n  Syntax: to write a function, we start with the def() keyword\n def sum (a, b): # we define the parameters indentation ---\u0026gt; return a + b # return keyword to return result variable    It does not specify the type of return, it is dynamically resolved at the time of execution of the program.\n  Call of the function: result = sum (12,4) # result = 16\n  Python also supports keyword arguments.\n def sum (a, b): # we define the parameters return a + b # return keyword to return result variable result = sum (12, b = 4) ----------------------------- def sum (a, b = 4): # we define the parameters return a + b # return keyword to return result variable result = sum (12)  def sign(x): if x \u0026gt; 0: return \u0026#39;positive\u0026#39; elif x \u0026lt; 0: return \u0026#39;negative\u0026#39; else: return \u0026#39;zero\u0026#39; my_list_to_test = [0,2,-8,10,1,-6] for element in my_list_to_test: print(sign(element)) zero positive negative positive positive negative  We will often define functions to take optional keyword arguments, like this:\ndef hello(name, loud=False): if loud: print(\u0026#39;HELLO, %s\u0026#39; % name.upper()) else: print(\u0026#39;Hello, %s!\u0026#39; % name) hello(\u0026#39;Bob\u0026#39;) hello(\u0026#39;Fred\u0026#39;, loud=True) Hello, Bob! HELLO, FRED  4.4 Functions with *args  Special syntax in python that allows to manage a variable number of parameters when calling a function It's the * that counts args is a convention   args is a list of parameters containing the parameters of a function    Syntax * args:\n def print_ingredients (* args): for ingredients in args: print (ingredient) print_ingredients ( 'Tomatoes') print_ingredients ( 'Tomatoes' Banana) print_ingredients ( 'Tomatoes' Banana, apple)  def sum(*args): total = 0 for number in args: total += number print(total) sum(2,3) sum(2,3,10,90,23) 5 128  4.5 Functions with **kwargs  Like * args, but for keyword arguments It's the * that counts ** kwargs is a python dictionary containing the keys / values ​​of the parameters of a function it must always be present last in the list of parameters of a function  Syntax ** kwargs:\n def print_languages ​​(** args): for language, definition in kwargs.items (): print ('{} is {}'. format (language, definition)) print_languages ​​(Python = 'awesome') print_languages ​​(Python = 'awesome', Java = 'verbose')  def capitals(**kwargs): for country, capital in kwargs.items(): print(\u0026#34;The capital of {} in {}\u0026#34;.format(country, capital)) capitals(France = \u0026#39;Paris\u0026#39;, Germany=\u0026#39;Berlin\u0026#39;) The capital of France in Paris The capital of Germany in Berlin  def capitals(title, ending=\u0026#39;\u0026#39;, **kwargs): print(title) for country, capital in kwargs.items(): print(\u0026#34;The capital of {} in {}\u0026#34;.format(country, capital)) if ending: print(ending) capitals(\u0026#34;List of countries\u0026#34;, France = \u0026#39;Paris\u0026#39;, Germany=\u0026#39;Berlin\u0026#39;) List of countries The capital of France in Paris The capital of Germany in Berlin  keywords = {\u0026#39;france\u0026#39;: \u0026#39;Paris\u0026#39;, \u0026#39;Germany\u0026#39; : \u0026#39;Allemage\u0026#39;} capitals(\u0026#34;List of countries 2 \u0026#34;, **keywords) List of countries 2 The capital of france in Paris The capital of Germany in Allemage  4.6 Import own functions Modules refer to a file containing Python statements and definitions.\nA file containing Python code, for e.g.: example.py, is called a module and its module name would be example.\nWe use modules to break down large programs into small manageable and organized files. Furthermore, modules provide reusability of code.\nWe can define our most used functions in a module and import it, instead of copying their definitions into different programs.\nHere, we have defined different funcitons inside a module named my_functions.\nWe use the import keyword to import our module and then our functions. To import our previously defined module example we type the following in the Python prompt:\nimport my_functions Using the module name we can access the function using the dot . operator. For example:\nmy_functions.substraction(4,6) -2  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"30ee55255960cecb37c5fffee543c4b3","permalink":"/courses/tutorial_python/4-functions/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/4-functions/","section":"courses","summary":"4 Libraries and functions in Python 4.1 Introduction to librairies To facilitate the processing, manipulation and visualization of data, Python has many libraries.\nPython libraries allow you to import codes and functions that will make our analysis easier.\nHere are the most popular libraries:\n numpy: mathematics or scientific calculations: http://www.numpy.org/ pandas: data manipulation: http://pandas.pydata.org/ matplotlib: visualization of data: https://matplotlib.org/ scikit-learn: machine learning: https://scikit-learn.org Datetime: formatting dates: https: //docs.python.org/2/library/datetime.html  To import a library under Python, use the function: import + library_name","tags":null,"title":"4 Functions","type":"docs"},{"authors":null,"categories":null,"content":" The dictionary stores (key, value) pairs They are unordered data structures Principle: we can link a key to a value The values of a dictionary can be of any type, but the keys must be of an immutable data type such as strings, numbers, or tuples. Keys are unique within a dictionary while values may not be. Each key is separated from its value by a colon (:), the items are separated by commas, and the whole thing is enclosed in curly braces. An empty dictionary without any items is written with just two curly braces, like this: {}. Dictionary values have no restrictions. They can be any arbitrary Python object, either standard objects or user-defined objects. However, same is not true for the keys. Keys must be immutable. Which means you can use strings, numbers or tuples as dictionary keys but something like [\u0026lsquo;key\u0026rsquo;] is not allowed.  You can find all you need to know about dictionaries in the documentation: https://docs.python.org/2/library/stdtypes.html#dict\n5.1 Create a dictionary: In a dictionary, each key is separated from its value by a colon (:), the items are separated by commas, and the whole thing is enclosed in curly braces like this: {}.\n# exemple : students\u0026#39; note dictionary my_dictionary = { \u0026#34;Marie\u0026#34; : 15, \u0026#34;Thomas\u0026#34; : 12, \u0026#34;Julien\u0026#34; : \u0026#34;absent\u0026#34;, \u0026#34;Elise\u0026#34; : 9, \u0026#34;Samuel\u0026#34; : 17 } my_dictionary # We have the key : the value  {'Marie': 15, 'Thomas': 12, 'Julien': 'absent', 'Elise': 9, 'Samuel': 17}  5.2 Update a dictionary: You can update a dictionary by adding a new entry or a key-value pair, modifying an existing entry, or deleting an existing entry as shown below.\n To addkey-value pair  # Adding a key-value pair in a dictionary my_dictionary[\u0026#34;Julie\u0026#34;]=9 print(my_dictionary) {'Marie': 15, 'Thomas': 12, 'Julien': 'absent', 'Elise': 9, 'Samuel': 17, 'Julie': 9}   To changea value using existing key  my_dictionary[\u0026#34;Julien\u0026#34;]=13 print(my_dictionary) {'Marie': 15, 'Thomas': 12, 'Julien': 13, 'Elise': 9, 'Samuel': 17, 'Julie': 9}  More than one entry per key not allowed. Which means no duplicate key is allowed. When duplicate keys encountered during assignment, the last assignment wins.\ndict = {\u0026#39;Name\u0026#39;: \u0026#39;Zara\u0026#39;, \u0026#39;Age\u0026#39;: 7, \u0026#39;Name\u0026#39;: \u0026#39;Manni\u0026#39;} dict {'Name': 'Manni', 'Age': 7}   To deletea value using a key  You can either remove individual dictionary elements or clear the entire contents of a dictionary. You can also delete entire dictionary in a single operation.\nTo explicitly remove an entire dictionary, just use the del statement. Following is a simple example −\ndel my_dictionary[\u0026#34;Julie\u0026#34;] # remove entry with key \u0026#39;Name\u0026#39; my_dictionary.clear() # remove all entries in my_dictionary del my_dictionary # delete entire dictionary print(my_dictionary) --------------------------------------------------------------------------- NameError Traceback (most recent call last) \u0026lt;ipython-input-7-04bcd56b6daf\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 print(my_dictionary) NameError: name 'my_dictionary' is not defined ---------------------------------------------------------------------------  5.3 Access a dictionary: To access dictionary elements, you can use the familiar square brackets along with the key to obtain its value.\nmy_dictionary = { \u0026#34;Marie\u0026#34; : 15, \u0026#34;Thomas\u0026#34; : 12, \u0026#34;Julien\u0026#34; : \u0026#34;absent\u0026#34;, \u0026#34;Elise\u0026#34; : 9, \u0026#34;Samuel\u0026#34; : 17 } my_dictionary[\u0026#34;Samuel\u0026#34;] 17   We can access ditionary elements using FORloop:  by value valeur by key by key-value pair    # .keys() method returns list of dictionary dict\u0026#39;s keys: my_dictionary.keys() dict_keys(['Marie', 'Thomas', 'Julien', 'Elise', 'Samuel'])  # .values() method returns list of dictionary dict\u0026#39;s values my_dictionary.values() dict_values([15, 12, 'absent', 9, 17])  # Example using for loop and keys dictionary for key in my_dictionary.keys(): print(key) Marie Thomas Julien Elise Samuel  for value in my_dictionary.values(): print(value) 15 12 absent 9 17  # .items() method returns a list of dict\u0026#39;s (key, value) tuple pairs my_dictionary.items() dict_items([('Marie', 15), ('Thomas', 12), ('Julien', 'absent'), ('Elise', 9), ('Samuel', 17)])  # Example using for loop and key-value pairs in dictionary for key,value in my_dictionary.items(): # la boucle for obtient un tuple à chaque itération if value == \u0026#39;absent\u0026#39;: print(\u0026#39;Absent\u0026#39;) else: print(\u0026#39;The average of %sis %s/20\u0026#39; % (key, value)) The average of Marie is 15 /20 The average of Thomas is 12 /20 Absent The average of Elise is 9 /20 The average of Samuel is 17 /20  Exercise on dictionaries Goal: To manipulate a dictionary containing the notes of 15 students. 1- Create a dictionary named notes_levels containing the following notes: Mary: 15; Samuel: 17; Gaston: 12; Fred: 10; Mae: 5; Julie: 15; Zoe: 7; Claire: 20; Chloe: 8; Julian: 14, Gael: 9, Samia: 15, Omar: 11, Gabriel: 16, Manon: 2 2- What is the average of the class? 3- Display the total number of students in the class. 4- How many students have a grade strictly above average? 5- What is the name of the best student in the class? 6- How many students have a first name with strictly less than 4 letters? 7- Show the first name of the pupils who have an even note (multiple of 2).!  Correction  1:  notes_levels ={ \u0026#34;Marie\u0026#34; : 15, \u0026#34;Samuel\u0026#34; : 17, \u0026#34;Gaston\u0026#34; : 12, \u0026#34;Fred\u0026#34; : 10, \u0026#34;Mae\u0026#34; : 5, \u0026#34;Julie\u0026#34; : 15, \u0026#34;Zoe\u0026#34; : 7, \u0026#34;Claire\u0026#34; : 20, \u0026#34;Chloe\u0026#34; : 8, \u0026#34;Julien\u0026#34; : 14, \u0026#34;Gaël\u0026#34; : 9, \u0026#34;Samia\u0026#34; : 15, \u0026#34;Omar\u0026#34; : 11, \u0026#34;Gabriel\u0026#34; : 16, \u0026#34;Manon\u0026#34; : 2 }  2:  notes_eleves.values() dict_values([15, 17, 12, 10, 5, 15, 7, 20, 8, 14, 9, 15, 11, 16, 2])  import numpy average_note =numpy.mean(list(notes_eleves.values())) `` ```python average_note 11.733333333333333   3:  nombre_eleves=len(notes_eleves) print(\u0026#34;Le nombre d\u0026#39;élèves dans la classe est de %d\u0026#34; % (nombre_eleves)) Le nombre d'élèves dans la classe est de 15  len(notes_eleves.keys()) 15   4:  nombre_eleves_avec_note_sup_moyenne=0 for valeur in notes_eleves.values(): if valeur \u0026gt; moyenne_generale: nombre_eleves_avec_note_sup_moyenne=nombre_eleves_avec_note_sup_moyenne+1 print(\u0026#34;Le nombre d\u0026#39;élèves avec une note supérieure à %.2fest de %délèves\u0026#34; % (moyenne_generale, nombre_eleves_avec_note_sup_moyenne)) Le nombre d'élèves avec une note supérieure à 11.73 est de 8 élèves   5:  # On va d\u0026#39;abord déterminer la meilleure note: utilisation de la fonction max() meilleure_note=max(notes_eleves.values()) ```python meilleure_note 20  # On va parcourir notre dictionnaire et trouver la clef associée à notre valeur # Pour cela on va travailler sur les tuples avec la méthode item() for prenom,note in notes_eleves.items(): if note == meilleure_note: print(prenom) Claire   6:  # On va parcourir les clefs de notre dictionnaire et mettre une condition sur la longueur de chaque clef nombre_eleves=0 for prenom in notes_eleves.keys(): if len(prenom) \u0026lt; 4: nombre_eleves=nombre_eleves+1 print(nombre_eleves) 2   7:  # On va parcourir les tuples du dictionnaire et mettre une condition sur les valeurs  for prenom,note in notes_eleves.items(): if note % 2 == 0: print(prenom, note) Gaston 12 Fred 10 Claire 20 Chloe 8 Julien 14 Gabriel 16 Manon 2  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ceaee89e50e6b119c01402939535006b","permalink":"/courses/tutorial_python/5-dictionaries/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/5-dictionaries/","section":"courses","summary":"The dictionary stores (key, value) pairs They are unordered data structures Principle: we can link a key to a value The values of a dictionary can be of any type, but the keys must be of an immutable data type such as strings, numbers, or tuples. Keys are unique within a dictionary while values may not be. Each key is separated from its value by a colon (:), the items are separated by commas, and the whole thing is enclosed in curly braces.","tags":null,"title":"5 Dictionaries","type":"docs"},{"authors":null,"categories":null,"content":" Numpy is the core library for scientific computing in Python A new data container will be used: the ndarray (N-dimensional array). There are vectors (one-dimensional arrays), multidimensional arrays It provides a high-performance multidimensional array object, and tools for working with these arrays We'll see how to initialize Numpy arrays in several ways, access values in arrays, perform math and matrix operations, and use arrays for both masking and comparisons.  https://docs.scipy.org/doc/numpy/reference/\n6.1- Create a Numpy array or ndarray We first need to import the numpy package:\n# We will import Numpy library and create alias.  # Codes with aliases are easier to write and read.  import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.\nUnlike a list, you can not create empty Numpy tables. You will find below several ways to initialize a Numpy table according to your needs:\n Using array()function to create Numpy array:  table1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]) table1 array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])  Creation of a two-dimensional array with rows and columns; we create a list of lists. Each list is a row of the table.\n# 2 rows and 3 columns table2 = np.array([[1,2,3], [4,5,6]]) table2 array([[1, 2, 3], [4, 5, 6]])  # 3 rows and 3 columns table3 = np.array([[1,2,3], [4,5,6], [7,8,9]]) table3 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])   Using range()function to create Numpy array:  table4 = np.array(range(10)) # table with values from 0 to 9 table4 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])   Using zeros()function to create Numpy array with \u0026lsquo;0\u0026rsquo; value :  table5 = np.zeros((4,3)) # table with 4 rows and 3 columns table5 array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])   Using ones()function to create Numpy array with ones:  table6 = np.ones((4,3)) # table with 4 rows and 3 columns table6 array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])   Using identity()function to create Numpy array as matrix identity:  table7 = np.identity(4) # 4 dimensions table7 array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]])   Converting list to ndarray with array()function  my_list = [0,1,2,3,4,5,6] my_list [0, 1, 2, 3, 4, 5, 6]  table8 = np.array(my_list) table8 array([0, 1, 2, 3, 4, 5, 6])   Using random()function to create ndarray with random values:  table9 = np.random.randint(100,size=(4,3)) # 4*3 ndarray with random values between 0 and 100  table9 array([[78, 47, 23], [79, 17, 5], [61, 41, 71], [12, 41, 27]])   Using full()to create a constant array.  table10 = np.full((2,2), 7) # Create a constant array table10 array([[7, 7], [7, 7]])  6.2 Access data in a Numpy or ndarray array We can access an individual element or a slice of values. Similar to lists, the first element is indexed to 0. For example, array1 [0,0] indicates that we are accessing the first row and the first column. The first number of the tuple [0,0] indicates the index of the line and the second number indicates the index of the column:\nmy_table = np.array([[1,2,3], [4,5,6], [7,8,9]]) my_table array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])   Here are some examples to access values in a numpy array:  my_table[1,2] # We want the element located at row with index 1 and column with index 2 # ndarray(row,column)  6  my_table[1,-1] # We want the element at row index 1 and last column 6  my_table[0,1] # We want the element located at row with index 0 and column with index 1 2  my_table[1,0] # We want the element located at row with index 1 and column with index 0  4   Here are some examples to access data using Slicing  my_table[:,0] # We want all rows with column index 0 array([1, 4, 7])  my_table[0,:] # We want all columns with row index 0 array([1, 2, 3])  my_table[:,0:3:2] # We want all rows and index columns from 0 to 3 with steps of 2. # So all rows and columns 1 and 2. array([[1, 3], [4, 6], [7, 9]])  my_table[:,-1] #We want all rows at last column array([3, 6, 9])  my_table[:,1:-1] # We want all rows with columns between index 1 and last index array([[2], [5], [8]])  my_table[2,1:-1] # We want values at row with index 2 and rows between index 1 and last index array([8])  6.3 Mathematical and matrix calculations on a Numpy array: Numpy tables are very easy to manipulate: concatenate, add, multiply, transpose with a single line of code. Below you will find some examples of various arithmetic and multiplicative operations with Numpy tables.\narray1 = np.arange(9).reshape(3,3) array1 array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])  array2 = np.random.randint(50, size=(3,3)) array2 array([[47, 20, 49], [40, 15, 7], [ 4, 35, 14]])   Basic arithmetic operations: addition, subtraction, multiplication, division Here a list of mathematical functions provided by numpy in: https://docs.scipy.org/doc/numpy/reference/routines.math.html  array1 + 10 # add a value to each element array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])  array1 - 10 # substract value to each element  array([[-10, -9, -8], [ -7, -6, -5], [ -4, -3, -2]])  array1 * 100 # multiply value to each element  array([[ 0, 100, 200], [300, 400, 500], [600, 700, 800]])  array1[:,0] * 10 # we multiply by 10 all elements at column with index 0 array([ 0, 30, 60])  array1 / 2 # we divide a value to each element array([[0. , 0.5, 1. ], [1.5, 2. , 2.5], [3. , 3.5, 4. ]])   some Numpy functions and methods applicable on Numpy tables:  all,any,apply_along_axis,argmax,argmin,argsort,average,bincount,ceil,clip,conj,corrcoef,cov,crosscumprod,cumsum,diff,dot,floor,inner,lexsort,max,maximum,mean,median,min,minimum,nonzero,outer,re,round,sort,std,sum,trace,transpose,var,vdotvectorize,wherenp.add(array1,10) # add a value to each element array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])  np.subtract(array1,10) # substract a value to each element  array([[-10, -9, -8], [ -7, -6, -5], [ -4, -3, -2]])  np.multiply(array1,100) # multiply a value to each element array([[ 0, 100, 200], [300, 400, 500], [600, 700, 800]])  np.divide(array1, 2) # divide a value to each element array([[0. , 0.5, 1. ], [1.5, 2. , 2.5], [3. , 3.5, 4. ]])  np.mean(array1) # computing the Numpy table average using the mean() function 4.0  array1.mean() # computing the Numpy table average using the mean() method 4.0  array1.min() # computing the Numpy table mimimum using the min() method 0  array1.max() # computing the Numpy table maximum using the max() method 8  np.mean(array1, axis=0) # we apply mean() function only over columns  array([3., 4., 5.])  np.mean(array1, axis=1) # we apply mean() function only over rows  array([1., 4., 7.])   Operations between several tables  (array1 +1) * array2 # multiplication of 2 tables array([[ 47, 40, 147], [160, 75, 42], [ 28, 280, 126]])  array1 + array2 # sum of 2 tables array([[47, 21, 51], [43, 19, 12], [10, 42, 22]])  np.dot(array1, array2) # dot product of 2 tables array([[ 48, 85, 35], [321, 295, 245], [594, 505, 455]])  6.4 Update Numpy array: Other interesting features include concatenation, splitting, transposition (changing elements from one row to another and vice versa) and obtaining elements diagonally.\na) - To manipulate / modify the dimensions of a Numpy array: The dimension of a table is given by the number of elements following each axis. We have specific methods and attributes specific to ndarray ():\nnp.floor(10*np.random.random((3,4))) array([[1., 5., 5., 0.], [5., 5., 4., 4.], [6., 2., 5., 2.]])  a = np.floor(10*np.random.random((3,4))) print(a, a.shape, a.ndim) [[3. 5. 5. 6.] [0. 7. 1. 6.] [4. 8. 2. 5.]] (3, 4) 2  Depending on our programming needs, we can change the size of a table.\na.ravel() # ravel() function to write over table on 1 dimension (flattened) array([3., 5., 5., 6., 0., 7., 1., 6., 4., 8., 2., 5.])  a.reshape(6,2) # reshape() function to change the dimension of our array array([[3., 5.], [5., 6.], [0., 7.], [1., 6.], [4., 8.], [2., 5.]])  a.T #T method to calculate the transpose of our array. array([[3., 0., 4.], [5., 7., 8.], [5., 1., 2.], [6., 6., 5.]])  print(a.T.shape, a.shape) (4, 3) (3, 4)  The reshape function returns its argument with a modified form, while the ndarray.resize method modifies the array itself:\na.resize((2,6)) a array([[3., 5., 5., 6., 0., 7.], [1., 6., 4., 8., 2., 5.]])  If a dimension is set to -1 in a resizing operation, the other dimensions are automatically calculated:\na.reshape(4,-1) array([[3., 5., 5.], [6., 0., 7.], [1., 6., 4.], [8., 2., 5.]])  b) - Working with a subset of a Numpy table: my_table = np.array([[1,2,3], [4,5,6], [7,8,9]]) my_table array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  We will select a subset of our ndarray with selecting the first row.\nsubset = my_table[0] subset array([1, 2, 3])  We want to change the first element of our subset.\nsubset[0] = 100 subset array([100, 2, 3])  But by modifying our sub-table, we realize that we have modified our initial table.\nmy_table array([[100, 2, 3], [ 4, 5, 6], [ 7, 8, 9]])  A modification of the subset causes a modification of the initial table. Our subset array is a view of our initial table. Reason: saving memory when working with large volumes of data.\nIf you really want to work with a subset without modifying the original array, you must make a copy with the copy () \u0026lt;/ b\u0026gt; function.\nmy_table = np.array([[1,2,3], [4,5,6], [7,8,9]]) subset = my_table[0].copy() subset[0]=101 subset array([101, 2, 3])  my_table array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  c)- To concatenate Numpy array: vstack, column_stack, concatenate concatenate()function to join a sequence of arrays along an existing axis.  Concatenate function can take two or more arrays of the same shape and by default it concatenates row-wise i.e. axis=0.\narray1=np.array([[1,2,3],[4,5,6],[7,8,9]]) array1 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  array2=np.array([[2,5,6],[9,10,11],[5,6,9]]) array2 array([[ 2, 5, 6], [ 9, 10, 11], [ 5, 6, 9]])  np.concatenate([array1,array2], axis=0) # to join a sequence of arrays along rows (axis = 0)  array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [ 2, 5, 6], [ 9, 10, 11], [ 5, 6, 9]])  np.concatenate([array1,array2], axis=1) # to join a sequence of arrays along columns (axis = 1)  array([[ 1, 2, 3, 2, 5, 6], [ 4, 5, 6, 9, 10, 11], [ 7, 8, 9, 5, 6, 9]])  In addition to the concatenate function, NumPy also offers two convenient functions hstack and vstack to stack/combine arrays horizontally or vertically.\n vstack()function stacks arrays in sequence vertically i.e. row wise. And the result is the same as using concatenate with axis=0. hstack()function stacks arrays horizontally i.e. column wise. And the result is the same as using concatenate with axis=1.  array3=np.array([10,20,30]) # 1D Numpy array array3 array([10, 20, 30])  array4=np.array([[1,2,3],[4,5,6],[7,8,9]]) # bi-dimentionnal Numpy array array4 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  np.vstack([array4,array3]) array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 20, 30]])  array5=np.array([[10],[20],[30]]) array5.shape (3, 1)  np.hstack([array4,array5]) array([[ 1, 2, 3, 10], [ 4, 5, 6, 20], [ 7, 8, 9, 30]])  d)- To split Numpy arrays: This is the opposite of concatenation. We have the split (), hsplit () and vsplit () functions.\narray=np.array([15,16,17,12,49,52,12,14,36]) len(array) # array size 9  np.split(array,3) # we split the array into 3 arrays [array([15, 16, 17]), array([12, 49, 52]), array([12, 14, 36])]  We can split our numpy array using breaking point with index.\nnp.split(array,[2,6]) # we want to cut our table into 3 tables, the numbers between [] are the breakpoints. # corresponds to the indexes where we cut [array([15, 16]), array([17, 12, 49, 52]), array([12, 14, 36])]  array1,array2,array3=np.split(array,[2,6]) print(array1,array2,array3) [15 16] [17 12 49 52] [12 14 36]  array1,array2,array3,array4=np.split(array,[2,4,6]) print(array1,array2,array3,array4) [15 16] [17 12] [49 52] [12 14 36]  To break 2-dimensional tables. we use hsplit or vsplit.\narray2=np.array([[1,2,3],[4,5,6],[7,8,9]]) array2 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  array1,array1bis=np.vsplit(array2, [2]) print(array1,array1bis) [[1 2 3] [4 5 6]] [[7 8 9]]  array2 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  array1,array1bis=np.hsplit(array2,[2]) print(array1,array1bis) [[1 2] [4 5] [7 8]] [[3] [6] [9]]  e)- To delete rows and columns in a Numpy array: delete()function array=np.array([[1,2,3],[4,5,6],[7,8,9]]) array array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  np.delete(array,2,axis=0) # to delete row with index 2  array([[1, 2, 3], [4, 5, 6]])  np.delete(array2,2,axis=1) # to delete column with index 2  array([[1, 2], [4, 5], [7, 8]])  f)- To calculate the transpose of a Numpy table: transpose()function np.transpose(array) array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])  g)- To get the values of the diagonal of a Numpy table: diagonal()function array.diagonal() array([1, 5, 9])  6.5 Comparisons and masks : With Numpy arrays, you can use a boolean matrix to filter and compare Numpy arrays.\ntable = np.random.randint(100,size=(6,6)) # 4*3 Numpy array with random values between 0 and 100  table array([[62, 72, 26, 37, 76, 39], [38, 66, 42, 78, 15, 97], [81, 19, 83, 89, 87, 66], [11, 49, 53, 71, 63, 25], [75, 87, 68, 88, 30, 12], [95, 61, 14, 40, 61, 31]])  mask = table\u0026gt;50 # we generate here a mask with same dimension than our original table but with Boolean values mask array([[ True, True, False, False, True, False], [False, True, False, True, False, True], [ True, False, True, True, True, True], [False, False, True, True, True, False], [ True, True, True, True, False, False], [ True, True, False, False, True, False]])  Being both of the same size, we can use this Boolean matrix to our advantage. In other words, we can do Boolean masking. With this Boolean matrix as a mask, we can use it to select the particular subset of data that interests us.\ntable[mask] #table[table\u0026gt;50] # same job array([62, 72, 76, 66, 78, 97, 81, 83, 89, 87, 66, 53, 71, 63, 75, 87, 68, 88, 95, 61, 61])  table[table\u0026gt;50] array([62, 72, 76, 66, 78, 97, 81, 83, 89, 87, 66, 53, 71, 63, 75, 87, 68, 88, 95, 61, 61])  We have many other comparison operators to compare two arrays such as == (equality),! = (No equality), \u0026lt;= (less than or equal to). We can even combine two Boolean statements \u0026amp; (for \u0026ldquo;AND\u0026rdquo; conditions) or | (for the \u0026ldquo;OR\u0026rdquo; conditions).\n#table[table\u0026gt;=50]  #table[table\u0026lt;50]  #table[table!=50]  #table[table==50]  #table[(table \u0026gt;=50) \u0026amp; (table \u0026lt;=70)]  table[(table\u0026gt;=50) | (table\u0026lt;=40)] array([62, 72, 26, 37, 76, 39, 38, 66, 78, 15, 97, 81, 19, 83, 89, 87, 66, 11, 53, 71, 63, 25, 75, 87, 68, 88, 30, 12, 95, 61, 14, 40, 61, 31])  Exercise using Numpy library: we will work on data station using Numpy library.  read the file containing the daily precipitation and temperature data for the Ottawa station for the year 2017  1- Create a Numpy table with a column for temperature and a column for precipitation. (With two Numpy 1D tables, create a 2D array (365 rows and 2 columns).\n2- Convert the temperature data into Celcius (T [Celcius] = T [Kelvin] - 273.15).\n3- How many days have an accumulation greater than 25mm?\n4- What temperature was recorded for the day with the greatest accumulation?\n5- Calculate the number of degree days (\u0026gt; 0degC) for the year 2017.\n6- Calculate the daily precipitation totals for the year 2017 and assign this variable to the cumul_recipitation table. Add the cumul_recipitation table to the table.\n7- Just for the exercise, split the array into 2 arrays, then concatenate them again to get the initial array.\nimport numpy as np # to read a csv file with numpy, we can use genfromtxt() function.  temperature = np.genfromtxt(\u0026#34;./DATA/OTTAWA_tasmoy_2017.csv\u0026#34;, dtype=float) precipitation = np.genfromtxt(\u0026#34;./DATA/OTTAWA_PrecTOT_2017.csv\u0026#34;, dtype=float) print(temperature.shape, precipitation.shape) (365,) (365,)  Correction #1-  tableau = np.column_stack([temperature,precipitation]) #tableau = np.hstack([temperature,precipitation]) tableau = np.hstack([temperature.reshape(len(temperature),-1),precipitation.reshape(len(precipitation),-1)]) `\nprint(tableau.shape) (365, 2)  #2-  tableau[:,0]=tableau[:,0]-273.15 `` ```python len(tableau[tableau[:,1]\u0026gt;=25]) 10  # 4-  precipitation_max = np.nanmax(tableau[:,1]) print(precipitation_max) 45.01  tableau[tableau[:,1] == precipitation_max] array([[15.8 , 45.01]])  # 5-  sum(tableau[:,1][tableau[:,1]\u0026gt;0]) 1328.5300000000002  # 6-  cumul_precipitation = np.nancumsum(tableau[:,1]) #  tableau = np.column_stack([tableau, cumul_precipitation]) array([[-6.50000e+00, 1.10000e-01, 1.10000e-01], [-6.80000e+00, 3.03000e+00, 3.14000e+00], [-1.80000e+00, 2.65700e+01, 2.97100e+01], ..., [ nan, 0.00000e+00, 1.32726e+03], [-2.15000e+01, 1.27000e+00, 1.32853e+03], [-2.20000e+01, 0.00000e+00, 1.32853e+03]])  # 7.  temperature2,precipitation2,cumul2 = np.hsplit(tableau, 3) tableau_original=np.concatenate([temperature2+273.15, precipitation2], axis = 1) print(tableau_original) ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"d72fbdd2d25ce0013ee0035ecdfc22c5","permalink":"/courses/tutorial_python/6-numpy_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/6-numpy_library/","section":"courses","summary":"Numpy is the core library for scientific computing in Python A new data container will be used: the ndarray (N-dimensional array). There are vectors (one-dimensional arrays), multidimensional arrays It provides a high-performance multidimensional array object, and tools for working with these arrays We'll see how to initialize Numpy arrays in several ways, access values in arrays, perform math and matrix operations, and use arrays for both masking and comparisons.  https://docs.","tags":null,"title":"6 Numpy library","type":"docs"},{"authors":null,"categories":null,"content":"Pandas is a library specialized in data manipulation. This library contains a set of optimized functions for handling large datasets. It allows to create and export tables of data from text files (separators, .csv, fixed format, compressed), binary (HDF5 with Pytable), HTML, XML, JSON, MongoDB, SQL \u0026hellip;\nA new data structure is used with this library: the DataFrame. There are two types of data with pandas: seriesand dataframes.\n  a dataframe is an array that is created with dictionaries or lists\n  they are based on Numpy or ndarray tables\n  they can have column and line names\n  they have the particularity of being able to mix the types of data: str, float, Nan, Int \u0026hellip;\n  they can be viewed as an excel sheet but with a larger number of data volumes and a larger number of functions and attributes.\n  7.1 Series introduction - Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.). The axis labels are collectively called index.\n# first, we must import Pandas library import pandas as pd # aliasing as pd import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) A pandas Series can be created using the following constructor:\nserie = pd.Series([11,15,12,13,14]) print(serie) 0 11 1 15 2 12 3 13 4 14 dtype: int64  We have in series, indexes and values. These indexes can be replaced by text with the index option. Be careful, the number of indexes must correspond to the number of values.\nserie = pd.Series([11,15,12,13,14], index=[\u0026#34;Montreal\u0026#34;, \u0026#34;Ottawa\u0026#34;, \u0026#34;Toronto\u0026#34;, \u0026#34;Gatineau\u0026#34;, \u0026#34;Quebec\u0026#34;]) print(serie) Montreal 11 Ottawa 15 Toronto 12 Gatineau 13 Quebec 14 dtype: int64  The describe() method computes a summary of statistic\nserie.describe() count 5.000000 mean 13.000000 std 1.581139 min 11.000000 25% 12.000000 50% 13.000000 75% 14.000000 max 15.000000 dtype: float64  We can use index to access to an element.\nserie[\u0026#34;Montreal\u0026#34;] 11  We can use index number.\nserie[3] 13  We can use several indexes.\nserie[[\u0026#34;Montreal\u0026#34;, \u0026#34;Quebec\u0026#34;, \u0026#34;Toronto\u0026#34;]] Montreal 11 Quebec 14 Toronto 12 dtype: int64  There are large number of methods collectively compute descriptive statistics such as min(), max(), sum() \u0026hellip;\nserie.min() 11  serie.max() 15  We can apply comparison operators.\nserie[serie\u0026gt;12] Ottawa 15 Gatineau 13 Québec 14 dtype: int64  serie\u0026gt;12 Montréal False Ottawa True Toronto False Gatineau True Québec True dtype: bool  7.2 Pandas Dataframes Pandas DataFrame is two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). A Data frame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows and columns. Pandas DataFrame consists of three principal components, the data, rows, and columns.\n7.2.1 Create a Dataframe In the real world, a Pandas DataFrame will be created by loading the datasets from existing storage, storage can be SQL Database, CSV file, and Excel file. Pandas DataFrame can be created from the lists, dictionary, and from a list of dictionary etc. Dataframe can be created in different ways here are some ways by which we create a dataframe:\n- a) Using a Numpy array import numpy as np stations = np.genfromtxt(\u0026#34;./DATA/DATA_Barrage_1963_2017_5.csv\u0026#34;, delimiter=\u0026#34;,\u0026#34;, dtype=\u0026#39;float\u0026#39;) stations array([[ 39.7 , 39.09 , 39.55645161, 23.23 , 22.5 , 22.85903226, 2390.52612903, 3164.97 , 1673.8 ], [ 41.16 , 40.96 , 41.08806452, 23.22 , 22.43 , 22.7583871 , 2227.28290323, 3008.18 , 1697.87 ], [ 41.15 , 41.05 , 41.11322581, 23.35 , 22.87 , 23.15548387, 2851.27419355, 3231.8 , 2367.85 ], [ 41.09 , 40.61 , 40.9683871 , 23.78 , 22.7 , 23.03258065, 2635.03774194, 3967.33 , 2069.65 ], [ 41.09 , 39.6 , 40.26967742, 24.24 , 22.87 , 23.64580645, 3924.23451613, 5407.32 , 2417.89 ]])  To create the dataframe, we use the Pandas DataFrame () function. It is at this stage that we define the names of our columns. In input we put the table Numpy.\ndataframe = pd.DataFrame(stations, columns=[\u0026#34;Amont Max\u0026#34;, \u0026#34;Amont Min\u0026#34;, \u0026#34;Amont Mean\u0026#34;, \u0026#34;Aval Max\u0026#34;, \u0026#34;Aval Max\u0026#34;, \u0026#34;Aval Mean\u0026#34;, \u0026#34;Debit Mean\u0026#34;, \u0026#34;Debit Max\u0026#34;,\u0026#34;Debit Min\u0026#34;]) dataframe     Amont Max Amont Min Amont Mean Aval Max Aval Max Aval Mean Debit Mean Debit Max Debit Min     0 39.7 39.09 39.5565 23.23 22.5 22.859 2390.53 3164.97 1673.8   1 41.16 40.96 41.0881 23.22 22.43 22.7584 2227.28 3008.18 1697.87   2 41.15 41.05 41.1132 23.35 22.87 23.1555 2851.27 3231.8 2367.85   3 41.09 40.61 40.9684 23.78 22.7 23.0326 2635.04 3967.33 2069.65   4 41.09 39.6 40.2697 24.24 22.87 23.6458 3924.23 5407.32 2417.89    - b) Create Dataframe loading csv file: read_table()or read_csv()function  read_table () and read_csv () are the most useful functions under Pandas for reading text files and generating a DataFrame.  We will work with a dataset from a hydraulic dam. Our csv file has 9 variables, the first line gives us the names of the variables (or labels).\nA csv document can be read with the read_table () function, with the separator attribute \u0026ldquo;,\u0026quot;.\nbarrage = pd.read_table(\u0026#34;./DATA/DATA_EXTREME_Carillon_1963_2017_5.csv\u0026#34;, sep=\u0026#34;,\u0026#34;) barrage.head()     Amont_max Amont_min Amont_moyen Aval_max Aval_min Aval_moyen Debit_Moyen Debit_max Debit_min     0 39.7 39.09 39.5565 23.23 22.5 22.859 2390.53 3164.97 1673.8   1 41.16 40.96 41.0881 23.22 22.43 22.7584 2227.28 3008.18 1697.87   2 41.15 41.05 41.1132 23.35 22.87 23.1555 2851.27 3231.8 2367.85   3 41.09 40.61 40.9684 23.78 22.7 23.0326 2635.04 3967.33 2069.65   4 41.09 39.6 40.2697 24.24 22.87 23.6458 3924.23 5407.32 2417.89    However, if we know that our file to read is a csv, we can use a simpler function of Pandas which is read_csv () .\nNo need to use sep=\u0026rsquo;\u0026rsquo; option. He will find the separator by default.\nhelp(pd.read_csv) Several options are available to the read_csv () function. It is important to know the list of possibilities and options offered by this simple command.\n         path Path to our file   sep Delimiter like , ; \\t or \\s+ for a variable number of spaces   header default 0, the first line contains the name of the variables; if None the names are generated or defined later   index_col Names or numbers of columns defining the indexes of lines, indexes which can be hierarchized   names If header = None, list of variable names   nrows Useful for testing and limiting the number of lines to read   skiprow List of lines to jump in reading   skip_footer Number of lines to jump at the end of file   na_values Definition of the code or codes signaling missing values. They can be defined in a dictionary to associate variables and codes specific missing values   usecols Selects a list of variables to read to avoid reading large or unnecessary fields or variables   skip_blank_lines If True , we skip the white lines   thousand Separator: \u0026ldquo;.\u0026rdquo; or \u0026ldquo;,\u0026rdquo;    # Using example file1 = \u0026#34;./DATA/DATA_EXTREME_Carillon_1963_2017_5.csv\u0026#34; col_names = [\u0026#39;Variable1\u0026#39;, \u0026#39;Variable2\u0026#39;, \u0026#39;Variable3\u0026#39;] df2 = pd.read_csv(file1, skiprows=1, usecols=[0, 1, 3], names=col_names) df2.head()     Variable1 Variable2 Variable3     0 39.7 39.09 23.23   1 41.16 40.96 23.22   2 41.15 41.05 23.35   3 41.09 40.61 23.78   4 41.09 39.6 24.24    - c)Create Dataframe loading ascii file: with open(\u0026#39;./DATA/Daily_Precipitation_1963-2017.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() with open(\u0026#39;./DATA/Daily_Precipitation_1963-2017.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() dataset = [float(row) for row in rows.split()] df3 = pd.DataFrame({\u0026#34;Precipitation\u0026#34; : dataset}) df3.head()     Precipitation     0 0   1 0   2 0   3 0   4 0   5 0   6 0   7 0   8 1.3    - d)Create Dataframe loading excell (.xls) file: read_excel()function We will open here an excel file (.xls extension). This file is a database containing information on all homogenized Environmental and Climate Change Canada temperature stations.\nThis database has 11 columns with data starting at the 4 th line.\nWe will define the \u0026ldquo;Province\u0026rdquo; column as index of our DataFrame.\ndf4 = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, index_col=0,skiprows = range(0, 3)) df4.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y    7.2.2 Access data from DataFrames The first thing to do when opening a new dataset is print out a few rows. We accomplish this with .head()method:\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.head()     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   4 BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    To see the last five rows use .tail()method. tail() also accepts a number, and in this case we printing the bottom two rows.:\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.tail()     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     332 NL PLUM POINT 8402958 1972 7 2016 6 51.07 -56.88 6 N   333 NL PORT AUXBASQUES 8402975 1909 2 2017 9 47.58 -58.97 40 N   334 NL ST ANTHONY 8403389 1946 6 2017 12 51.37 -55.6 33 Y   335 NL ST JOHN'S 8403505 1874 1 2017 12 47.62 -52.75 141 Y   336 NL STEPHENVILLE 8403801 1895 6 2017 12 48.53 -58.55 26 Y   337 NL WABUSH LAKE 8504177 1960 11 2017 12 52.93 -66.87 551 Y    Before exploring a Dataframe, you can modify the index to make it easier to analyze the dataset. For this, we use the .set_index () function. We must create a new object.\ndataframe_Prov_index = dataframe.set_index(\u0026#34;Prov\u0026#34;) dataframe_Prov_index.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    We can directly select a column from a Dataframe:\ndataframe_Prov_index[\u0026#39;Nom de station\u0026#39;].head() Prov BC AGASSIZ BC ATLIN BC BARKERVILLE BC BEAVERDELL BC BELLA COOLA Name: Nom de station, dtype: object  Pandas supports Multi-axes indexing to get the subset of pandas object. Then, to access an element in a dataframe, there are two methods:\n  the iloc () method to access data from index numbers\n  the loc () method to access data from labels\n  a- iloc () method: We can access data from Dataframe using index integer. Like numpy, this method is 0-based indexing.\n# Example1: select specific row and specific column dataframe_Prov_index.iloc[0,0] 'AGASSIZ'  # Example2: iloc: # select first 4 rows f and all columns dataframe_Prov_index.iloc[0:4,:]    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y    # Example3: iloc: # select all rows and 4 specific columns  dataframe_Prov_index.iloc[:,0:4].head()    Prov Nom de station stnid année déb. mois déb.     BC AGASSIZ 1100120 1893 1   BC ATLIN 1200560 1905 8   BC BARKERVILLE 1090660 1888 2   BC BEAVERDELL 1130771 1939 1   BC BELLA COOLA 1060841 1895 5    # Example4: iloc:# Slicing through list of values print(dataframe_Prov_index.iloc[[1, 3, 5], [1, 3]]) print(dataframe_Prov_index.iloc[1:3, :]) dataframe_Prov_index.iloc[:,1:3].head()  stnid mois déb. Prov BC 1200560 8 BC 1130771 1 BC 1021480 7 Nom de station stnid année déb. mois déb. année fin. mois fin. \\ Prov BC ATLIN 1200560 1905 8 2017 12 BC BARKERVILLE 1090660 1888 2 2015 3 lat (deg) long (deg) élév (m) stns jointes Prov BC 59.57 -133.70 674 N BC 53.07 -121.52 1265 N     Prov stnid année déb.     BC 1100120 1893   BC 1200560 1905   BC 1090660 1888   BC 1130771 1939   BC 1060841 1895    b- La méthode loc(): This method has purely label based indexing.\n.loc() has multiple access methods like −\n -single scalar label -list of labels -slice object -Boolean array  .loc takes two single/list/range operator separated by \u0026lsquo;,'. The first one indicates the row and the second one indicates columns.\n# Example1: loc: select all rows for a specific column dataframe_Prov_index.loc[:,\u0026#34;Nom de station\u0026#34;].head() Prov BC AGASSIZ BC ATLIN BC BARKERVILLE BC BEAVERDELL BC BELLA COOLA Name: Nom de station, dtype: object  # Example2: loc: select all rows for a specific index name dataframe_Prov_index.loc[\u0026#34;QC\u0026#34;,:].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     QC AMOS 709CEE9 1913 6 2017 8 48.57 -78.13 305 Y   QC BAGOTVILLE 7060400 1880 11 2017 12 48.33 -71 159 Y   QC BAIE COMEAU 704S001 1965 1 2017 12 49.13 -68.2 130 Y   QC BEAUCEVILLE 7027283 1913 8 2017 8 46.15 -70.7 168 Y   QC BELLETERRE 7080600 1951 9 2004 4 47.38 -78.7 322 N    # Example3: Select all rows for multiple columns, say list[] dataframe_Prov_index.loc[:,[\u0026#34;Nom de station\u0026#34;, \u0026#34;année déb.\u0026#34;, \u0026#34;année fin.\u0026#34;]].head()    Prov Nom de station année déb. année fin.     BC AGASSIZ 1893 2017   BC ATLIN 1905 2017   BC BARKERVILLE 1888 2015   BC BEAVERDELL 1939 2006   BC BELLA COOLA 1895 2017    # Example4: Select few rows for multiple columns, say list[] dataframe_Prov_index.loc[[\u0026#39;BC\u0026#39;,\u0026#39;QC\u0026#39;],[\u0026#34;Nom de station\u0026#34;, \u0026#34;année déb.\u0026#34;, \u0026#34;année fin.\u0026#34;]].head()    Prov Nom de station année déb. année fin.     BC AGASSIZ 1893 2017   BC ATLIN 1905 2017   BC BARKERVILLE 1888 2015   BC BEAVERDELL 1939 2006   BC BELLA COOLA 1895 2017    # Example 5: # for getting values with a boolean array (dataframe_Prov_index.loc[\u0026#39;BC\u0026#39;,[\u0026#34;année déb.\u0026#34;]]\u0026gt;1900).head()    Prov année déb.     BC False   BC True   BC False   BC True   BC False    # Example 6: # for getting values with a boolean array dataframe_Prov_index.loc[dataframe_Prov_index[\u0026#34;année fin.\u0026#34;]\u0026gt;2015,:].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y   BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N   BC BLUE RIVER 1160899 1946 9 2017 12 52.13 -119.28 683 Y    # Example 7: # for getting values with a boolean array df2 = dataframe_Prov_index.loc[\u0026#34;QC\u0026#34;,:] df2.loc[df2[\u0026#34;année fin.\u0026#34;]==2017,:].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     QC AMOS 709CEE9 1913 6 2017 8 48.57 -78.13 305 Y   QC BAGOTVILLE 7060400 1880 11 2017 12 48.33 -71 159 Y   QC BAIE COMEAU 704S001 1965 1 2017 12 49.13 -68.2 130 Y   QC BEAUCEVILLE 7027283 1913 8 2017 8 46.15 -70.7 168 Y   QC CAUSAPSCAL 7051200 1913 11 2017 8 48.37 -67.23 168 N    7.2.3 Change a Dataframe 7.2.3.1 Column Selection/Addition/Deletion- We will use here our previous Dataframe.\ndataframe_Prov_index.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    a- Create a new variable We can select columns from our Dataframe to create a new one. In this example, we will calculate the number of recording years for each station.\ndelta_year = (dataframe_Prov_index[\u0026#34;année fin.\u0026#34;] - dataframe_Prov_index[\u0026#34;année déb.\u0026#34;]) + 1 delta_year.head() Prov BC 125 BC 113 BC 128 BC 68 BC 123 dtype: int64  b- Column Addition in a DataFrame dataframe_Prov_index[\u0026#34;total année\u0026#34;] = delta_year dataframe_Prov_index.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes total année     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N 125   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 113   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N 128   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 68    c- Column Deletion in a DataFrame Columns from a Dataframe can be deleted or popped; let us take an example to understand how.\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) # using del function print (\u0026#34;Deleting \u0026#39;stns jointes\u0026#39;column using DEL function:\u0026#34;) del dataframe[\u0026#39;stns jointes\u0026#39;] dataframe.head() Deleting 'stns jointes' column using DEL function:      Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m)     0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15   1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674   2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265   3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838   4 BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18    # using pop function print (\u0026#34;Deleting \u0026#39;stnid\u0026#39;column using POP function:\u0026#34;) dataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.pop(\u0026#39;stnid\u0026#39;) dataframe.head() Deleting 'stnid' column using POP function:      Prov Nom de station année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     0 BC AGASSIZ 1893 1 2017 12 49.25 -121.77 15 N   1 BC ATLIN 1905 8 2017 12 59.57 -133.7 674 N   2 BC BARKERVILLE 1888 2 2015 3 53.07 -121.52 1265 N   3 BC BEAVERDELL 1939 1 2006 9 49.48 -119.05 838 Y   4 BC BELLA COOLA 1895 5 2017 11 52.37 -126.68 18 Y    # using drop method dataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.drop([\u0026#34;stns jointes\u0026#34;], axis=1).head()     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m)     0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15   1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674   2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265   3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838   4 BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18    7.2.3.2 Row Selection/Addition/Deletion- We will now understand row selection, addition and deletion through examples. Let us begin with the concept of selection.\na- Row Selection Selection by Label Rows can be selected by passing row label to a loc function.\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)).set_index(\u0026#34;Prov\u0026#34;) dataframe.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    dataframe.loc[\u0026#39;BC\u0026#39;].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    Rows can be selected by passing integer location to an iloc function.\ndataframe.iloc[0] Nom de station AGASSIZ stnid 1100120 année déb. 1893 mois déb. 1 année fin. 2017 mois fin. 12 lat (deg) 49.25 long (deg) -121.77 élév (m) 15 stns jointes N Name: BC, dtype: object  Multiple rows can be selected using ‘ : ’ operator.\ndataframe[2:4]    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y    b- Row Addition Add new rows to a DataFrame using function append()function. This function will append the rows at the end.\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)).set_index(\u0026#34;Prov\u0026#34;) df_new = pd.DataFrame({\u0026#39;Nom de station\u0026#39;: [\u0026#39;station1\u0026#39;, \u0026#39;station2\u0026#39;], \u0026#39;stnid\u0026#39;: [8888, 9999], \u0026#39;Prov\u0026#39;: [\u0026#39;BC\u0026#39;, \u0026#39;QC\u0026#39;]}).set_index(\u0026#34;Prov\u0026#34;) df_new    Prov Nom de station stnid     BC station1 8888   QC station2 9999    dataframe = dataframe.append(df_new) dataframe.tail()    Prov Nom de station année déb. année fin. lat (deg) long (deg) mois déb. mois fin. stnid stns jointes élév (m)     NL ST JOHN'S 1874 2017 47.62 -52.75 1 12 8403505 Y 141   NL STEPHENVILLE 1895 2017 48.53 -58.55 6 12 8403801 Y 26   NL WABUSH LAKE 1960 2017 52.93 -66.87 11 12 8504177 Y 551   BC station1 nan nan nan nan nan nan 8888 nan nan   QC station2 nan nan nan nan nan nan 9999 nan nan    c- Row Deletion Use index label to delete or drop rows from a DataFrame. If label is duplicated, then multiple rows will be dropped.\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)).set_index(\u0026#34;Prov\u0026#34;) dataframe.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    # Drop rows with label \u0026#39;BC\u0026#39; dataframe = dataframe.drop(\u0026#39;BC\u0026#39;) dataframe.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     YT BURWASH 2100181 1966 10 2017 12 61.37 -139.05 807 Y   YT DAWSON 2100LRP 1901 1 2017 12 64.05 -139.13 370 Y   N YT HAINES JUNCTIO 2100630 1944 10 2017 12 60.75 -137.5 596 N   YT KOMAKUK BEACH 2100682 1958 7 2017 12 69.62 -140.2 13 Y   YT MAYO 2100701 1924 10 2017 12 63.62 -135.87 504 Y    dataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)).set_index(\u0026#34;Prov\u0026#34;) dataframe.loc[\u0026#39;ON\u0026#39;].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     ON ATIKOKAN 6020LPQ 1917 1 2017 12 48.8 -91.58 442 Y   ON BEATRICE 6110607 1878 1 2017 12 45.13 -79.4 297 Y   ON BELLEVILLE 6150689 1921 1 2017 12 44.15 -77.4 76 N   ON BIG TROUT LAKE 6010735 1939 2 2017 12 53.83 -89.87 224 Y   ON BROCKVILLE 6100971 1915 7 2017 12 44.6 -75.67 96 Y    7.2.3.3 Merging/Joining Dataframe- Pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL.\npd.merge(left, right, how=\u0026#39;inner\u0026#39;, on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True)          left DataFrame object   right Another DataFrame object   on Columns (names) to join on. Must be found in both the left and right DataFrame objects   left_on Columns from the left DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame   right_on Columns from the right DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame   left_index If True, use the index (row labels) from the left DataFrame as its join key(s). In case of a DataFrame with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame   right_index Same usage as left_index for the right DataFrame   how One of \u0026lsquo;left\u0026rsquo;, \u0026lsquo;right\u0026rsquo;, \u0026lsquo;outer\u0026rsquo;, \u0026lsquo;inner\u0026rsquo;. Defaults to inner. Each method has been described below   sort Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve the performance substantially in many cases    Let us now create two different DataFrames and perform the merging operations on it.\nleft_dataframe = pd.DataFrame({ \u0026#39;id\u0026#39;:[1,2,3,4], \u0026#39;Nom de station\u0026#39;: [\u0026#39;MONTREAL TAVISH\u0026#39;, \u0026#39;QUEBEC\u0026#39;, \u0026#39;TADOUSSAC\u0026#39;,\u0026#39;OKA\u0026#39;], \u0026#39;variable\u0026#39;:[\u0026#39;var1\u0026#39;,\u0026#39;var2\u0026#39;,\u0026#39;var6\u0026#39;,\u0026#39;var5\u0026#39;]}) right_dataframe = pd.DataFrame( {\u0026#39;id\u0026#39;:[1,2,3,4], \u0026#39;Nom de station\u0026#39;: [\u0026#39;TORONTO\u0026#39;, \u0026#39;OTTAWA\u0026#39;, \u0026#39;KINGSTON\u0026#39;,\u0026#39;CHAPLEAU\u0026#39;], \u0026#39;variable\u0026#39;:[\u0026#39;var3\u0026#39;,\u0026#39;var1\u0026#39;,\u0026#39;var6\u0026#39;,\u0026#39;var5\u0026#39;]}) left_dataframe     id Nom de station variable     0 1 MONTREAL TAVISH var1   1 2 QUEBEC var2   2 3 TADOUSSAC var6   3 4 OKA var5    right_dataframe     id Nom de station variable     0 1 TORONTO var3   1 2 OTTAWA var1   2 3 KINGSTON var6   3 4 CHAPLEAU var5    a- Merge Two DataFrames on a Key pd.merge(left_dataframe,right_dataframe,on=\u0026#39;id\u0026#39;)     id Nom de station_x variable_x Nom de station_y variable_y     0 1 MONTREAL TAVISH var1 TORONTO var3   1 2 QUEBEC var2 OTTAWA var1   2 3 TADOUSSAC var6 KINGSTON var6   3 4 OKA var5 CHAPLEAU var5    b- Merge Two DataFrames on a Key pd.merge(left_dataframe,right_dataframe,on=[\u0026#39;id\u0026#39;,\u0026#39;variable\u0026#39;])     id Nom de station_x variable Nom de station_y     0 3 TADOUSSAC var6 KINGSTON   1 4 OKA var5 CHAPLEAU    c- Merge Two DataFrames Using \u0026lsquo;How\u0026rsquo; argument The how argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or the right tables, the values in the joined table will be NA.\n# Left Join pd.merge(left_dataframe, right_dataframe, on=\u0026#39;variable\u0026#39;, how=\u0026#39;left\u0026#39;)     id_x Nom de station_x variable id_y Nom de station_y     0 1 MONTREAL TAVISH var1 2 OTTAWA   1 2 QUEBEC var2 nan nan   2 3 TADOUSSAC var6 3 KINGSTON   3 4 OKA var5 4 CHAPLEAU    # right Join pd.merge(left_dataframe, right_dataframe, on=\u0026#39;variable\u0026#39;, how=\u0026#39;right\u0026#39;)     id_x Nom de station_x variable id_y Nom de station_y     0 1 MONTREAL TAVISH var1 2 OTTAWA   1 3 TADOUSSAC var6 3 KINGSTON   2 4 OKA var5 4 CHAPLEAU   3 nan nan var3 1 TORONTO    # outer Join pd.merge(left_dataframe, right_dataframe, on=\u0026#39;variable\u0026#39;, how=\u0026#39;outer\u0026#39;)     id_x Nom de station_x variable id_y Nom de station_y     0 1 MONTREAL TAVISH var1 2 OTTAWA   1 2 QUEBEC var2 nan nan   2 3 TADOUSSAC var6 3 KINGSTON   3 4 OKA var5 4 CHAPLEAU   4 nan nan var3 1 TORONTO    # inner Join pd.merge(left_dataframe, right_dataframe, on=\u0026#39;variable\u0026#39;, how=\u0026#39;inner\u0026#39;)     id_x Nom de station_x variable id_y Nom de station_y     0 1 MONTREAL TAVISH var1 2 OTTAWA   1 3 TADOUSSAC var6 3 KINGSTON   2 4 OKA var5 4 CHAPLEAU    7.2.3.4 Dataframe Concatenation- Pandas provides various facilities for easily combining together DataFrame objects.\npd.concat(objs,axis=0,join='outer',join_axes=None,ignore_index=False)           objs This is a sequence or mapping of Series, DataFrame objects   axis {0, 1, \u0026hellip;}, default 0. This is the axis to concatenate along   join {‘inner’, ‘outer’}, default ‘outer’. How to handle indexes on other axis(es). Outer for union and inner for intersection   ignore_index boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, \u0026hellip;, n - 1   join_axes This is the list of Index objects. Specific indexes to use for the other (n-1) axes instead of performing inner/outer set logic    dataframe1 = pd.DataFrame({ \u0026#39;id\u0026#39;:[1,2,3,4], \u0026#39;Nom de station\u0026#39;: [\u0026#39;MONTREAL TAVISH\u0026#39;, \u0026#39;QUEBEC\u0026#39;, \u0026#39;TADOUSSAC\u0026#39;,\u0026#39;OKA\u0026#39;], \u0026#39;variable\u0026#39;:[\u0026#39;var1\u0026#39;,\u0026#39;var2\u0026#39;,\u0026#39;var6\u0026#39;,\u0026#39;var5\u0026#39;]}) dataframe2 = pd.DataFrame( {\u0026#39;id\u0026#39;:[1,2,3,4], \u0026#39;Nom de station\u0026#39;: [\u0026#39;TORONTO\u0026#39;, \u0026#39;OTTAWA\u0026#39;, \u0026#39;KINGSTON\u0026#39;,\u0026#39;CHAPLEAU\u0026#39;], \u0026#39;variable\u0026#39;:[\u0026#39;var3\u0026#39;,\u0026#39;var1\u0026#39;,\u0026#39;var6\u0026#39;,\u0026#39;var5\u0026#39;]}) pd.concat([dataframe1,dataframe2])     id Nom de station variable     0 1 MONTREAL TAVISH var1   1 2 QUEBEC var2   2 3 TADOUSSAC var6   3 4 OKA var5   0 1 TORONTO var3   1 2 OTTAWA var1   2 3 KINGSTON var6   3 4 CHAPLEAU var5    Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can do this by using the keys argument −\npd.concat([dataframe1,dataframe2],keys=[\u0026#39;QC\u0026#39;,\u0026#39;ON\u0026#39;])     id Nom de station variable     (\u0026lsquo;QC\u0026rsquo;, 0) 1 MONTREAL TAVISH var1   (\u0026lsquo;QC\u0026rsquo;, 1) 2 QUEBEC var2   (\u0026lsquo;QC\u0026rsquo;, 2) 3 TADOUSSAC var6   (\u0026lsquo;QC\u0026rsquo;, 3) 4 OKA var5   (\u0026lsquo;ON\u0026rsquo;, 0) 1 TORONTO var3   (\u0026lsquo;ON\u0026rsquo;, 1) 2 OTTAWA var1   (\u0026lsquo;ON\u0026rsquo;, 2) 3 KINGSTON var6   (\u0026lsquo;ON\u0026rsquo;, 3) 4 CHAPLEAU var5    If we don't want the index being duplicated, set ignore_index to True.\npd.concat([dataframe1,dataframe2],keys=[\u0026#39;QC\u0026#39;,\u0026#39;ON\u0026#39;],ignore_index=True)     id Nom de station variable     0 1 MONTREAL TAVISH var1   1 2 QUEBEC var2   2 3 TADOUSSAC var6   3 4 OKA var5   4 1 TORONTO var3   5 2 OTTAWA var1   6 3 KINGSTON var6   7 4 CHAPLEAU var5    If the two Dataframes need to be added along axis=1, then the new columns will be appended.\npd.concat([dataframe1,dataframe2],axis=1)     id Nom de station variable id Nom de station variable     0 1 MONTREAL TAVISH var1 1 TORONTO var3   1 2 QUEBEC var2 2 OTTAWA var1   2 3 TADOUSSAC var6 3 KINGSTON var6   3 4 OKA var5 4 CHAPLEAU var5    Concatenating Using append A useful shortcut to concat are the append instance methods on DataFrame. They concatenate along axis=0, namely the index\ndataframe1.append(dataframe2)     id Nom de station variable     0 1 MONTREAL TAVISH var1   1 2 QUEBEC var2   2 3 TADOUSSAC var6   3 4 OKA var5   0 1 TORONTO var3   1 2 OTTAWA var1   2 3 KINGSTON var6   3 4 CHAPLEAU var5    7.2.4 Basic Functionality on DataFrame There are many built-in functions and methods:\nhttps://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html\nWe will present some useful functions with exploring a dataset.\ndataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) dataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     0 0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N nan nan nan nan nan   1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 5.63043 18.9033 -3.46052 -19.235 860.083   2 2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N nan nan nan nan nan   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N 12.1556 20.2007 6.77689 1.09467 2518.68     .shapemethod:  Returns a tuple representing the dimensionality of the DataFrame. Tuple (a,b), where a represents the number of rows and b represents the number of columns.\ndataframe.shape (289, 17)   .columnsmethod:  Returns names of the columns in our Dataframe.\ndataframe.columns Index(['Unnamed: 0', 'Prov', 'Nom de station', 'stnid', 'année déb.', 'mois déb.', 'année fin.', 'mois fin.', 'lat (deg)', 'long (deg)', 'élév (m)', 'stns jointes', 'Tmax', 'Tmax90p', 'Tmin', 'Tmin10p', 'DG0'], dtype='object')   .emptymethod:  Returns the Boolean value saying whether the Object is empty or not; True indicates that the object is empty.\ndataframe.empty False   .isnullmethod:  To detect missing values easier (and across different array dtypes), Pandas provides the isnull() and notnull() functions.\ndataframe[\u0026#39;Tmax\u0026#39;].isnull().head() 0 True 1 False 2 True 3 False 4 False Name: Tmax, dtype: bool  We combine this method with .sum() to know the number of missing values.\ndataframe[\u0026#39;Tmax\u0026#39;].isnull().sum() 117   .dropnamethod:  If you want to exclude the missing values, then use the dropna function along with the axis argument. By default, axis=0, i.e., along row, which means that if any value within a row is NA then the whole row is excluded.\ndataframe_sans_NaN = dataframe.dropna() dataframe_sans_NaN[\u0026#39;Tmax\u0026#39;].isnull().sum() 0  dataframe_sans_NaN.shape # nouvelle dimension de notre tableau  (169, 17)   sort_valuesmethod:  sort_values() is the method for sorting by values. It accepts a \u0026lsquo;by\u0026rsquo; argument which will use the column name of the DataFrame with which the values are to be sorted.\ndataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     0 0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N nan nan nan nan nan   1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 5.63043 18.9033 -3.46052 -19.235 860.083   2 2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N nan nan nan nan nan   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N 12.1556 20.2007 6.77689 1.09467 2518.68    df_label_sorted = dataframe.sort_values(by=\u0026#34;Prov\u0026#34;) df_label_sorted.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     127 127 AB SLAVE LAKE 3065995 1922 8 2017 12 55.3 -114.78 583 Y 7.48137 22.4643 -3.42227 -20.3803 1131.03   106 106 AB EDMONTON 3012216 1880 7 2017 12 53.57 -113.52 723 Y 8.98212 24.328 -3.11709 -19.1907 1095.93   104 104 AB COLD LAKE 3081680 1925 7 2017 12 54.42 -110.28 541 Y 7.72448 24.2737 -3.03759 -21.2037 1303.95   103 103 AB CARWAY 3031402 1914 8 2017 12 49 -113.37 1354 Y 11.1397 24.925 -1.61886 -13.98 956.837   102 102 AB CAMROSE 3011240 1946 3 2017 12 53.03 -112.82 739 N 8.96497 24.1757 -3.71442 -20.588 1048.13    The argument could takes a list of column values.\ndf_label_sorted = dataframe.sort_values(by=[\u0026#39;Prov\u0026#39;,\u0026#39;année déb.\u0026#39;]) df_label_sorted.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     106 106 AB EDMONTON 3012216 1880 7 2017 12 53.57 -113.52 723 Y 8.98212 24.328 -3.11709 -19.1907 1095.93   110 110 AB FORT CHIPEWYAN 3072655 1883 10 2017 12 58.77 -111.12 238 Y 4.28219 23.566 -6.54421 -28.6297 1135.11   121 121 AB MEDICINE HAT 3034485 1883 8 2017 12 50.02 -110.72 717 Y 12.6489 28.4973 -0.1559 -15.7833 1563.87   99 99 AB CALGARY 3031092 1885 1 2017 12 51.12 -114.02 1084 Y 10.835 24.4673 -1.44461 -15.235 1172.05   97 97 AB BANFF 3050519 1887 11 2017 12 51.2 -115.55 1397 Y 8.90652 23.132 -3.08488 -15.8583 750.37     sort_index()method:  Using the sort_index() method, by passing the axis arguments and the order of sorting, DataFrame can be sorted. By default, sorting is done on row labels in ascending order.\ndf_label_sorted.sort_index().head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     0 0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N nan nan nan nan nan   1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 5.63043 18.9033 -3.46052 -19.235 860.083   2 2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N nan nan nan nan nan   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N 12.1556 20.2007 6.77689 1.09467 2518.68    df_label_sorted.sort_index(ascending=False).head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     288 288 NL WABUSH LAKE 8504177 1960 11 2017 12 52.93 -66.87 551 Y 2.33974 19.6937 -8.02478 -29.4537 792.913   287 287 NL STEPHENVILLE 8403801 1895 6 2017 12 48.53 -58.55 26 Y 8.80653 20.614 1.81275 -10.582 1710.17   286 286 NL ST JOHN'S 8403505 1874 1 2017 12 47.62 -52.75 141 Y 9.07367 21.6237 1.54977 -8.919 1474.82   285 285 NL ST ANTHONY 8403389 1946 6 2017 12 51.37 -55.6 33 Y nan nan nan nan nan   284 284 NL PORT AUXBASQUES 8402975 1909 2 2017 9 47.58 -58.97 40 N nan nan nan nan nan    df_label_sorted.sort_index(axis=1).head()     DG0 Nom de station Prov Tmax Tmax90p Tmin Tmin10p Unnamed: 0 année déb. année fin. lat (deg) long (deg) mois déb. mois fin. stnid stns jointes élév (m)     106 1095.93 EDMONTON AB 8.98212 24.328 -3.11709 -19.1907 106 1880 2017 53.57 -113.52 7 12 3012216 Y 723   110 1135.11 FORT CHIPEWYAN AB 4.28219 23.566 -6.54421 -28.6297 110 1883 2017 58.77 -111.12 10 12 3072655 Y 238   121 1563.87 MEDICINE HAT AB 12.6489 28.4973 -0.1559 -15.7833 121 1883 2017 50.02 -110.72 8 12 3034485 Y 717   99 1172.05 CALGARY AB 10.835 24.4673 -1.44461 -15.235 99 1885 2017 51.12 -114.02 1 12 3031092 Y 1084   97 750.37 BANFF AB 8.90652 23.132 -3.08488 -15.8583 97 1887 2017 51.2 -115.55 11 12 3050519 Y 1397     .describe()method:  Generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.\ndataframe[\u0026#39;Tmax\u0026#39;].describe() count 172.000000 mean 7.916314 std 5.524630 min -15.366463 25% 7.364646 50% 8.988052 75% 11.018241 max 16.105511 Name: Tmax, dtype: float64   .dtypes()method:  Returns the dtypes in this object.\ndataframe.dtypes Unnamed: 0 int64 Prov object Nom de station object stnid object année déb. int64 mois déb. int64 année fin. int64 mois fin. int64 lat (deg) float64 long (deg) float64 élév (m) int64 stns jointes object Tmax float64 Tmax90p float64 Tmin float64 Tmin10p float64 DG0 float64 dtype: object  7.2.5 DataFrame Function Application To apply your own or another library’s functions to Pandas objects, you should be aware of the three important methods. The methods have been discussed below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame, row- or column-wise, or element wise.\n  Table wise Function Application: .pipe()  Row or Column Wise Function Application: .apply()  Element wise Function Application: .applymap()  .apply()method:\n  Arbitrary functions can be applied along the axes of a DataFrame or Panel using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument. By default, the operation performs column wise, taking each column as an array-like.\ndataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) dataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) dataframe[\u0026#34;stns jointes\u0026#34;]=dataframe[\u0026#34;stns jointes\u0026#34;].apply(lambda x: x.replace(\u0026#34;N\u0026#34;, \u0026#34;NaN\u0026#34;)) dataframe[\u0026#34;stns jointes\u0026#34;]=dataframe[\u0026#34;stns jointes\u0026#34;].apply(lambda x: x.replace(\u0026#34;Y\u0026#34;, \u0026#34;1\u0026#34;)) dataframe = dataframe.dropna() dataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 nan 5.63043 18.9033 -3.46052 -19.235 860.083   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 1 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 nan 12.1556 20.2007 6.77689 1.09467 2518.68   5 5 BC BLUE RIVER 1160899 1946 9 2017 12 52.13 -119.28 683 1 10.4408 25.9917 -1.12501 -12.5823 987.363   9 9 BC COMOX 1021830 1935 11 2017 12 49.72 -124.9 26 1 13.7376 23.1023 6.42485 -0.526 2444.39    dataframe[\u0026#34;Tmin\u0026#34;]=dataframe[\u0026#34;Tmin\u0026#34;].apply(lambda x: round(x,2)) dataframe[\u0026#34;Tmax\u0026#34;]=dataframe[\u0026#34;Tmax\u0026#34;].apply(lambda x: int(x)) dataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 nan 5 18.9033 -3.46 -19.235 860.083   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 1 12 23.6417 4.02 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 nan 12 20.2007 6.78 1.09467 2518.68   5 5 BC BLUE RIVER 1160899 1946 9 2017 12 52.13 -119.28 683 1 10 25.9917 -1.13 -12.5823 987.363   9 9 BC COMOX 1021830 1935 11 2017 12 49.72 -124.9 26 1 13 23.1023 6.42 -0.526 2444.39    7.2.6 DataFrame GroupBY method Any groupby operation involves one of the following operations on the original object. They are −\n Splitting the Object Applying a function Combining the results  In many situations, we split the data into sets and we apply some functionality on each subset. In the apply functionality, we can perform the following operations −\n Aggregation − computing a summary statistic Transformation − perform some group-specific operation Filtration − discarding the data with some condition  Let us now create a DataFrame object and perform all the operations on it −\ndataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) dataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     0 0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N nan nan nan nan nan   1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 5.63043 18.9033 -3.46052 -19.235 860.083   2 2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N nan nan nan nan nan   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N 12.1556 20.2007 6.77689 1.09467 2518.68    Looking at the DataFrame above, we see that there are at least 3 variables that we can use to group our dataset. For example, we can group our data by province (Prov), by year of beginning of recording or year of end of recording.\nWe will use the Pandas groupby module to group our data.\n .unique()method :  Returns the unique values of a column.\ndataframe[\u0026#34;Prov\u0026#34;].unique() array(['BC', 'YT', 'N YT', 'NT', 'NU', 'AB', 'SK', 'MB', 'ON', 'QC', 'NB', 'NS', 'PE', 'NL'], dtype=object)  Split Data into Groups: dataframe.groupby(\u0026#39;Prov\u0026#39;) \u0026lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000000008D35860\u0026gt;   To view groups:  dataframe.groupby(\u0026#39;Prov\u0026#39;).groups {'AB': Int64Index([ 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130], dtype='int64'), 'BC': Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], dtype='int64'), 'MB': Int64Index([156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175], dtype='int64'), 'N YT': Int64Index([52], dtype='int64'), 'NB': Int64Index([254, 255, 256, 257, 258, 259, 260, 261], dtype='int64'), 'NL': Int64Index([275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288], dtype='int64'), 'NS': Int64Index([262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], dtype='int64'), 'NT': Int64Index([61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], dtype='int64'), 'NU': Int64Index([74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], dtype='int64'), 'ON': Int64Index([176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215], dtype='int64'), 'PE': Int64Index([274], dtype='int64'), 'QC': Int64Index([216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253], dtype='int64'), 'SK': Int64Index([131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155], dtype='int64'), 'YT': Int64Index([51, 53, 54, 55, 56, 57, 58, 59, 60], dtype='int64')}   Group by with multiple columns:  dataframe.groupby([\u0026#39;Prov\u0026#39;,\u0026#39;année fin.\u0026#39;]).groups {('AB', 2011): Int64Index([116], dtype='int64'), ('AB', 2013): Int64Index([101], dtype='int64'), ('AB', 2016): Int64Index([100, 130], dtype='int64'), ('AB', 2017): Int64Index([ 96, 97, 98, 99, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129], dtype='int64'), ('BC', 2006): Int64Index([3], dtype='int64'), ('BC', 2013): Int64Index([20], dtype='int64'), ('BC', 2014): Int64Index([21, 25], dtype='int64'), ('BC', 2015): Int64Index([2, 8, 11, 16], dtype='int64'), ('BC', 2016): Int64Index([4, 6, 41], dtype='int64'), ('BC', 2017): Int64Index([ 0, 1, 5, 7, 9, 10, 12, 13, 14, 15, 17, 18, 19, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50], dtype='int64'), ('MB', 2016): Int64Index([156], dtype='int64'), ('MB', 2017): Int64Index([157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175], dtype='int64'), ('N YT', 2017): Int64Index([52], dtype='int64'), ('NB', 2017): Int64Index([254, 255, 256, 257, 258, 259, 260, 261], dtype='int64'), ('NL', 2011): Int64Index([282], dtype='int64'), ('NL', 2015): Int64Index([276], dtype='int64')}  Iterating through Groups: grouped = dataframe.groupby(\u0026#39;Prov\u0026#39;) for name,group in grouped: print(name) print(group) AB Unnamed: 0 Prov Nom de station stnid année déb. mois déb. \\ 96 96 AB ATHABASCA 3060L20 1918 6 97 97 AB BANFF 3050519 1887 11 98 98 AB BEAVERLODGE 3070600 1913 4 99 99 AB CALGARY 3031092 1885 1 année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes \\ 96 2017 12 54.82 -113.53 626 Y 97 2017 12 51.20 -115.55 1397 Y 98 2017 12 55.20 -119.40 745 Y 99 2017 12 51.12 -114.02 1084 Y  Select a group:  .get_group()method:  We can select a single group.\ngrouped.get_group(\u0026#39;QC\u0026#39;).head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     216 216 QC AMOS 709CEE9 1913 6 2017 8 48.57 -78.13 305 Y nan nan nan nan nan   217 217 QC BAGOTVILLE 7060400 1880 11 2017 12 48.33 -71 159 Y 8.28139 25.2973 -1.83359 -20.8117 1528.41   218 218 QC BEAUCEVILLE 7027283 1913 8 2017 8 46.15 -70.7 168 Y 10.5515 26.0783 -1.37634 -19.6283 1509.36   219 219 QC BROME 7020840 1890 9 2014 7 45.18 -72.57 206 N 11.1409 26.1767 -0.294151 -17.56 1676.23   220 220 QC CAUSAPSCAL 7051200 1913 11 2017 8 48.37 -67.23 168 N nan nan nan nan nan    Aggregations An aggregated function returns a single aggregated value for each group. Once the group by object is created, several aggregation operations can be performed on the grouped data.\nAn obvious one is aggregation via the aggregate or equivalent agg method −\ndataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) grouped = dataframe.groupby(\u0026#39;Prov\u0026#39;) grouped[\u0026#39;Tmin\u0026#39;].agg(np.mean) Prov AB -3.050928 BC 1.578024 MB -4.493900 N YT NaN NB 0.341750 NL -1.324334 NS 2.542629 NT -8.518655 NU -16.522658 ON 0.031970 PE 1.986827 QC -1.992695 SK -3.208984 YT -8.995421 Name: Tmin, dtype: float64  Applying Multiple Aggregation Functions at Once With grouped Series, you can also pass a list or dict of functions to do aggregation with, and generate DataFrame as output −\ngrouped[\u0026#39;Tmin\u0026#39;].agg([np.min, np.mean, np.max, np.std])    Prov amin mean amax std     AB -6.54421 -3.05093 -0.1559 1.40133   BC -6.06858 1.57802 7.01629 3.89638   MB -10.0943 -4.4939 -1.98027 2.61835   N YT nan nan nan nan   NB -0.491043 0.34175 0.840796 0.598429   NL -8.02478 -1.32433 1.81275 3.42585   NS 1.21413 2.54263 3.75946 0.982653   NT -12.4518 -8.51865 -6.68323 2.16937   NU -21.9351 -16.5227 -12.2332 3.0491   ON -7.48484 0.0319696 5.91872 3.64788   PE 1.98683 1.98683 1.98683 nan   QC -8.76805 -1.99269 1.62212 2.70668   SK -5.01656 -3.20898 -1.42885 1.03055   YT -10.2177 -8.99542 -7.77313 1.72858    Transformations Transformation on a group or a column returns an object that is indexed the same size of that is being grouped. Thus, the transform should return a result that is the same size as that of a group chunk.\ngrouped = dataframe.groupby(\u0026#39;Prov\u0026#39;) and_stand = lambda x: (x - x.mean()) / x.std() grouped[\u0026#39;Tmin\u0026#39;].transform(and_stand).head() 0 NaN 1 -1.293133 2 NaN 3 0.626082 4 1.334280 Name: Tmin, dtype: float64  Filtration Filtration filters the data on a defined criteria and returns the subset of data. The filter() function is used to filter the data.\ndataframe.groupby(\u0026#39;Prov\u0026#39;).filter(lambda x: len(x) == 1)     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     52 52 N YT HAINES JUNCTIO 2100630 1944 10 2017 12 60.75 -137.5 596 N nan nan nan nan nan   274 274 PE CHARLOTTETOWN 8300301 1872 11 2017 12 46.28 -63.13 49 Y 10.0063 23.7683 1.98683 -12.0367 1912.16    In the above filter condition, we are asking to return the Provinces which have only one station.\n7.2.7 Save a DataFrame: For writing a DataFrame, use the .to_csv or _table functions with similar options as read_csv () seen previously.\ndataframe.to_csv(\u0026#34;./DATA/My_new_DataFrame.csv\u0026#34;, index = False, header = True, sep = \u0026#39;,\u0026#39;) 7.3 Date Functionality: Using the date.range()function by specifying the periods and the frequency, we can create the date series. By default, the frequency of range is Days.\npd.date_range(\u0026#39;1/1/2011\u0026#39;, periods=5) DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03', '2011-01-04', '2011-01-05'], dtype='datetime64[ns]', freq='D')  We can change the date frequency:\npd.date_range(\u0026#39;1/1/2011\u0026#39;, periods=5,freq=\u0026#39;M\u0026#39;) DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-30', '2011-05-31'], dtype='datetime64[ns]', freq='M')  bdate_range()stands for business date ranges. Unlike date_range(), it excludes Saturday and Sunday.\npd.bdate_range(\u0026#39;1/1/2011\u0026#39;, periods=10) DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-10', '2011-01-11', '2011-01-12', '2011-01-13', '2011-01-14'], dtype='datetime64[ns]', freq='B')  Convenience functions like date_range and bdate_range utilize a variety of frequency aliases. The default frequency for date_range is a calendar day while the default for bdate_range is a business day.\nstart = pd.datetime(2011, 1, 1) end = pd.datetime(2011, 1, 5) pd.date_range(start, end) DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03', '2011-01-04', '2011-01-05'], dtype='datetime64[ns]', freq='D')  7.4 Format dates with the Datetime module: Python provides many features to work with dates and time.\nDatetime is a module that allows you to manipulate dates and times as objects. The idea is simple: you manipulate the object to do all your calculations, and when you need to display it, you format the object into a string.\nhttps://docs.python.org/2/library/datetime.html\nYou can artificially create a datetime object with the following parameters:\n datetime (year, month, day, hour, minute, second, microsecond, timezone)  The parameters \u0026ldquo;year\u0026rdquo;, \u0026ldquo;month\u0026rdquo; and \u0026ldquo;day\u0026rdquo; are mandatory.\nThe datetime module provides the following classes:\n   Class Description     datetime.date A date instance represents a date   datetime.datetime An instance of datetime represents a date and time according to the Gregorian calendar   datetime.time An instance of time represents the time, except for the date   datetime.timedelta The timedelta class is used to keep the differences between two temporal or dated objects   datetime.tzinfo The tzinfo class is used to implement time zone support for time and datetime objects    We will see some examples of using DateTime and its classe.\n1- The datetime class of the datetime module -a Creating a datetime objectfrom datetime import datetime datetime(2019, 3, 1) # instance of datetime datetime.datetime(2019, 3, 1, 0, 0)  now = datetime.now() now datetime.datetime(2019, 10, 24, 14, 8, 0, 512783)  now = now.today() now datetime.datetime(2019, 10, 24, 14, 8, 0, 531785)  now = datetime.utcnow() now datetime.datetime(2019, 10, 24, 18, 8, 0, 546785)  When opening a csv or text file, we have information about the date and time of the measurements but in the form of strings: \u0026ldquo;2018-11-01 15:20\u0026rdquo; or \u0026ldquo;2017/12/1 16:35:22 \u0026ldquo;\u0026hellip;\nIt is possible during the reading to convert these strings into a datetime object.\ndt = datetime.strptime(\u0026#34;2018/11/01 15:20\u0026#34;, \u0026#34;%Y/%m/%d%H:%M\u0026#34;) dt datetime.datetime(2018, 11, 1, 15, 20)  dt = datetime.strptime(\u0026#34;2017/12/1 16:35:22\u0026#34;, \u0026#34;%Y/%m/%d%H:%M:%S\u0026#34;) dt datetime.datetime(2017, 12, 1, 16, 35, 22)  dt = datetime.strptime(\u0026#34;01/11/19 10-35:22\u0026#34;, \u0026#34;%d/%m/%y %H-%M:%S\u0026#34;) dt datetime.datetime(2019, 11, 1, 10, 35, 22)  dt = datetime.strptime(\u0026#34;1Mar 2019 à 09h35\u0026#34;, \u0026#34;%d%b %Y à %Hh%M\u0026#34;) dt datetime.datetime(2019, 3, 1, 9, 35)  b- Manipulate datetime objectFrom an object or instance of datetime, you can retrieve the time and date.\n#now.year #now.month #now.day #maintenant.hour now.minute #now.second #now.microsecond #now 8  We can change datetime instance:\nnow.replace(year=1995) datetime.datetime(1995, 10, 24, 18, 8, 0, 546785)  now.replace(month=1) datetime.datetime(2019, 1, 24, 18, 8, 0, 546785)  We can then convert datetime instance to string:\nd = datetime.now(); print(d) d 2019-10-24 14:08:00.720795 datetime.datetime(2019, 10, 24, 14, 8, 0, 720795)  d.strftime(\u0026#34;%H:%M\u0026#34;), d.strftime(\u0026#34;%Hh%Mmin\u0026#34;) ('14:08', '14h08min')  d.strftime(\u0026#34;%Y-%m %H:%M\u0026#34;) '2019-10 14:08'  \u0026#39;The day today is {0:%d} {0:%B} and it s {0:%Hh%Mmin} \u0026#39;.format(d, \u0026#34;day\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;time\u0026#34;) 'The day today is 24 October and it s 14h08min '   dateet timeclass  These two classes can be used to create a datetime instance.\nfrom datetime import datetime, date, time d = date(2005, 7, 14) t = time(12, 30) t datetime.time(12, 30)  datetime.combine(d, t) datetime.datetime(2005, 7, 14, 12, 30)  now = datetime.utcnow() now.date() now.time() datetime.time(18, 8, 0, 855803)   The timedelta class of the datetime module  from datetime import timedelta delta = timedelta(days=3, seconds=100) # we create our own timedelta datetime.now() datetime.datetime(2019, 10, 24, 14, 8, 0, 899806)  datetime.now() + delta datetime.datetime(2019, 10, 27, 14, 9, 40, 935808)  datetime.now() + timedelta(days=2, hours=4, minutes=3, seconds=12) datetime.datetime(2019, 10, 26, 18, 11, 12, 966809)  time_range = datetime(2010, 12, 31) - datetime(1981, 12, 31) time_range datetime.timedelta(days=10592)   Example1: Calculate the year of birth from a given age  from datetime import datetime old = 25 month = 10 actual_year = datetime.today().year actual_month = datetime.today().month result = actual_year - old - (1 if month \u0026gt; actual_month else 0) print(result) 1994   Example2: Calculate the year of birth from a given age  We can generate dates for time series with an arbitrary time step:\nfrom datetime import timedelta dt = timedelta(days = 5, hours = 6, minutes = 25) d0 = datetime(2000, 2, 21) [str(d0 + i * dt) for i in range(10)] ['2000-02-21 00:00:00', '2000-02-26 06:25:00', '2000-03-02 12:50:00', '2000-03-07 19:15:00', '2000-03-13 01:40:00', '2000-03-18 08:05:00', '2000-03-23 14:30:00', '2000-03-28 20:55:00', '2000-04-03 03:20:00', '2000-04-08 09:45:00'] ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"e257612f18141e3399f89ed09ec181c4","permalink":"/courses/tutorial_python/7-pandas_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/7-pandas_library/","section":"courses","summary":"Pandas is a library specialized in data manipulation. This library contains a set of optimized functions for handling large datasets. It allows to create and export tables of data from text files (separators, .csv, fixed format, compressed), binary (HDF5 with Pytable), HTML, XML, JSON, MongoDB, SQL \u0026hellip;\nA new data structure is used with this library: the DataFrame. There are two types of data with pandas: seriesand dataframes.\n  a dataframe is an array that is created with dictionaries or lists","tags":null,"title":"7 Pandas library","type":"docs"},{"authors":null,"categories":null,"content":"The Matplotlib library is one of the most used libraries for plotting data in Python.\nMany types of graphics can be developed with this library: https://matplotlib.org/gallery/index.html\nTo illustrate some features of the Python matplotlib.pyplot module (which provides a plotting system similar to that of MATLAB), we will use a database from the UQAM station.\nimport pandas as pd import numpy as np import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) dataframe_UQAM = pd.read_csv(\u0026#39;./DATA/UQAM_DATA_STATION_2018.csv\u0026#39;) dataframe_UQAM[\u0026#39;Date\u0026#39;]=pd.to_datetime(dataframe_UQAM[\u0026#39;Date\u0026#39;]) dataframe_UQAM = dataframe_UQAM.set_index(\u0026#34;Date\u0026#34;, drop=True) dataframe_UQAM.head()    Date Temperature minimale Temperature maximale Temperature moyenne Precipitation totale Dir_wind Mod_wind     2014-02-01 00:00:00 -4.6 0.9 -1.5 0 244 4   2014-02-02 00:00:00 -6.2 0.2 -3.5 0 163 2   2014-02-03 00:00:00 -4.8 0.5 -1.2 0 255 3   2014-02-04 00:00:00 -9.9 -4.8 -8 0 148 2   2014-02-05 00:00:00 -9.9 -6.3 -7.6 0 261 2    Introduction to Matplotlib conda install matplotlib By running this special iPython command, we will be displaying plots inline:\n%matplotlib inline import matplotlib.pyplot as plt plt.plot() # you create an empty graph or instance and then add layers. plt.show() Add data to our charts dataframe_UQAM[\u0026#39;2015\u0026#39;].head()    Date Temperature minimale Temperature maximale Temperature moyenne Precipitation totale Dir_wind Mod_wind     2015-01-01 00:00:00 -13.3 -6.4 -9.5 0 254 3   2015-01-02 00:00:00 -7.3 -3 -4.9 0 231 3   2015-01-03 00:00:00 -12 -3.2 -7.8 0 282 4   2015-01-04 00:00:00 -15.3 -8.7 -12.5 0 113 2   2015-01-05 00:00:00 -8.8 3.9 -2.6 0 178 3    year_to_plot = dataframe_UQAM[\u0026#39;2015\u0026#39;] plt.plot(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;]) plt.show() year_to_plot = dataframe_UQAM[\u0026#39;2015\u0026#39;] plt.plot(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;], marker=\u0026#39;x\u0026#39;) plt.show()  markeroption :  https://matplotlib.org/api/markers_api.html\n linestyleoption: to delete or not the lines  year_to_plot = dataframe_UQAM[\u0026#39;2015\u0026#39;] plt.plot(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;], marker=\u0026#39;x\u0026#39;, linestyle=\u0026#34;--\u0026#34;) plt.show()  scatter fonction: function allows you to create scatter plot .rcParamsmethod is used to enlarge the graphic window  plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] plt.scatter(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;]) plt.show()  to use the scatter color option, python wants an input list and not a dictionary color option : c=list()  plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] plt.scatter(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;], c=list(year_to_plot[\u0026#39;Temperature moyenne\u0026#39;])) plt.xlabel(\u0026#34;Temps\u0026#34;) plt.ylabel(\u0026#34;Température\u0026#34;) plt.title(\u0026#34;Temperature\u0026#34;, y=1.05) plt.show()  We used default scatter color () with the cmap option, we can choose our color panel: https://matplotlib.org/examples/color/colormaps_reference.html we will for example choose the color palette \u0026ldquo;seismic\u0026rdquo; via the option cmap  to change the shape and size of the points: use the marker and s options to save a graph: pyplot function: savefig ()  To add a color bar: colorbar () function To rotate the labels in x: function xticks ()   plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] plt.scatter(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;], c=list(year_to_plot[\u0026#39;Temperature moyenne\u0026#39;]), cmap=\u0026#34;seismic\u0026#34;, marker=\u0026#34;D\u0026#34;, s=100) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Temperature\u0026#34;) plt.title(\u0026#34;Temperature\u0026#34;, y=1.05) plt.colorbar() plt.xticks(rotation=45) plt.show() plt.savefig(\u0026#34;figures/my_graph.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) \u0026lt;Figure size 1152x648 with 0 Axes\u0026gt;  Matplotlib classes When creating a graph, matplotlib:\n stores a container for all the graphics stores a container so that the graphic is positioned on a grid stores visual symbols on the graph  You can plot different things in the same figure using the subplot function. Here is an example:\nfig = plt.figure() ax1 = fig.add_subplot(2,2,1) # up and left  ax2 = fig.add_subplot(2,2,2) # up and right  ax3 = fig.add_subplot(2,2,3) # down and left  ax4 = fig.add_subplot(2,2,4) # down and right plt.show() Here is a documentation on subplot: https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot\nAdd data fig = plt.figure() ax1 = fig.add_subplot(2,1,1) ax2 = fig.add_subplot(2,1,2) ax1.plot(dataframe_UQAM[\u0026#39;2017\u0026#39;].index,dataframe_UQAM[\u0026#39;2017\u0026#39;][\u0026#39;Temperature moyenne\u0026#39;]) ax2.plot(dataframe_UQAM[\u0026#39;2016\u0026#39;].index,dataframe_UQAM[\u0026#39;2016\u0026#39;][\u0026#39;Temperature moyenne\u0026#39;]) [\u0026lt;matplotlib.lines.Line2D at 0xa516dd8\u0026gt;]  Improvement of the graph fig = plt.figure(figsize=(15, 8)) colors = [\u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;] for i in range(3): ax = fig.add_subplot(3,1,i+1) year = str(2014+i) label=year plt.plot(dataframe_UQAM[year].index,dataframe_UQAM[year][\u0026#39;Temperature moyenne\u0026#39;], c=colors[i], label = label) plt.legend(loc=\u0026#39;upper left\u0026#39;) plt.show() Example Objectives: to draw a meteogram of the UQAM station for the day of 14/12/2018 import pandas as pd dataframe_UQAM = pd.read_csv(\u0026#39;./DATA/UQAM_DATA_STATION.csv\u0026#39;) print(dataframe_UQAM.head())  Time Precipitation pressure humidex Rosee Temperature Chill \\ 0 18-12-09_09 0.14 1038.0 -8.0 -7.8 -5.1 -8.0 1 18-12-09_10 0.00 1039.0 -7.0 -6.7 -3.9 -7.0 2 18-12-09_11 0.00 1033.0 -5.0 -6.0 -2.7 -5.0 3 18-12-09_12 0.00 1018.0 -2.0 -5.8 -0.2 -3.0 4 18-12-09_13 0.00 1106.0 -2.0 -6.0 0.0 -5.0 Humidite Dir_wind Mod_wind 0 81.0 208.0 6.0 1 81.0 201.0 6.0 2 78.0 184.0 5.0 3 66.0 208.0 8.0 4 64.0 268.0 18.0  dataframe_UQAM2 = pd.read_csv(\u0026#39;./DATA/UQAM_DATA_STATION.csv\u0026#39;, parse_dates=[\u0026#34;Time\u0026#34;],date_parser=lambda x: pd.to_datetime(x, format=\u0026#34;%y-%m-%d_%H\u0026#34;)) dataframe_UQAM2.head()     Time Precipitation pressure humidex Rosee Temperature Chill Humidite Dir_wind Mod_wind     0 2018-12-09 09:00:00 0.14 1038 -8 -7.8 -5.1 -8 81 208 6   1 2018-12-09 10:00:00 0 1039 -7 -6.7 -3.9 -7 81 201 6   2 2018-12-09 11:00:00 0 1033 -5 -6 -2.7 -5 78 184 5   3 2018-12-09 12:00:00 0 1018 -2 -5.8 -0.2 -3 66 208 8   4 2018-12-09 13:00:00 0 1106 -2 -6 0 -5 64 268 18    # We just want data for 14/12/2018 start_date = \u0026#39;2018-12-14\u0026#39; end_date = \u0026#39;2018-12-15\u0026#39; df= dataframe_UQAM2 mask = (df[\u0026#39;Time\u0026#39;] \u0026gt; start_date) \u0026amp; (df[\u0026#39;Time\u0026#39;] \u0026lt;= end_date) dataframe_jour = dataframe_UQAM2.loc[mask] print(dataframe_jour.head()) print(len(dataframe_jour))  Time Precipitation pressure humidex Rosee Temperature \\ 112 2018-12-14 01:00:00 0.0 1039.0 -10.0 -10.0 -6.3 113 2018-12-14 02:00:00 0.0 1039.0 -9.0 -9.0 -5.8 114 2018-12-14 03:00:00 0.0 1036.0 -9.0 -8.5 -5.4 115 2018-12-14 04:00:00 0.0 1036.0 -9.0 -8.4 -5.3 116 2018-12-14 05:00:00 0.0 1037.0 -8.0 -7.5 -4.8 Chill Humidite Dir_wind Mod_wind 112 -8.0 75.0 208.0 4.0 113 -8.0 78.0 117.0 4.0 114 -7.0 79.0 89.0 4.0 115 -7.0 79.0 79.0 3.0 116 -7.0 81.0 92.0 4.0 24  from matplotlib.font_manager import FontProperties import matplotlib.dates as mdates from datetime import datetime from datetime import timedelta as td daylabels = [] i = 0 for index, row in dataframe_jour.iterrows(): daylabels.append(dataframe_jour.iloc[i][0].replace(minute=0, second=0, microsecond=0).strftime(\u0026#39;%Hh\u0026#39;)) i += 1 print(daylabels) start_date = \u0026#39;2018-12-14\u0026#39; end_date = \u0026#39;2018-12-15\u0026#39; fig = plt.figure(figsize=(20, 8)) fontP = FontProperties() fontP.set_size(\u0026#39;xx-small\u0026#39;) t = np.arange(0, 24, 1) ##### on trace les températures ax1 = plt.subplot(411) ax1.grid(True) plt.plot(t, dataframe_jour[\u0026#39;Temperature\u0026#39;], \u0026#39;r-\u0026#39;, label=\u0026#39;Temperature de l\\\u0026#39;air\u0026#39;, linewidth=2) plt.plot(t, dataframe_jour[\u0026#39;Rosee\u0026#39;], \u0026#39;r--\u0026#39;,label=\u0026#39;Temperature du point de rosee\u0026#39;, linewidth=2) plt.plot(t, dataframe_jour[\u0026#39;Chill\u0026#39;], \u0026#39;g--\u0026#39;,label=\u0026#39;Wind Chill\u0026#39;, linewidth=2) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 1, 1, 0),fontsize =15) plt.ylabel(\u0026#39;Température ($^\\circ$C)\u0026#39;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 15}) plt.setp(ax1.get_xticklabels(), fontsize=15) plt.title(\u0026#39;Meteogram station UQAM:\u0026#39;+ start_date + \u0026#39;/ \u0026#39; + end_date, weight=\u0026#39;bold\u0026#39;).set_fontsize(\u0026#39;20\u0026#39;) plt.setp(ax1.get_xticklabels(), visible=False) ########## TRACE DES ACCUMULATIONS PRECIPITATION ##########  # share x only ax2 = plt.subplot(412, sharex=ax1) ax2.grid(True) t = np.arange(0, 24, 1) width=1 ax2.bar(t,dataframe_jour[\u0026#39;Precipitation\u0026#39;].values,width,color=\u0026#39;b\u0026#39;, label=\u0026#39;Pluie\u0026#39;, linewidth=2) plt.setp(ax2.get_xticklabels(), visible=False) plt.ylim(0, np.round(dataframe_jour[\u0026#39;Precipitation\u0026#39;].max() ) +1) plt.ylabel(\u0026#39;Accumulation (mm)\u0026#39;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 15}) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 0, 1, 1), fontsize =15) # share x only ##### on trace l humidite ax2 = plt.subplot(413, sharex=ax1) ax2.grid(True) plt.plot(t, dataframe_jour[\u0026#39;Humidite\u0026#39;], \u0026#39;b-\u0026#39;, linewidth=2) plt.setp(ax2.get_xticklabels(), visible=False) plt.ylabel(\u0026#39;Humidite relative (%)\u0026#39;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 15}) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 0, 1, 1), fontsize =15) ##### on trace la pression ax3 = plt.subplot(414, sharex=ax1) plt.plot(t, dataframe_jour[\u0026#39;pressure\u0026#39;], \u0026#39;g-\u0026#39;, linewidth=2) plt.xlim(0.01, 24) plt.ylim(np.round(min(dataframe_jour[\u0026#39;pressure\u0026#39;])) - 2, np.round(max(dataframe_jour[\u0026#39;pressure\u0026#39;])) + 2) plt.ylabel(\u0026#39;Pression (hPa)\u0026#39;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 15}) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 0, 1, 1), fontsize =15) ax3.grid(True) for label in ax3.get_yticklabels(): label.set_color(\u0026#34;black\u0026#34;) ax3.set(xticks=np.arange(0,len(daylabels),1), xticklabels=daylabels) #Same as plt.xticks spacing = 1 visible = ax3.xaxis.get_ticklabels()[::spacing] for label in ax3.xaxis.get_ticklabels(): if label not in visible: label.set_visible(False) fig.autofmt_xdate() fig.set_size_inches(18.5, 10.5) fileout=\u0026#39;figures/Meteogram_UQAM.png\u0026#39; plt.savefig(fileout) fig.autofmt_xdate() plt.show() ['01h', '02h', '03h', '04h', '05h', '06h', '07h', '08h', '09h', '10h', '11h', '12h', '13h', '14h', '15h', '16h', '17h', '18h', '19h', '20h', '21h', '22h', '23h', '00h']  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"26fc5414d1b2e3baeb0d5ef9e55d56ff","permalink":"/courses/tutorial_python/8-matplotlib_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/8-matplotlib_library/","section":"courses","summary":"The Matplotlib library is one of the most used libraries for plotting data in Python.\nMany types of graphics can be developed with this library: https://matplotlib.org/gallery/index.html\nTo illustrate some features of the Python matplotlib.pyplot module (which provides a plotting system similar to that of MATLAB), we will use a database from the UQAM station.\nimport pandas as pd import numpy as np import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) dataframe_UQAM = pd.read_csv(\u0026#39;./DATA/UQAM_DATA_STATION_2018.csv\u0026#39;) dataframe_UQAM[\u0026#39;Date\u0026#39;]=pd.to_datetime(dataframe_UQAM[\u0026#39;Date\u0026#39;]) dataframe_UQAM = dataframe_UQAM.","tags":null,"title":"8 Matplotlib library","type":"docs"},{"authors":null,"categories":null,"content":"1: ECCC temperature data In previous sections, we presented how to use the Pandas library which allowed us to process and manipulate data sets. Combining this with Python's Datetime and Matplotlib libraries, we were able to quickly visualize our data.\nWe will continue to discover the functionality of these libraries in a practical case by analyzing the daily temperature data recorded by one of the Environment and Climate Change Canada stations located in Montreal / McTavish between the period 1948 and 2017 (file named \u0026lsquo;MONTREAL_tasmoy_1948_2017.txt\u0026rsquo; in ./DATA directory)\nTo complete and enrich our analysis, a new Python library will be presented: Seaborn .\n the Seaborn library is based on matplotlib. it allows to draw more complex graphs  For more information:\nhttps://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html\nThis link presents a gallery of chart types to be realized with Seaborn: https://seaborn.pydata.org/examples/index.html\n1- Opening and reading our time series import numpy as np import pandas as pd import datetime from datetime import date import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) df = pd.DataFrame() # We open ascii file and store information in a new DataFrame with open(\u0026#39;./DATA/MONTREAL_tasmoy_1948_2017.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() data_EC_Montreal = [float(row) for row in rows.split()] We have created a 1D field but we have no temporal information in our file. Knowing that our registration covers the period 1948 - 2017, we will format our dates with the datetime module of Python.\n# We know that the time series starts on January 1, 1948 and ends on December 31, 2017 inclusively # We create a Datetime object instance to complete our DataFrame start = date(1948, 1, 1) end = date(2017, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) # We will use DateTime object as DataFrame index df[\u0026#39;datetime\u0026#39;] = rng df.index = df[\u0026#39;datetime\u0026#39;] df[\u0026#39;Temperature Montreal\u0026#39;] = data_EC_Montreal df.head()    datetime datetime Temperature Montreal     1948-01-01 00:00:00 1948-01-01 00:00:00 -12   1948-01-02 00:00:00 1948-01-02 00:00:00 -8.9   1948-01-03 00:00:00 1948-01-03 00:00:00 -3.4   1948-01-04 00:00:00 1948-01-04 00:00:00 -3.4   1948-01-05 00:00:00 1948-01-05 00:00:00 -3.1    dir(df) # the dir () function allows to list the functions that are applicable to our DataFrame object We assigned our time series in a Pandas DataFrame and then formatted the date as a Datetime object.\nIt is now easy to manipulate the dataset and apply some simple functions.\n2- Calculation of indices on the temperature data   We will develop and apply a function to calculate the quantiles of our distribution\n  By resampling our series with the .resample () method of Pandas, we will see how to apply native functions of numpy and apply our own function.\n  # Creating our index that calculates the quantile of the distribution # We use the numpy .percentile () function def percentile(n): def percentile_(x): return np.nanpercentile(x, n) percentile_.__name__ = \u0026#39;percentile_%s\u0026#39; % n return percentile_  .resample () and .agg () methods of Pandas  The .resample () method is very useful for frequency conversion and time series resampling. The object (here DataFrame) must have a data / time index (DatetimeIndex) in order to be used. Several resampling frequencies are available (time, week, month, season, year \u0026hellip;)\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.resample.html\nThe .agg () method is used for aggregation of data according to a list of functions to be applied to each column, resulting in an aggregated result with a hierarchical index.\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html\nIn our example, we will resample our data set by month and calculate for each month the average, the minimum, the maximum and the 90th and 95th quantiles.\nresamp_Montreal = df.resample(\u0026#39;M\u0026#39;).agg([np.mean, np.min, np.max, percentile(90), percentile(95)]) resamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo month     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4 1948 1 Jan   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32 1948 2 Feb   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4 1948 3 Mar   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13 1948 4 Apr   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35 1948 5 May    # simple step to remove the row \u0026#39;Temperature Montreal\u0026#39;  resamp_Montreal = resamp_Montreal.loc[:,\u0026#39;Temperature Montreal\u0026#39;] resamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35    3- Some examples of graphics with the Seaborn library Now that we have some statistics on our DataFrame, we will use Python's Seaborn library to visualize them.\n Example: Heatmap  https://seaborn.pydata.org/generated/seaborn.heatmap.html\nFor example, we would like to observe the variation of the average temperature for all the months of the year and all the years. We are going to define two new columns in our DataFrame in which will be assigned only the years and the months respectively.\nresamp_Montreal[\u0026#39;year\u0026#39;] = resamp_Montreal.index.year resamp_Montreal[\u0026#39;MonthNo\u0026#39;] = resamp_Montreal.index.month resamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4 1948 1   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32 1948 2   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4 1948 3   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13 1948 4   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35 1948 5    Before we plot our heatmap, we need to reorganize our dataframe.\n The .pivot_table () method: this method allows you to cross tables dynamically.  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html\nWe would like to visualize only the average monthly temperatures, so work with the \u0026lsquo;mean\u0026rsquo; column, put the year in Index and have one month per column. The .pivot_table () method allows us to do this.\nMontreal_pivot = resamp_Montreal.pivot_table(values=\u0026#39;mean\u0026#39;,index=\u0026#39;year\u0026#39;,columns=[\u0026#39;MonthNo\u0026#39;]) Montreal_pivot.head()    year 1 2 3 4 5 6 7 8 9 10 11 12     1948 -10.9903 -9.58621 -2.46129 6.64 12.6645 17.7467 21.4903 21.1065 17.15 9.1 5.98333 -2.52258   1949 -5.77419 -5.69286 -2.19677 7.51667 13.5129 20.56 22.871 21.5452 14.62 12.0258 -0.193333 -3.21935   1950 -5.34839 -9.79643 -4.89355 4.71 14.1161 18.9567 21.3258 19.1129 13.6167 10.1 3.95667 -4.83548   1951 -7.35484 -6.48571 -0.490323 7.29667 14.4097 18.27 21.3129 18.6677 15.1333 10.371 0.18 -5.3129   1952 -7.80323 -5.27241 -0.987097 7.99667 12.4065 19.6 23.0806 20.7323 16.03 7.67419 3.65667 -3.18387    We then apply .heatmap()function on our DataFrame.\nimport seaborn as sns import matplotlib.pyplot as plt ax = plt.axes() sns.heatmap(Montreal_pivot) figure = ax.get_figure() figure.set_size_inches(15, 10) plt.show() We can improve our display.\nax = plt.axes() sns.heatmap(Montreal_pivot, cmap=\u0026#39;RdYlGn_r\u0026#39;, linewidths=0.5, annot=True , ax = ax,vmin=-30, vmax=30,center=0, fmt=\u0026#39;.1f\u0026#39;,yticklabels=True, cbar_kws={\u0026#39;label\u0026#39;: \u0026#39;Celcius\u0026#39;}) ax.set_title(\u0026#39;Mean temperature\u0026#39;, weight=\u0026#39;bold\u0026#39;, fontsize=\u0026#34;x-large\u0026#34;) figure = ax.get_figure() figure.set_size_inches(22, 15) plt.show()  Other examples: Boxplot, violin plot, line plot  https://seaborn.pydata.org/generated/seaborn.boxplot.html\nhttps://seaborn.pydata.org/generated/seaborn.violinplot.html\nhttps://seaborn.pydata.org/generated/seaborn.lineplot.html\nAt first, we create a new variable containing the months but in string of characters. For this we apply the .strftime () method of datetime.\nresamp_Montreal[\u0026#39;month\u0026#39;] = resamp_Montreal.index.strftime(\u0026#34;%b\u0026#34;) resamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo month     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4 1948 1 Jan   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32 1948 2 Feb   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4 1948 3 Mar   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13 1948 4 Apr   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35 1948 5 May     Boxplot:  ax = plt.axes() sns.boxplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;mean\u0026#34;, data=resamp_Montreal, palette=\u0026#34;Set1\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show()  Violin plot:  ax = plt.axes() sns.violinplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;mean\u0026#34;, data=resamp_Montreal, palette=\u0026#34;Set1\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show()  Line plot:  ax = plt.axes() sns.lineplot(x=resamp_Montreal.index.year, y=\u0026#34;mean\u0026#34;, hue=\u0026#34;month\u0026#34;, data=resamp_Montreal, palette=\u0026#34;tab10\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show() sns.catplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;percentile_90\u0026#34;, data=resamp_Montreal, kind=\u0026#34;swarm\u0026#34;) plt.show()  We can combine several Seaborn charts:  ax = plt.axes() ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2) sns.lineplot(x=resamp_Montreal.index.year, y=\u0026#34;mean\u0026#34;, hue=\u0026#34;month\u0026#34;, data=resamp_Montreal, palette=\u0026#34;tab10\u0026#34;) ax2 = plt.subplot2grid((2, 2), (1, 0), colspan=1) sns.boxplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;mean\u0026#34;, data=resamp_Montreal, palette=\u0026#34;tab10\u0026#34;) ax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1) sns.violinplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;mean\u0026#34;, data=resamp_Montreal, palette=\u0026#34;tab10\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show() 4- Fonction groupby We previously see .groupby()method from Pandas.\nhttps://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm\nThis method is very useful using datetime objects.\nresamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo month     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4 1948 1 Jan   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32 1948 2 Feb   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4 1948 3 Mar   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13 1948 4 Apr   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35 1948 5 May    We want to group our dataframe by month with .groupeby()method.\nresamp_Montreal_grouped = resamp_Montreal.groupby(\u0026#34;month\u0026#34;) resamp_Montreal_grouped.groups # Pour voir les groupes {'Apr': DatetimeIndex(['1948-04-30', '1949-04-30', '1950-04-30', '1951-04-30', '1952-04-30', '1953-04-30', '1954-04-30', '1955-04-30', '1956-04-30', '1957-04-30', '1958-04-30', '1959-04-30', '1960-04-30', '1961-04-30', '1962-04-30', '1963-04-30', '1964-04-30', '1965-04-30', '1966-04-30', '1967-04-30', '1968-04-30', '1969-04-30', '1970-04-30', '1971-04-30', '1972-04-30', '1973-04-30', '1974-04-30', '1975-04-30', '1976-04-30', '1977-04-30', '1978-04-30', '1979-04-30', '1980-04-30', '1981-04-30', '1982-04-30', '1983-04-30', '1984-04-30', '1985-04-30', '1986-04-30', '1987-04-30', '1988-04-30', '1989-04-30', '1990-04-30', '1991-04-30', '1992-04-30', '1993-04-30', '1994-04-30', '1995-04-30', '1996-04-30', '1997-04-30', '1998-04-30', '1999-04-30', '2000-04-30', '2001-04-30', '2002-04-30', '2003-04-30', '2004-04-30', '2005-04-30', '2006-04-30', '2007-04-30', '2008-04-30', '2009-04-30', '2010-04-30', '2011-04-30', '2012-04-30', '2013-04-30', '2014-04-30', '2015-04-30', '2016-04-30', '2017-04-30'], dtype='datetime64[ns]', name='datetime', freq='12M'), 'Aug': DatetimeIndex(['1948-08-31', '1949-08-31', '1950-08-31', '1951-08-31', '1952-08-31', '1953-08-31', '1954-08-31', '1955-08-31', '1956-08-31', '1957-08-31', '1958-08-31', '1959-08-31', '1960-08-31', '1961-08-31', '1962-08-31', '1963-08-31', '1964-08-31', '1965-08-31', '1966-08-31', '1967-08-31', '1968-08-31', '1969-08-31', '1970-08-31', '1971-08-31', '1972-08-31', '1973-08-31', '1974-08-31', '1975-08-31', '1976-08-31', '1977-08-31', '1978-08-31', '1979-08-31', '1980-08-31', '1981-08-31', '1982-08-31', '1983-08-31', '1984-08-31', '1985-08-31', '1986-08-31', '1987-08-31', '1988-08-31', '1989-08-31', '1990-08-31', '1991-08-31', '1992-08-31', '1993-08-31', '1994-08-31', '1995-08-31', '1996-08-31', '1997-08-31', '1998-08-31', '1999-08-31', '2000-08-31', '2001-08-31', '2002-08-31', '2003-08-31', '2004-08-31', '2005-08-31', '2006-08-31', '2007-08-31', '2008-08-31', '2009-08-31', '2010-08-31', '2011-08-31', '2012-08-31', '2013-08-31', '2014-08-31', '2015-08-31', '2016-08-31', '2017-08-31'], ... }   To iterate through groups:  for MonthNo,group in resamp_Montreal_grouped : print(MonthNo) print(group) Apr mean amin amax percentile_90 percentile_95 year \\ datetime 1948-04-30 6.640000 -1.7 14.5 12.08 13.130 1948 1949-04-30 7.516667 2.0 15.8 12.43 14.370 1949 1950-04-30 4.710000 -4.2 11.7 8.56 10.770 1950 1951-04-30 7.296667 1.4 15.0 11.78 13.105 1951 1952-04-30 7.996667 -1.4 15.3 14.01 15.000 1952 ... [70 rows x 8 columns] Aug mean amin amax percentile_90 percentile_95 year \\ datetime 1948-08-31 21.106452 15.0 27.8 26.40 27.400 1948 1949-08-31 21.545161 15.0 27.0 25.60 26.700 1949 1950-08-31 19.112903 12.5 23.4 22.50 23.050 1950 1951-08-31 18.667742 13.1 23.4 22.00 22.950 1951 1952-08-31 20.732258 14.2 25.6 23.40 25.300 1952 1953-08-31 20.480645 15.6 27.2 25.00 27.000 1953 1954-08-31 19.312903 12.8 24.5 22.00 23.350 1954 ...   To select a group: get_group()  # we grouped our dataframe using month index resamp_Montreal_grouped.get_group(\u0026#39;Dec\u0026#39;).head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo month     1948-12-31 00:00:00 -2.52258 -15 7 2.2 3.9 1948 12 Dec   1949-12-31 00:00:00 -3.21935 -13.9 8.9 5.8 6.7 1949 12 Dec   1950-12-31 00:00:00 -4.83548 -23.4 3.9 0 1.95 1950 12 Dec   1951-12-31 00:00:00 -5.3129 -19.2 13.1 7 11.15 1951 12 Dec   1952-12-31 00:00:00 -3.18387 -17 7.8 2 4.6 1952 12 Dec     Aggregation: Python has several methods are available to perform aggregations on data. It is done using the pandas and numpy libraries. The data must be available or converted to a dataframe to apply the aggregation functions.  # to calculate \u0026#39;percentile_90\u0026#39; in each group.  resamp_Montreal_grouped[\u0026#39;percentile_90\u0026#39;].agg(np.mean) month Apr 12.434848 Aug 24.283582 Dec 1.790000 Feb -0.098529 Jan -1.046324 Jul 25.119242 Jun 23.557385 Mar 5.021493 May 19.483846 Nov 8.615147 Oct 15.280294 Sep 21.236765 Name: percentile_90, dtype: float64  resamp_Montreal_grouped[\u0026#39;percentile_90\u0026#39;].agg(np.size) month Apr 70.0 Aug 70.0 Dec 70.0 Feb 70.0 Jan 70.0 Jul 70.0 Jun 70.0 Mar 70.0 May 70.0 Nov 70.0 Oct 70.0 Sep 70.0 Name: percentile_90, dtype: float64  Exercice : Compute day degres index for Montreal station 1- open and read file \u0026ldquo;MONTREAL_tasmoy_1948_2017.txt\u0026rdquo;\n2- Define datetime index (time range: 01/01/1948 : 31/12/2017 )\n3- Write a function to compute the index\n4- For each year, compute temperature mean, min , max and DG0 index\n5- Make a plot\n1-\nimport numpy as np with open(\u0026#39;./data/MONTREAL_tasmoy_1948_2017.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() data_EC_Montreal = [float(row) for row in rows.split()] 2-\nimport pandas as pd import datetime from datetime import date df = pd.DataFrame() start = date(1948, 1, 1) end = date(2017, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) df[\u0026#39;datetime\u0026#39;] = rng df.index = df[\u0026#39;datetime\u0026#39;] df[\u0026#39;Temperature Montreal\u0026#39;] = data_EC_Montreal 3-\ndef DG0(S): ind_DGO=[] ind_DGO = sum(x for x in S if x \u0026gt;= 0) return ind_DGO 4-\nresamp = df.resample(\u0026#39;AS\u0026#39;) dataset = resamp.agg([np.mean, np.min, np.max, DG0]) dataset.head()    datetime (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;mean\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;amin\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;amax\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;DG0\u0026rsquo;)     1948-01-01 00:00:00 7.23388 -23.4 27.8 3506.1   1949-01-01 00:00:00 8.04767 -15 28.4 3602.9   1950-01-01 00:00:00 6.84877 -23.6 25.9 3318.9   1951-01-01 00:00:00 7.24521 -24.2 25.3 3404.9   1952-01-01 00:00:00 7.85546 -20 27.5 3474.2    5-\ndataset.columns = dataset.columns.droplevel(0) dataset[\u0026#39;DG0\u0026#39;].head() datetime 1948-01-01 3506.1 1949-01-01 3602.9 1950-01-01 3318.9 1951-01-01 3404.9 1952-01-01 3474.2 Freq: AS-JAN, Name: DG0, dtype: float64  import matplotlib.pyplot as plt dataset[\u0026#39;DG0\u0026#39;].plot(figsize=(12,5)) plt.show() # we must filter missing values def DG0bis(S): ind_DGObis=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if ((N2/N) \u0026lt; 0.8): ind_DGObis = np.empty(1) ind_DGObis = np.nan else: ind_DGObis = sum(x for x in S if x \u0026gt;= 0) return ind_DGObis dataset = resamp.agg([np.mean, np.min, np.max, DG0, DG0bis]) dataset.head()    datetime (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;mean\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;amin\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;amax\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;DG0\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;DG0bis\u0026rsquo;)     1948-01-01 00:00:00 7.23388 -23.4 27.8 3506.1 3506.1   1949-01-01 00:00:00 8.04767 -15 28.4 3602.9 3602.9   1950-01-01 00:00:00 6.84877 -23.6 25.9 3318.9 3318.9   1951-01-01 00:00:00 7.24521 -24.2 25.3 3404.9 3404.9   1952-01-01 00:00:00 7.85546 -20 27.5 3474.2 3474.2    dataset.columns = dataset.columns.droplevel(0) dataset[\u0026#39;DG0\u0026#39;].plot(figsize=(12,5)) dataset[\u0026#39;DG0bis\u0026#39;].plot(figsize=(12,5)) plt.show() ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"cec6dfc0e49e5bd66e97e1ef6b926e47","permalink":"/courses/tutorial_python/project1-eccc_temperature/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/project1-eccc_temperature/","section":"courses","summary":"1: ECCC temperature data In previous sections, we presented how to use the Pandas library which allowed us to process and manipulate data sets. Combining this with Python's Datetime and Matplotlib libraries, we were able to quickly visualize our data.\nWe will continue to discover the functionality of these libraries in a practical case by analyzing the daily temperature data recorded by one of the Environment and Climate Change Canada stations located in Montreal / McTavish between the period 1948 and 2017 (file named \u0026lsquo;MONTREAL_tasmoy_1948_2017.","tags":null,"title":"Project 1 ECCC temperature data","type":"docs"},{"authors":null,"categories":null,"content":"ECCC Precipitation Data We will continue to discover the functionality of these libraries in a practical case by now analyzing the daily precipitation data recorded by the ECCC stations located at the Montreal Trudeau Airport between the period 1961 and 2010.\nWe will use \u0026lsquo;MONTREAL_preacc_1961_2010.dat\u0026rsquo; file in .data/ directory.\n1- Opening and reading our time series import numpy as np import pandas as pd import datetime from datetime import date import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) df_Precipitation = pd.DataFrame() # Ouverture du fichier text  with open(\u0026#39;./DATA/MONTREAL_preacc_1961_2010.dat\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() data_EC_Montreal = [float(row) for row in rows.split()] # We know that the time series begins on January 1, 1961 # and ends on December 31, 2010 # We can rebuild the index of Dataframes start = date(1961, 1, 1) end = date(2010, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) # Create a column from the datetime datatype df_Precipitation[\u0026#39;datetime\u0026#39;] = rng # Set the datetime column as the index df_Precipitation.index = df_Precipitation[\u0026#39;datetime\u0026#39;] # Create a column from the numeric score variable df_Precipitation[\u0026#39;Precipitation Montreal\u0026#39;] = data_EC_Montreal df_Precipitation.head()    datetime datetime Precipitation Montreal     1961-01-01 00:00:00 1961-01-01 00:00:00 22.66   1961-01-02 00:00:00 1961-01-02 00:00:00 1.05   1961-01-03 00:00:00 1961-01-03 00:00:00 1.05   1961-01-04 00:00:00 1961-01-04 00:00:00 0.66   1961-01-05 00:00:00 1961-01-05 00:00:00 0    2- Applying functions to DataFram: precipitation indices:  CDD: calculation of the maximum number of consecutive dry days (Precipitation \u0026lt;1mm) CWD: calculation of the maximum number of consecutive wet days (Precipitation\u0026gt; 1mm) Prcp1: calculation of the percentage of precipitation days (Precipitation\u0026gt; 1mm) SDII: calculation of the precipitation intensity average for wet days (Precipitation\u0026gt; 1mm  # calculation of the maximum number of consecutive dry days (inf to 1mm)  # on the incoming signal (less than 20% of missing values)  def CDD(S): import numpy as np ind_CDD=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if ((N2/N) \u0026lt; 0.8): ind_CDD = np.empty(1) ind_CDD = np.nan else: temp = 0 ind_CDD = 0 j =0 while (j \u0026lt; N2): while (j \u0026lt; N2 ) and (S_no_nan[j] \u0026lt; 1.0 ): j += 1 temp +=1 if ind_CDD \u0026lt; temp: ind_CDD = temp temp = 0 j += 1 return ind_CDD # calculation of the maximum number of consecutive wet days (sup to 1mm)  # on the incoming signal (less than 20% of missing values)  def CWD(S): import numpy as np ind_CWD=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if ((N2/N) \u0026lt; 0.8): ind_CWD = np.empty(1) ind_CWD = np.nan else: temp = 0 ind_CWD = 0 j =0 while (j \u0026lt; N2): while (j \u0026lt; N2 ) and (S_no_nan[j] \u0026gt; 1.0 ): j += 1 temp +=1 if ind_CWD \u0026lt; temp: ind_CWD = temp temp = 0 j += 1 return ind_CWD # Calculation of the percentage of precipitation days  # on the incoming signal (less than 20% of missing values) def Prcp1(S): import numpy as np ind_Prcp1=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if (N2 == 0): N2=1 if ((N2/N) \u0026lt; 0.8): ind_Prcp1 = np.empty(1) ind_Prcp1 = np.nan else: ind_Prcp1 = 0 for row in S_no_nan: if row \u0026gt;= 1 : ind_Prcp1 += 1 ind_Prcp1 = 100 * (ind_Prcp1/N2) return ind_Prcp1 # average precipitation intensity for wet days (PR greater than 1mm) (less than 20% missing values) def SDII(S): import numpy as np ind_SDII=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if ((N2/N) \u0026lt; 0.8): ind_SDII = np.empty(1) ind_SDII = np.nan else: SS = S[S \u0026gt; 1] ind_SDII = np.nanmean(SS) return ind_SDII The previous functions make it possible to calculate precipitation indices on a daily signal. We want to apply these functions per month.\nSo we have to work with a subset of our df_Precipitation dataframe grouped by month.\nIn the same way as the previous section on temperatures, we will apply the .resample (\u0026lsquo;M\u0026rsquo;) method: \u0026ldquo;M\u0026rdquo; for months (it's possible here to work at week , season or year scale) As a reminder, at this stage there is no calculation, the data are simply sorted on a monthly basis.\ndf_Precipitation_resamp = df_Precipitation.resample(\u0026#39;M\u0026#39;) df_Precipitation_resamp.count().head()    datetime datetime Precipitation Montreal     1961-01-31 00:00:00 31 31   1961-02-28 00:00:00 28 28   1961-03-31 00:00:00 31 31   1961-04-30 00:00:00 30 30   1961-05-31 00:00:00 31 31    We can now apply our functions defined previously for each month of our DataFrame. For this, we will use the .agg () method on our subset grouped by month.\nindice_precipitation = df_Precipitation_resamp.agg([CDD, CWD, Prcp1, SDII, np.sum, np.mean]) indice_precipitation.head()    datetime (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;CDD\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;CWD\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;Prcp1\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;SDII\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;sum\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;mean\u0026rsquo;)     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516    3- Filter the data directly on a dataframe  the data can be filtered according to a condition on a column.  We will for example only extract precipitation data greater than or equal to 1mm.\nSDII2=df_Precipitation[df_Precipitation[\u0026#34;Precipitation Montreal\u0026#34;]\u0026gt;=1.0] SDII2.head()    datetime datetime Precipitation Montreal     1961-01-01 00:00:00 1961-01-01 00:00:00 22.66   1961-01-02 00:00:00 1961-01-02 00:00:00 1.05   1961-01-03 00:00:00 1961-01-03 00:00:00 1.05   1961-01-06 00:00:00 1961-01-06 00:00:00 8.65   1961-01-07 00:00:00 1961-01-07 00:00:00 4.05    We first created a Boolean type mask on the column \u0026lsquo;Precipitation Montreal\u0026rsquo; which was then applied to our DataFrame.\n(df_Precipitation[\u0026#34;Precipitation Montreal\u0026#34;]\u0026gt;1.0).head() datetime 1961-01-01 True 1961-01-02 True 1961-01-03 True 1961-01-04 False 1961-01-05 False Name: Precipitation Montreal, dtype: bool  We then group our DataFrame by month and we apply a function by calling .agg() method.\nSDII2 = SDII2.resample(\u0026#39;M\u0026#39;).agg({\u0026#39;SDII2\u0026#39;: lambda x: x.mean()}) SDII2.head()    datetime (\u0026lsquo;SDII2\u0026rsquo;, \u0026lsquo;Precipitation Montreal\u0026rsquo;)     1961-01-31 00:00:00 5.69111   1961-02-28 00:00:00 9.895   1961-03-31 00:00:00 11.8688   1961-04-30 00:00:00 6.68333   1961-05-31 00:00:00 5.13    4- Updating our initial DataFrame We want to add to our initial DataFrame the variable SDII2 previously created.\nindice_precipitation[\u0026#34;SDII2\u0026#34;]=SDII2[:].values indice_precipitation.head()    datetime (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;CDD\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;CWD\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;Prcp1\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;SDII\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;sum\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;mean\u0026rsquo;) (\u0026lsquo;SDII2\u0026rsquo;, \u0026lsquo;')     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13    indice_precipitation.columns = indice_precipitation.columns.droplevel(0) indice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean      1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13    Using index datetime column, we want to create new columns with year and month.\nindice_precipitation[\u0026#39;year\u0026#39;] = indice_precipitation.index.year indice_precipitation[\u0026#39;month\u0026#39;] = indice_precipitation.index.month indice_precipitation[\u0026#39;month\u0026#39;] = indice_precipitation.index.strftime(\u0026#39;%b\u0026#39;) indice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean  year month     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111 1961 Jan   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895 1961 Feb   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688 1961 Mar   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333 1961 Apr   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13 1961 May    We want to save our Dataframe indice_precipitation.\nindice_precipitation.to_csv(\u0026#39;indice_precipitation.csv\u0026#39;) 4- Viewing the results We will visualize our final dataframe index_precipitation with the Seaborn library. However, we will see some examples with multi-dimensional graphics.\n Boxplot: boxplot()function  Let's use our precipitation DataFrame:\nindice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean  year month     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111 1961 Jan   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895 1961 Feb   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688 1961 Mar   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333 1961 Apr   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13 1961 May    We want to visualize the nomber of days with precipitation by month. We will then plot Prcp1 variable from our DataFrame.\nimport seaborn as sns import matplotlib.pyplot as plt ax = plt.axes() sns.boxplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;Prcp1\u0026#34;, data=indice_precipitation, palette=\u0026#34;Set1\u0026#34;) ax.set_title(\u0026#39;Number of days with precipitation in Montreal from 1948 to 2017\u0026#39;) ax.set_ylabel(\u0026#39;%\u0026#39;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show()  Histogram: barplot()function  For this example, we will plot the mean accumulation of prepicipitation by month, to do this we must use \u0026lsquo;sum\u0026rsquo; variable.\nax = plt.axes() sns.barplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;sum\u0026#34;, data=indice_precipitation) ax.set_title(\u0026#39;Monthly mean of cumulated precipitation in Montréal from 1948 to 2017\u0026#39;) ax.set_ylabel(\u0026#39;mm\u0026#39;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show()  bivariate graph: jointplot()function  Jointplot displays a relationship between 2 variables (bivariate)\nhttps://seaborn.pydata.org/generated/seaborn.jointplot.html\nsns.jointplot(x=\u0026#34;Prcp1\u0026#34;, y=\u0026#34;sum\u0026#34;, data=indice_precipitation,kind=\u0026#39;reg\u0026#39;, space=0, height=6, ratio=4) plt.show() # kind = \u0026#39;kde\u0026#39;  # kind=\u0026#34;hex\u0026#34; # kind=\u0026#34;reg\u0026#34;  bivariate graph: pairplot()function https://seaborn.pydata.org/generated/seaborn.pairplot.html  # Attributes of interest cols = [\u0026#39;CWD\u0026#39;, \u0026#39;SDII\u0026#39;, \u0026#39;sum\u0026#39;, \u0026#39;Prcp1\u0026#39;, \u0026#39;mean\u0026#39;] pp = sns.pairplot(indice_precipitation[cols], height=1.8, aspect=1.2, markers=\u0026#34;+\u0026#34;, plot_kws=dict(edgecolor=\u0026#34;k\u0026#34;, linewidth=0.5), diag_kws=dict(shade=True), # \u0026#34;diag\u0026#34; adjusts/tunes the diagonal plots diag_kind=\u0026#34;kde\u0026#34;) # use \u0026#34;kde\u0026#34; for diagonal plots fig = pp.fig fig.subplots_adjust(top=0.93, wspace=0.3) fig.suptitle(\u0026#39;Indice de précipitation Pairwise Plots\u0026#39;, fontsize=14, fontweight=\u0026#39;bold\u0026#39;) plt.show()  Correlation matrix: The Pandas library has a method for calculating the correlations between each column of a DataFrame.  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\nindice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean  year month     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111 1961 Jan   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895 1961 Feb   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688 1961 Mar   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333 1961 Apr   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13 1961 May    We first drop \u0026lsquo;year\u0026rsquo; column and then apply .corr() method.\nindice_precipitation2 = indice_precipitation.drop([\u0026#34;year\u0026#34;], axis=1) corr_matrix = indice_precipitation2.corr() corr_matrix     CDD CWD Prcp1 SDII sum mean      CDD 1 -0.288579 -0.687758 0.0253577 -0.480416 -0.481019 0.0251715   CWD -0.288579 1 0.631665 -0.0400715 0.41894 0.417954 -0.0396542   Prcp1 -0.687758 0.631665 1 -0.0645136 0.661685 0.663935 -0.0645538   SDII 0.0253577 -0.0400715 -0.0645136 1 0.659944 0.661113 0.999642   sum -0.480416 0.41894 0.661685 0.659944 1 0.997694 0.660287   mean -0.481019 0.417954 0.663935 0.661113 0.997694 1 0.661422    0.0251715 -0.0396542 -0.0645538 0.999642 0.660287 0.661422 1    We can call the .heatmap () function to visualize our correlation matrix and thus facilitate our interpretations.\nfig, (ax) = plt.subplots(1, 1, figsize=(10,6)) hm = sns.heatmap(corr_matrix, ax=ax, # Axes in which to draw the plot, otherwise use the currently-active Axes. cmap=\u0026#34;coolwarm\u0026#34;, # Color Map. #square=True, # If True, set the Axes aspect to “equal” so each cell will be square-shaped. annot=True, fmt=\u0026#39;.2f\u0026#39;, # String formatting code to use when adding annotations. #annot_kws={\u0026#34;size\u0026#34;: 14}, linewidths=.05) fig.subplots_adjust(top=0.93) fig.suptitle(\u0026#39;Correlation Matrix: Precipitation Indices Montreal Station\u0026#39;, fontsize=14, fontweight=\u0026#39;bold\u0026#39;) plt.show()  Graphiques 3D:  Bonus: add correlation indices with the scipy library The stats module of the Scipy library has many statistical functions. https://docs.scipy.org/doc/scipy/reference/stats.html\ndef corr_pearson(x, y, **kws): r, p = stats.pearsonr(x, y) p_stars = \u0026#39;\u0026#39; if p \u0026lt;= 0.05: p_stars = \u0026#39;*\u0026#39; if p \u0026lt;= 0.01: p_stars = \u0026#39;**\u0026#39; if p \u0026lt;= 0.001: p_stars = \u0026#39;***\u0026#39; # r, _ = stats.spearmanr(x, y) ax = plt.gca() pos = (0, .9) color2=\u0026#39;red\u0026#39; ax.annotate(\u0026#34;Pearson = {:.2f}\u0026#34;.format(r) + p_stars, xy=pos, xycoords=ax.transAxes, color=color2, fontweight=\u0026#39;bold\u0026#39;) def corr_spearman(x, y, **kws): r, p = stats.spearmanr(x, y) p_stars = \u0026#39;\u0026#39; if p \u0026lt;= 0.05: p_stars = \u0026#39;*\u0026#39; if p \u0026lt;= 0.01: p_stars = \u0026#39;**\u0026#39; if p \u0026lt;= 0.001: p_stars = \u0026#39;***\u0026#39; # r, _ = stats.spearmanr(x, y) ax = plt.gca() pos = (0, .6) color2=\u0026#39;darkgreen\u0026#39; ax.annotate(\u0026#34;Spearman = {:.2f}\u0026#34;.format(r) + p_stars, xy=pos, xycoords=ax.transAxes, color=color2, fontweight=\u0026#39;bold\u0026#39;) def corr_kendall(x, y, **kws): r, p = stats.kendalltau(x, y) p_stars = \u0026#39;\u0026#39; if p \u0026lt;= 0.05: p_stars = \u0026#39;*\u0026#39; if p \u0026lt;= 0.01: p_stars = \u0026#39;**\u0026#39; if p \u0026lt;= 0.001: p_stars = \u0026#39;***\u0026#39; # r, _ = stats.spearmanr(x, y) ax = plt.gca() pos = (0, .3) color2=\u0026#39;darkorange\u0026#39; ax.annotate(\u0026#34;Kendall = {:.2f}\u0026#34;.format(r) + p_stars, xy=pos, xycoords=ax.transAxes, color=color2, fontweight=\u0026#39;bold\u0026#39;) indice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean  year month     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111 1961 Jan   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895 1961 Feb   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688 1961 Mar   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333 1961 Apr   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13 1961 May    indice_month=indice_precipitation[[\u0026#34;CWD\u0026#34;, \u0026#34;SDII\u0026#34;, \u0026#34;sum\u0026#34;, \u0026#34;Prcp1\u0026#34;]].loc[(indice_precipitation[\u0026#34;month\u0026#34;]==\u0026#34;Jan\u0026#34;)] from scipy import stats import seaborn as sns; sns.set() g = sns.PairGrid(indice_month[[\u0026#34;CWD\u0026#34;, \u0026#34;SDII\u0026#34;, \u0026#34;sum\u0026#34;, \u0026#34;Prcp1\u0026#34;g]]) g.map_upper(corr_pearson) g.map_upper(corr_spearman) g.map_upper(corr_kendall) g.map_lower(sns.regplot) g.map_diag(plt.hist) g.set(alpha=0.5) plt.show() ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ca3308d7dc0eddd7d50033bd96585e4c","permalink":"/courses/tutorial_python/project2-eccc_precipitation/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/project2-eccc_precipitation/","section":"courses","summary":"ECCC Precipitation Data We will continue to discover the functionality of these libraries in a practical case by now analyzing the daily precipitation data recorded by the ECCC stations located at the Montreal Trudeau Airport between the period 1961 and 2010.\nWe will use \u0026lsquo;MONTREAL_preacc_1961_2010.dat\u0026rsquo; file in .data/ directory.\n1- Opening and reading our time series import numpy as np import pandas as pd import datetime from datetime import date import numpy as np import warnings warnings.","tags":null,"title":"Project 2 ECCC precipitation data","type":"docs"},{"authors":["Victoria Ng","Aamir Fazil","Philippe Gachon","Guillaume Dueymes","Milka Radojević","Mariola Mascarenhas","Sophiya Garasia","Michael A. Johansson","Nicholas H. Ogden"],"categories":["2"],"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"4158d0dfcc50d2770479a4ef0027a9eb","permalink":"/publication/article8/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/article8/","section":"publication","summary":"Background: Chikungunya virus (CHIKV) is a reemerging pathogen transmitted by Aedes aegypti and Aedes albopictus mosquitoes. The ongoing Caribbean outbreak is of concern due to the potential for infected travelers to spread the virus to countries where vectors are present and the population is susceptible. Although there has been no autochthonous transmission of CHIKV in Canada, there is concern that both Ae. albopictus and CHIKV will become established, particularly under projected climate change. We developed risk maps for autochthonous CHIKV transmission in Canada under recent (1981–2010) and projected climate (2011–2040 and 2041–2070). Methods: The risk for CHIKV transmission was the combination of the climatic suitability for CHIKV transmission potential and the climatic suitability for the presence of Ae. albopictus; the former was assessed using a stochastic model to calculate R0 and the latter was assessed by deriving a suitability indicator (SIG) that captures a set of climatic conditions known to influence the ecology of Ae. albopictus. R0 and SIG were calculated for each grid cell in Canada south of 60°N, for each time period and for two emission scenarios, and combined to produce overall risk categories that were mapped to identify areas suitable for transmission and the duration of transmissibility. Findings: The risk for autochthonous CHIKV transmission under recent climate is very low with all of Canada classified as unsuitable or rather unsuitable for transmission. Small parts of southern coastal British Columbia become progressively suitable with short-term and long-term projected climate; the duration of potential transmission is limited to 1–2 months of the year. Interpretation: Although the current risk for autochthonous CHIKV transmission in Canada is very low, our study could be further supported by the routine surveillance of Ae. albopictus in areas identified as potentially suitable for transmission given our uncertainty on the current distribution of this species in Canada.","tags":[],"title":"Assessment of the Probability of Autochthonous Transmission of Chikungunya Virus in Canada under Recent and Projected Climate Change","type":"publication"},{"authors":["E. D. Poan"," 'P. Gachon'"," 'R. Laprise'"," 'R. Aider'"," 'G. Dueymes'"],"categories":["2"],"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"f1739c472160d063dac6ebe9ca6a6ef8","permalink":"/publication/article7/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/article7/","section":"publication","summary":"Extratropical Cyclone (EC) characteristics depend on a combination of large-scale factors and regional processes. However, the latter are considered to be poorly represented in global climate models (GCMs), partly because their resolution is too coarse. This paper describes a framework using possibilities given by regional climate models (RCMs) to gain insight into storm activity during winter over North America (NA). Recent past climate period (1981–2005) is considered to assess EC activity over NA using the NCEP regional reanalysis (NARR) as a reference, along with the European reanalysis ERA-Interim (ERAI) and two CMIP5 GCMs used to drive the Canadian Regional Climate Model—version 5 (CRCM5) and the corresponding regional-scale simulations. While ERAI and GCM simulations show basic agreement with NARR in terms of climatological storm track patterns, detailed bias analyses show that, on the one hand, ERAI presents statistically significant positive biases in terms of EC genesis and therefore occurrence while capturing their intensity fairly well. On the other hand, GCMs present large negative intensity biases in the overall NA domain and particularly over NA eastern coast. In addition, storm occurrence over the northwestern topographic regions is highly overestimated. When the CRCM5 is driven by ERAI, no significant skill deterioration arises and, more importantly, all storm characteristics near areas with marked relief and over regions with large water masses are significantly improved with respect to ERAI. Conversely, in GCMdriven simulations, the added value contributed by CRCM5 is less prominent and systematic, except over western NA areas with high topography and over the Western Atlantic coastlines where the most frequent and intense ECs are located. Despite this significant added-value on seasonal mean characteristics, a caveat is raised on the RCM ability to handle storm temporal ‘seriality’, as a measure of their temporal variability at a given location. In fact, the driving models induce some significant footprints on the RCM skill to reproduce the intra-seasonal pattern of storm activity.","tags":[],"title":"Investigating added value of regional climate modeling in North American winter storm track simulations","type":"publication"},{"authors":["Philippe Gachon","Louise Bussières","Pierre Gosselin","Marie Raphoz","Ray Bustinza","Philippe Martin","Guillaume Dueymes","Denis Gosselin","Sylvain Labrecque","Sharon Jeffers","Abderrahmane Yagouti"],"categories":["4"],"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"fefd7849ab83658d135df5051d64595b","permalink":"/publication/article6/","publishdate":"2016-09-01T00:00:00Z","relpermalink":"/publication/article6/","section":"publication","summary":"Among natural-disaster risks, heat waves are responsible for a large number of deaths, diseases and economic losses around the world. As they will increase in severity, duration and frequency over the decades to come within the context of climate change, these extreme events constitute a genuine danger to human health, and heat-warning systems are strongly recommended by public health authorities to reduce this risk of diseases and of excessive mortality and morbidity. Thus, evidence-based public alerting criteria are needed to reduce impacts on human health before and during persistent hot weather conditions. The goal of this guide is to identify alert thresholds for heat waves in Canada based on evidence, and to propose an approach for better defining heat waves in the Canadian context in order to reduce the risks to human health and contribute to the well-being of Canadians. This guide is the result of the collaboration among various research and public institutions working on: 1) meteorological and climate aspects, i.e. the Meteorological Service of Canada (MSC, Environment and Climate Change Canada), and the ESCER centre at the Université du Québec à Montréal, and 2) public health, i.e. Health Canada and the Institut National de Santé Publique du Québec.","tags":[],"title":"Guide to identifying alert thresholds for heat waves in Canada based on evidence","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["E. D. Poan","P. Gachon","G.Dueymes","E. Diaconescu","R. Laprise","I. Seidou Sanda"],"categories":["2"],"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"2a86ded52b77b30f18f35e33be53fc7f","permalink":"/publication/article5/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/publication/article5/","section":"publication","summary":"The West African monsoon intraseasonal variability has huge socio-economic impacts on local populations but understanding and predicting it still remains a challenge for the weather prediction and climate scientific community. This paper analyses an ensemble of simulations from six regional climate models (RCMs) taking part in the coordinated regional downscaling experiment, the ECMWF ERA-Interim reanalysis (ERAI) and three satellite-based and observationally-constrained daily precipitation datasets, to assess the performance of the RCMs with regard to the intraseasonal variability. A joint analysis of seasonal-mean precipitation and the total column water vapor (also called precipitable water—PW) suggests the existence of important links at different timescales between these two variables over the Sahel and highlights the relevance of using PW to follow the monsoon seasonal cycle. RCMs that fail to represent the seasonal-mean position and amplitude of the meridional gradient of PW show the largest discrepancies with respect to seasonal-mean observed precipitation. For both ERAI and RCMs, spectral decompositions of daily PW as well as rainfall show an overestimation of low-frequency activity (at timescales longer than 10 days) at the expense of the synoptic (timescales shorter than 10 days) activity. Consequently, the effects of the African Easterly Waves and the associated mesoscale convective systems are substantially underestimated, especially over continental regions. Finally, the study investigates the skill of the models with respect to hydro-climatic indices related to the occurrence, intensity and frequency of precipitation events at the intraseasonal scale. Although most of these indices are generally better reproduced with RCMs than reanalysis products, this study indicates that RCMs still need to be improved (especially with respect to their subgrid-scale parameterization schemes) to be able to reproduce the intraseasonal variance spectrum adequately..","tags":[],"title":"West African monsoon intraseasonal activity and its daily precipitation indices in regional climate models: diagnostics and challenges.","type":"publication"},{"authors":["P.Gachon","G.Dueymes","Pierre Gosselin","Olivier Gagnon"],"categories":["4"],"content":"","date":1409529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409529600,"objectID":"875b9a6ebbb9ed7c424c037e449aa037","permalink":"/publication/article4/","publishdate":"2014-09-01T00:00:00Z","relpermalink":"/publication/article4/","section":"publication","summary":"Renforcer la capacité d’intervention et d’adaptation en santé publique nécessite d’améliorer l’efficacité des systèmes d’alerte précoce vis-à-vis des risques climatiques en évolution. Ceci implique des ajustements aux activités en cours, voire de modifier les façons de faire au sein des organisations et entre les organisations en augmentant, notamment, leurs collaborations. L’interdisciplinarité au service de la santé publique est donc de mise.","tags":[],"title":"L’interdisciplinarité au service de la santé: cas du Québec. Territoires incubateurs de santé ?","type":"publication"},{"authors":["Éric Girard","Guillaume Dueymes","Ping Du","Allan K Bertram"],"categories":["2"],"content":"","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"28f40025f8865d42e9918ea6c2452713","permalink":"/publication/article2/","publishdate":"2013-03-01T00:00:00Z","relpermalink":"/publication/article2/","section":"publication","summary":"Owing to the large-scale transport of pollution-derived aerosols from the mid-latitudes to the Arctic, most of the aerosols are coated with acidic sulfate during winter in the Arctic. Recent laboratory experiments have shown that acid coating on dust particles substantially reduces the ability of these particles to nucleate ice crystals. Simulations performed using the Limited Area version of the Global Multiscale Environmental Model (GEM-LAM) are used to assess the potential effect of acid-coated ice nuclei on the Arctic cloud and radiation processes during January and February 2007. Ice nucleation is treated using a new parameterization based on laboratory experiments of ice nucleation on sulphuric acid-coated and uncoated kaolinite particles. Results show that acid coating on dust particles has an important effect on cloud microstructure, atmospheric dehydration, radiation and temperature over the Central Arctic, which is the coldest part of the Arctic. Mid and upper ice clouds are optically thinner while low-level mixed-phase clouds are more frequent and persistent. These changes in the cloud microstructures affect the radiation at the top of the atmosphere with longwave negative cloud forcing values ranging between 0 and −6 W m −2 over the region covered by the Arctic air mass..","tags":[],"title":"Assessment of the effects of acid-coated ice nuclei on the Arctic cloud microstructure, atmospheric dehydration, radiation and temperature during winter ","type":"publication"},{"authors":["P.Gachon","M.Radojevic","A. Harding","L.Benyahya","R.Laprise","N.Khaliq","P.Roy","H-II Eum","G.Dueymes"],"categories":["4"],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"afda9015bd1fb44241aa8afde7145eff","permalink":"/publication/article3/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/article3/","section":"publication","summary":"","tags":[],"title":"Evaluation of regional Climate Model simulations: intercomparaisonover Canada and specific region ","type":"publication"},{"authors":["Éric Girard","A. Stefanof","M. Peltier-Champigny","Rodrigo Munoz-Alpizar","Guillaume Dueymes","Jean-Pierre Blanchet"],"categories":["2"],"content":"","date":1193875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1193875200,"objectID":"7ed48ed44aaee1b5f50f1eb03514b9c0","permalink":"/publication/assessment_of_dehydration/","publishdate":"2007-11-01T00:00:00Z","relpermalink":"/publication/assessment_of_dehydration/","section":"publication","summary":"The effect of pollution-derived sulphuric acid aerosols on the aerosol-cloud-radiation interactions is investigated over the Arctic for February 1990. Observations suggest that acidic aerosols can decrease the heterogeneous nucleation rate of ice crystals and lower the homogeneous freezing temperature of haze droplets. Based on these observations, we hypothesize that the cloud thermodynamic phase is modified in polluted air mass (Arctic haze). Cloud ice number concentration is reduced, thus promoting further ice crystal growth by the Bergeron-Findeisen process. Hence, ice crystals reach larger sizes and low-level ice crystal precipitation from mixed-phase clouds increases. Enhanced dehydration of the lower troposphere contributes to decrease the water vapour greenhouse effect and cool the surface. A positive feedback is created between surface cooling and air dehydration, accelerating the cold air production. This process is referred to as the dehydration-greenhouse feedback (DGF). Simulations performed using an arctic regional climate model for February 1990, February and March 1985 and 1995 are used to assess the potential effect of the DGF on the Arctic climate. Results show that the DGF has an important effect over the Central and Eurasian Arctic, which is the coldest part of the Arctic with a surface cooling ranging between 0 and -3K. Moreover, the lower tropospheric cooling over the Eurasian and Central Arctic strengthens the atmospheric circulation at upper level, thus increasing the aerosol transport from the mid-latitudes and enhancing the DGF. Over warmer areas, the increased aerosol concentration (caused by the DGF) leads to longer cloud lifetime, which contributes to warm these areas. It is also shown that the maximum ice nuclei reduction must be of the order of 100 to get a significant effect.","tags":[],"title":"Assessment of the Dehydration-Greenhouse Feedback Over the Arctic During Winter","type":"publication"}]