[{"authors":["admin"],"categories":null,"content":"I am able to discuss and negotiate with scientists/analysts from different backgrounds in order to understand their needs and offer them the best solution and more importantly develop tools that are unanimously accepted. I worked in various environments, ranging in the development of extreme weather indices (droughts, extreme precipitation\u0026hellip;), validation of regional climate models, development of epidemiological indices, monitoring of vector disease, analysis of storm trajectories or work on instrumental data. I am interested by Open Science and willing to learn about Data Science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am able to discuss and negotiate with scientists/analysts from different backgrounds in order to understand their needs and offer them the best solution and more importantly develop tools that are unanimously accepted. I worked in various environments, ranging in the development of extreme weather indices (droughts, extreme precipitation\u0026hellip;), validation of regional climate models, development of epidemiological indices, monitoring of vector disease, analysis of storm trajectories or work on instrumental data.","tags":null,"title":"Guillaume Dueymes","type":"authors"},{"authors":null,"categories":null,"content":"Python basics List in python Flow control Functions in Python Dictionaries A dictionary is an unordered and mutable Python container that stores mappings of unique keys to values. Dictionaries are written with curly brackets, including key-value pairs separated by commas.\nNumpy library Numpy is a python package for scientific computing that provides high-performance multidimensional arrays objects. This library is widely used for numerical analysis, matrix computations, and mathematical operations.\nPandas library Pandas is a Python open source library for data science that allows us to easily work with structured data, such as csv files, SQL tables, or Excel spreadsheets. It provides tools for reading and writing data in different formats, carrying out exploratory analysis, and cleaning data (reshaping data sets, handling missing data, or merging data sets).\nMatplotlib library Matplotlib library is a Python 2D plotting library that can be used in Python scripts, Jupyter notebooks, IPython shells, Spyder environment\u0026hellip;producing high quality figures. Matplotlib offers a wide range of visualizations such as histograms, bar chart, scatter plots , box plots\u0026hellip;\nProject 1 On this first project, we will work with temperature datasets from Environment Canada. We will use all librairies we presented in previous section.\nProject 2 On this second project, we will present analysis we can do with precipitation observation.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"28e269de8b1350c009cf8456e3f29d72","permalink":"/courses/tutorial_python/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/tutorial_python/","section":"courses","summary":"Learn how to use Python from basics .","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Python Netcdf Part1 Python Netcdf Part2 Cartopy library Xarray library ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"e99f1d7c22efb7c4a58bf423c441bd9e","permalink":"/courses/tutorial_python_netcdf/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/tutorial_python_netcdf/","section":"courses","summary":"Learn how to use Python from basics .","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"1.1 Variables A variable is an identifier or name to store information or results. All python programs use variables. It is important to remember that each type of information is stored in a special type of variable. A type of variable is an information about the contents of the variable. The type of variable will tell the python interpreter what it can do or not with this variable.\n Python can make the difference between upper and lower case letters You can not start a variable with a number Do not use accents in variable names Function : type() give you informations on the type of our variable  m=3.2 int(m) 3  1.1.1 Numeric type:    Type Description     Integer: int() 1 is an integer, 1.0 is not an integer   Float: float() Number that includes a decimal part   Complex numbers Association between a real number and an imaginary number    type(m) float  int(3) 3  int(3.5) 3  float(3) 3.0  3+5 8  1.1.2 Booleans:  variable storing binary values: True or False data types result from logical operations  Very useful in if() conditional structures.\n3\u0026gt;5 False  3\u0026lt;5 True  1.1.3 Strings: Python can also manipulate strings, which can be expressed in several ways. They can be enclosed in single quotes ('\u0026hellip;') or double quotes (\u0026quot;\u0026hellip;\u0026quot;)\nprint(\u0026#34;Hello\u0026#34;) Hello  type(\u0026#39;Hello\u0026#39;) str  type(\u0026#34;15\u0026#34;) str  \u0026#34;15\u0026#34; + 2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-13-3fe5846cb1c2\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 \u0026quot;15\u0026quot; + 2 TypeError: can only concatenate str (not \u0026quot;int\u0026quot;) to str ----------------------------------------------------------------------------  But we can concatenate two strings together.\n\u0026#34;15\u0026#34; + \u0026#34;56\u0026#34; '1556'  str(15) '15'  String objects have a bunch of useful methods; for example:\ns = \u0026#34;hello\u0026#34; print(s.capitalize()) # Capitalize a string; prints \u0026#34;Hello\u0026#34; print(s.upper()) # Convert a string to uppercase; prints \u0026#34;HELLO\u0026#34; print(s.rjust(7)) # Right-justify a string, padding with spaces; prints \u0026#34; hello\u0026#34; print(s.center(7)) # Center a string, padding with spaces; prints \u0026#34; hello \u0026#34; print(s.replace(\u0026#39;l\u0026#39;, \u0026#39;(ell)\u0026#39;)) # Replace all instances of one substring with another; # prints \u0026#34;he(ell)(ell)o\u0026#34; print(\u0026#39;world \u0026#39;.strip()) # Strip leading and trailing whitespace; prints \u0026#34;world\u0026#34; Hello HELLO hello hello he(ell)(ell)o world  1.2 Python Arithmetic Operators    Operator Description     + Adds values   - Subtracts right hand operand from left hand operand   * Multiplies values on either side of the operator   / Divides left hand operand by right hand operand   % Divides left hand operand by right hand operand and returns remainder (modulus)   %% Performs exponential (power) calculation on operators   // Floor Division - The division of operands where the result is the quotient in which the digits after the decimal point are removed. But if one of the operands is negative, the result is floored, i.e., rounded away from zero (towards negative infinity)    5-3 # Subtraction  2  5+3 # Addition  8  5*3 # Multiplication 15  5/3 # division  1.6666666666666667  5//3 # Floor Division  1  5%3 # Modulus  2  5**3 # power 125  ma_variable1=10 ma_variable2=20 ma_variable1*ma_variable2 200  ma_variable1*10 100  We cannot apply arithmetic operators on text.\n\u0026#34;mon texte\u0026#34; - \u0026#34;mon texte\u0026#34; --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-26-5483fafb4a74\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 \u0026quot;mon texte\u0026quot; - \u0026quot;mon texte\u0026quot; TypeError: unsupported operand type(s) for -: 'str' and 'str' ----------------------------------------------------------------------------  \u0026#34;Hi\u0026#34; + \u0026#34;how are you ?\u0026#34; 'Hi how are you ?'  \u0026#34;Hi \u0026#34; * 5 'Hi Hi Hi Hi Hi '  1.3 Python Comparison Operators These operators compare the values on either sides of them and decide the relation among them. They are also called Relational operators. The true value of relationships is often used to make decisions by ensuring that conditions are met to perform a certain task.\n   Operator Description     == If the values of two operands are equal, then the condition becomes true   != If values of two operands are not equal, then condition becomes true   \u0026gt; If the value of left operand is greater than the value of right operand, then condition becomes true   \u0026lt; If the value of left operand is less than the value of right operand, then condition becomes true   \u0026gt;= If the value of left operand is greater than or equal to the value of right operand, then condition becomes true   \u0026lt;= If the value of left operand is less than or equal to the value of right operand, then condition becomes true   is Identity operators compare the memory locations of two objects. Evaluates to true if the variables on either side of the operator point to the same object and false otherwise   is not Identity operators compare the memory locations of two objects. Evaluates to false if the variables on either side of the operator point to the same object and true otherwise    1 == 1 True  1==2 False  1!=2 True  1\u0026gt;2 False  2\u0026gt;1 True  1\u0026gt;1 False  1\u0026lt;2 True  1\u0026gt;=1 True  2 \u0026lt;= 5 True  a=1 # Warning !!!! this is an assignment operator a 1  1.4 Python Logical Operators Logical operators compare Boolean expressions instead of values as in the previous point.\nThey are used to create Boolean expressions that help us know if a certain task should be performed or not. Operator| Description | |\u0026mdash;\u0026mdash;|\u0026mdash;\u0026mdash;| | and| If both operands are true then condition becomes true| | or| If any of the two operands are non-zero then condition becomes true| | not| Used to reverse the logical state of its operand|\n# and (3==3) and (4==4) True  (3==3) and (4==5) False  # or (3==3) or (4==5) True  (3!=3) or (4==5) False  # not not((3!=3) or (4==5)) True  1.5 Python Assignment Operators Assignment operators store a value in a variable. We have already seen the simplest case (=), but python offers many other operators of this type, in particular to perform an arithmetic operation at the same time as the assignment.\n   Operator Description     = Assigns values from right side operands to left side operand   += It adds right operand to the left operand and assign the result to left operand   -= It subtracts right operand from the left operand and assign the result to left operand   *= It multiplies right operand with the left operand and assign the result to left operand   /= It divides left operand with the right operand and assign the result to left operand   %= It takes modulus using two operands and assign the result to left operand   **= Performs exponential (power) calculation on operators and assign value to the left operande   //= It performs floor division on operators and assign value to the left operand    Mavariable = 5 Mavariable = 2 Mavariable 2  Mavariable = 5 Mavariable += 2 Mavariable 7  Mavariable = 5 Mavariable -= 2 Mavariable 3  Mavariable = 5 Mavariable *= 2 Mavariable 10  Mavariable = 5. Mavariable /= 2 Mavariable 2.5  Mavariable = 5 Mavariable %= 2 Mavariable 1  Mavariable = 5 Mavariable **= 2 Mavariable 25  Mavariable = 5. Mavariable //= 2 Mavariable 2.0  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"2fa7b87af3ee9cc1febd8a01857099e5","permalink":"/courses/tutorial_python/1-python_basics/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/1-python_basics/","section":"courses","summary":"1.1 Variables A variable is an identifier or name to store information or results. All python programs use variables. It is important to remember that each type of information is stored in a special type of variable. A type of variable is an information about the contents of the variable. The type of variable will tell the python interpreter what it can do or not with this variable.\n Python can make the difference between upper and lower case letters You can not start a variable with a number Do not use accents in variable names Function : type() give you informations on the type of our variable  m=3.","tags":null,"title":"1 Basic data types","type":"docs"},{"authors":null,"categories":null,"content":"Network Common Data Form (Netcdf) is a way to create, access, and share scientific data in a format that is self-documenting and transparent for many types of machines.\nThe Netcdf file itself has information describing the data it contains.\nA NetCDF file has dimensions, variables, and attributes.\nHere is a small example of a NetCDF file, to illustrate these concepts of dimensions, variables and attributes.\nThe notation used to describe this NetCDF file is called Network Common Data Language (CDL). It gives a \u0026ldquo;text\u0026rdquo; version that allows an easy understanding of the structure and contents of a binary NetCDF file:\nnetcdf ./data/NARR_tasmax_201701.nc { dimensions: x = 349; y = 277; time = 31; variables: float lon(y=277, x=349); :units = \u0026quot;degrees_east\u0026quot;; :long_name = \u0026quot;Longitude\u0026quot;; :CoordinateAxisType = \u0026quot;Lon\u0026quot;; float lat(y=277, x=349); :units = \u0026quot;degrees_north\u0026quot;; :long_name = \u0026quot;Latitude\u0026quot;; :CoordinateAxisType = \u0026quot;Lat\u0026quot;; double time(time=31); :long_name = \u0026quot;Time\u0026quot;; :delta_t = \u0026quot;\u0026quot;; float tasmax(time=31, y=277, x=349); :long_name = \u0026quot;Daily maximum temperature\u0026quot;; :units = \u0026quot;Celcius\u0026quot;; :missing_value = -999.0; // double :coordinates = \u0026quot;lon lat\u0026quot;;  }\nPanoply is an apensource software used to quickly read aned visualize Netcdf file: https://www.giss.nasa.gov/tools/panoply/download/\nThe python module that we will use in this section is netCDF4: https://pypi.org/project/netCDF4/\nTo install it under anaconda: conda install -c anaconda netcdf41- Create a Netcdf file: from netCDF4 import Dataset import os import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) file_name = \u0026#34;./DATA/2D_Temperature.nc\u0026#34; if os.path.isfile(file_name): os.remove(file_name) #open the file for writing, you can Also specify format=\u0026#34;NETCDF4_CLASSIC\u0026#34; or \u0026#34;NETCDF3_CLASSIC\u0026#34; #The format is NETCDF4 by default ds = Dataset(file_name, mode=\u0026#34;w\u0026#34;)  Netcdf4-python: createDimension  We will create here a 2D field: (20,20)\nds.createDimension(\u0026#34;x\u0026#34;, 20) ds.createDimension(\u0026#34;y\u0026#34;, 20) ds.createDimension(\u0026#34;time\u0026#34;, None) \u0026lt;class 'netCDF4._netCDF4.Dimension'\u0026gt; (unlimited): name = 'time', size = 0   Netcdf4-python: createVariable  var1 = ds.createVariable(\u0026#34;field1\u0026#34;, \u0026#34;f4\u0026#34;, (\u0026#34;time\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) var2 = ds.createVariable(\u0026#34;field2\u0026#34;, \u0026#34;f4\u0026#34;, (\u0026#34;time\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) #add netcdf attributes var1.units = \u0026#34;Celcius\u0026#34; var1.long_name = \u0026#34;Surface air temperature\u0026#34; var2.units = \u0026#34;Kelvin\u0026#34; var2.long_name = \u0026#34;Surface air temperature\u0026#34; We can now assign our field or variable.\n#generate random data and tell the program where it should go import numpy as np data = np.random.randn(10, 20, 20) var1[:] = data var2[:] = data + 273.15 #actually write data to the disk ds.close(); 2- Read a Netcdf file We will read the file Netcdf4 that we created previously.\nfrom netCDF4 import Dataset ds = Dataset(\u0026#34;./DATA/2D_Temperature.nc\u0026#34;) We will select our variables of interest: no data is loaded at this level.\n# Which variables are in our Netcdf file ? print(ds.variables.keys()) data1_var = ds.variables[\u0026#34;field1\u0026#34;] data2_var = ds.variables[\u0026#34;field2\u0026#34;] #What\u0026#39;s the dimension ? print(data1_var.dimensions, data1_var.shape) odict_keys(['field1', 'field2']) ('time', 'x', 'y') (10, 20, 20)  We can now read our file:\n#now we ask to really read the data into the memory all_data = data1_var[:] #print all_data.shape data1 = data1_var[1,:,:] data2 = data2_var[2,:,:] #data1 print(data1.shape, all_data.shape, all_data.mean(axis = 0).mean(axis = 0).mean(axis = 0)) (20, 20) (10, 20, 20) 0.0055584796  3- Examples of manipulations of a Netcdf file In this example we will work with daily maximum temperature data for all January months from 1971 to 2000 from the CRCM5 regional model developed at the ESCER center.\nWe will import our own module for calculating temperature indices, this module is calld Indices_Temperature.\nAt first we will import the necessary libraries and define the input parameters.\nfrom netCDF4 import Dataset import Indices_Temperature import numpy as np Mois=\u0026#39;01\u0026#39; model=\u0026#39;CRCM5-v1_NAM-44_ll_CCCma-CanESM2\u0026#39; path_model=\u0026#39;CRCM5-v1_CCCma-CanESM2_historical\u0026#39; variable=\u0026#39;tasmax\u0026#39; indice = \u0026#39;Tmax90p\u0026#39; yi = 1971 yf = 2000 ######################################################### rep_data=\u0026#39;./DATA/CRCM5/\u0026#39; rep_out=\u0026#39;./DATA/CRCM5/\u0026#39; tot=(yf-yi)+1  Let's open \u0026lsquo;CRCM5-v1_NAM-44_ll_CCCma-CanESM2_historical_tasmax_197101.nc\u0026rsquo; file:  nc_Modc=Dataset(\u0026#39;./DATA/CRCM5/\u0026#39;+model+\u0026#39;_historical_\u0026#39;+variable+\u0026#39;_197101.nc\u0026#39;,\u0026#39;r\u0026#39;) lats=nc_Modc.variables[\u0026#39;lat\u0026#39;][:] lons=nc_Modc.variables[\u0026#39;lon\u0026#39;][:] varc=nc_Modc.variables[variable][:] ## Quick look of our file:  print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;Temperature dimension = \u0026#39;,varc.shape) print(\u0026#39;Minimum of temperature = \u0026#39;, np.nanmin(varc)) print(\u0026#39;Maximum of temperature = \u0026#39;, np.nanmax(varc)) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;Latitude dimension= \u0026#39;,lats.shape) print(\u0026#39;Minimum of latitude = \u0026#39;, np.min(lats)) print(\u0026#39;Maximum of latitude = \u0026#39;, np.max(lats)) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;Longitude dimesion = \u0026#39;,lons.shape) print(\u0026#39;Minimum of longitude = \u0026#39;, np.min(lons)) print(\u0026#39;Maximum of longitude = \u0026#39;, np.max(lons)) print(\u0026#39;-----------------------------------------\u0026#39;) ----------------------------------------- Temperature dimension = (31, 130, 155) Minimum of temperature = -57.66677 Maximum of temperature = 30.657953 ----------------------------------------- ----------------------------------------- Latitude dimension= (130, 155) Minimum of latitude = 12.538727 Maximum of latitude = 75.86 ----------------------------------------- ----------------------------------------- Longitude dimesion = (130, 155) Minimum of longitude = -170.71053 Maximum of longitude = -23.28948 -----------------------------------------  We can now loop over all our Netcdf files and apply a function to calculate our indice.\nnt=0 IND = np.zeros((tot,130,155),dtype=float) for year in range(yi,yf+1): ###### ouverture et lecture des fichiers Netcdf hist=model+\u0026#39;_historical_\u0026#39;+variable+\u0026#39;_\u0026#39;+str(year)+Mois+\u0026#39;.nc\u0026#39; modelc=rep_data+\u0026#39;/\u0026#39;+hist nc_Modc=Dataset(modelc,\u0026#39;r\u0026#39;) lats=nc_Modc.variables[\u0026#39;lat\u0026#39;][:] lons=nc_Modc.variables[\u0026#39;lon\u0026#39;][:] varc=nc_Modc.variables[variable][:] ###### boucle sur tous les points de grille et calcul de l\u0026#39;indice for ni in range(0, len(varc[0])): for nj in range(0, len(varc[0][0])): if indice == \u0026#39;Mean_tasmax\u0026#39;: IND[nt,ni,nj]=Indices_Temperature.MOY(varc[:,ni,nj]) description=\u0026#39;Monthly Mean of tasmax\u0026#39; unite=\u0026#39;°Celcius\u0026#39; elif indice == \u0026#39;Tmax90p\u0026#39;: IND[nt,ni,nj]=Indices_Temperature.Tmax90p(varc[:,ni,nj]) description=\u0026#39;Monthly Mean of Tmax90p\u0026#39; unite=\u0026#39;°Celcius\u0026#39; nt+=1 ###### Écriture du fichier Netcdf en sortie C = Dataset(rep_out+model+\u0026#39;_historical_\u0026#39;+indice+\u0026#39;_\u0026#39;+str(yi)+\u0026#39;-\u0026#39;+str(yf)+\u0026#39;_\u0026#39;+Mois+\u0026#39;.nc\u0026#39;, \u0026#39;w\u0026#39;) C.description = \u0026#39;Indice temperature\u0026#39; C.conventions = \u0026#39;CF-1.0\u0026#39; C.model_id = model C.grid=\u0026#39;latlon\u0026#39; C.CDO = \u0026#39;Climate Data Operators version 1.6.2 (http://code.zmaw.de/projects/cdo)\u0026#39; C.institution = \u0026#39;UQAM - ESCER Center, University of Quebec in Montreal\u0026#39; C.contact = \u0026#39;Guillaume Dueymes\u0026#39; ######################################## # Dimensions C.createDimension(\u0026#39;x\u0026#39;, len(varc[0][0])) C.createDimension(\u0026#39;y\u0026#39;, len(varc[0])) C.createDimension(\u0026#39;time\u0026#39;, tot) var=C.createVariable(str(indice), np.float32, (\u0026#39;time\u0026#39;,\u0026#39;y\u0026#39;,\u0026#39;x\u0026#39;)) var.long_name = str(description) var.unit = str(unite) lat=C.createVariable(\u0026#39;lat\u0026#39;, np.float32, (\u0026#39;y\u0026#39;,\u0026#39;x\u0026#39;)) lon=C.createVariable(\u0026#39;lon\u0026#39;, np.float32, (\u0026#39;y\u0026#39;,\u0026#39;x\u0026#39;)) time = C.createVariable(\u0026#39;time\u0026#39;, np.float64, (\u0026#39;time\u0026#39;,)) time.long_name = \u0026#39;time\u0026#39; for var in [\u0026#39;lon\u0026#39;,\u0026#39;lat\u0026#39;,\u0026#39;time\u0026#39;]: for att in nc_Modc.variables[var].ncattrs(): setattr(C.variables[var],att,getattr(nc_Modc.variables[var],att)) time[:]=range(1,nt+1) lat[:,:] = lats lon[:,:] = lons C.variables[str(indice)][:,:,:] = IND[::] C.close() IND.shape (30, 130, 155)  4- Netcdf gridpoint extraction In this example we will work with maximum daily temperature data from Regional Reanalysis (NARR) Regional Reanalysis. For more information about this product:\nwww.emc.ncep.noaa.gov/mmb/rreanl\nThe Netcdf files that we will open are daily values from January 1, 2017 to December 31, 2017. Files are archived by month.\nThe NARR grid point closest to the ECCC Montréal / McTavish weather station (45.5N, -73.8W) will be extracted here.\nWe first import the libraries.\nimport netCDF4 import numpy as np import pandas as pd import datetime from datetime import date, timedelta from dateutil.relativedelta import relativedelta rep1=\u0026#39;./DATA/NARR/\u0026#39; model=\u0026#39;NARR\u0026#39; variable=\u0026#39;tasmax\u0026#39; variable_name=\u0026#39;Temperature maximale\u0026#39; yeari=2017 monthi=1 yearf = 2017 monthf = 12 station=\u0026#39;Montreal\u0026#39; lati = 45.5 loni = -73.8 day_start=1 day_end = pd.date_range(\u0026#39;{}-{}\u0026#39;.format(yearf, monthf), periods=1, freq=\u0026#39;M\u0026#39;).day.tolist()[0] start=datetime.datetime(yeari,monthi,day_start) end=datetime.datetime(yearf,monthf,day_end) d0 = date(yeari, monthi, day_start) d1 = date(yearf, monthf, day_end) delta = d1 - d0 nb_days = delta.days+1 var_data=np.zeros(nb_days) We define a function that will calculate the distance between each grid point of the Netcdf file and our reference latitude / longitude. We then deduce the minimum distance.\nThe other function allows us to increment our months when reading Netcdf files.\ndef getclosest_ij(lats,lons,latpt,lonpt): # find squared distance of every point on grid dist_sq = (lats-latpt)**2 + (lons-lonpt)**2 # 1D index of minimum dist_sq element minindex_flattened = dist_sq.argmin() # Get 2D index for latvals and lonvals arrays from 1D index return np.unravel_index(minindex_flattened, lats.shape) def add_month(now): try: then = (now + relativedelta(months=1)).replace(day=now.day) except ValueError: then = (now + relativedelta(months=2)).replace(day=1) return then We can know apply our function over each grid point, month by month and save the value into a DataFrame.\ni=0 IND=[] # Début de notre boucle temporelle incr=start while incr \u0026lt;= end: filename= rep1 + model + \u0026#39;_\u0026#39; + variable + \u0026#39;_\u0026#39; + str(incr.year) + \u0026#39;{:02d}\u0026#39;.format(incr.month) + \u0026#39;.nc\u0026#39; f = netCDF4.Dataset(filename) # print(f.variables.keys()) # get all variable names var = f.variables[variable] # temperature variable #print(temp)  #temp.dimensions #temp.shape lat, lon = f.variables[\u0026#39;lat\u0026#39;], f.variables[\u0026#39;lon\u0026#39;] #print(lat) #print(lon) #print(lat[:]) # extract lat/lon values (in degrees) to numpy arrays latvals = lat[:]; lonvals = lon[:] # a function to find the index of the point closest pt # (in squared distance) to give lat/lon value.  iy_min, ix_min = getclosest_ij(latvals, lonvals, lati, loni) #print(iy_min) #print(ix_min) IND.append(var[:,iy_min,ix_min]) incr=add_month(incr) flattened_list = [y for x in IND for y in x] start=datetime.datetime(yeari,monthi,day_start) TIME=[] for i in range(0,nb_days,1): # TIME.append((start+timedelta(days=i)).strftime(\u0026#34;%Y-%m-%d\u0026#34;)) TIME.append((start+timedelta(days=i))) dataFrame_NARR = pd.DataFrame({\u0026#39;Date\u0026#39;: TIME, variable_name: flattened_list}, columns = [\u0026#39;Date\u0026#39;,variable_name]) dataFrame_NARR = dataFrame_NARR.set_index(\u0026#39;Date\u0026#39;) dataFrame_NARR.head()    Date Temperature maximale     2017-01-01 00:00:00 -0.893957   2017-01-02 00:00:00 -1.53144   2017-01-03 00:00:00 -1.73783   2017-01-04 00:00:00 -0.240698   2017-01-05 00:00:00 -0.791754    We save our DataFrame in csv.\ndataFrame_NARR.to_csv(\u0026#39;./DATA/NARR/NARR_\u0026#39;+station+\u0026#39;_1pt_\u0026#39;+str(variable)+\u0026#39;_\u0026#39;+ \u0026#39;{:02d}\u0026#39;.format(monthi) + str(yeari)+\u0026#39;_\u0026#39;+ \u0026#39;{:02d}\u0026#39;.format(monthf) + str(yearf)+\u0026#39;.csv\u0026#39;)  Observation vs NARR Now we have extracted our gridpoint near Montreal, we can compare these values with observations.  We will extract the data from the Montreal McTavish station and put them in the NARR dataFrame.\ndataframe_station = pd.read_csv(\u0026#39;./DATA/station/MONTREAL_TAVISH_tasmax_1948_2017.csv\u0026#39;, header=None, names=[\u0026#39;Maximum temperature: OBS\u0026#39;]) start = date(1948, 1, 1) end = date(2017, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) dataframe_station[\u0026#39;datetime\u0026#39;] = rng dataframe_station.index = dataframe_station[\u0026#39;datetime\u0026#39;] dataframe_station = dataframe_station.drop([\u0026#34;datetime\u0026#34;], axis=1) dataframe_station.head()    datetime Maximum temperature: OBS     1948-01-01 00:00:00 -10   1948-01-02 00:00:00 -3.9   1948-01-03 00:00:00 -0.6   1948-01-04 00:00:00 -2.8   1948-01-05 00:00:00 -2.2    dataFrame_NARR = dataFrame_NARR.rename(columns={\u0026#34;Temperature maximale\u0026#34;: \u0026#34;Maximum temperature: NARR\u0026#34;}) df_NARR_Station = pd.concat([dataframe_station,dataFrame_NARR],axis=1) df_NARR_Station.tail()     Maximum temperature: OBS Maximum temperature: NARR     2017-12-27 00:00:00 -17.6 -18.819   2017-12-28 00:00:00 -20.5 -19.3721   2017-12-29 00:00:00 -18.4 -16.4532   2017-12-30 00:00:00 -17.3 -16.2088   2017-12-31 00:00:00 -18.3 -16.7728    We want to plot valeurs for 2016 and 2017 years.\ndf=df_NARR_Station.loc[\u0026#39;2016\u0026#39; : \u0026#39;2017\u0026#39;] df.head()     Maximum temperature: OBS Maximum temperature: NARR     2016-01-01 00:00:00 -0.1 nan   2016-01-02 00:00:00 0.4 nan   2016-01-03 00:00:00 0.2 nan   2016-01-04 00:00:00 -13.5 nan   2016-01-05 00:00:00 -7.5 nan    import matplotlib.pyplot as plt color = [\u0026#39;black\u0026#39;, \u0026#39;red\u0026#39;] fig = plt.figure(figsize=(16,8)) plt.plot(df.index, df[\u0026#39;Maximum temperature: NARR\u0026#39;][:], label=\u0026#39;NARR Temperature\u0026#39;, linewidth=2, c=color[0]) plt.plot(df.index, df[\u0026#39;Maximum temperature: OBS\u0026#39;][:], label=\u0026#39;Observation Temperature\u0026#39;, linewidth=2, c=color[1]) # Autre méthode pour tracer avec Pandas #df_NARR_Station[\u0026#39;2017\u0026#39;].plot(figsize=(10,5)) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Temperature\u0026#34;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 10}) plt.title(\u0026#34;Time serie: Station MTL vs NARR\u0026#34;, y=1.05) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 1, 1, 0),fontsize =10) plt.savefig(\u0026#34;figures/NARR_time_Serie_temperature.png\u0026#34;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # bbox_inches= : option qui permet de propostionner le graphique lors de l\u0026#39;enregistrement plt.show()  Example of using the .rolling () \u0026lt;/ b\u0026gt; function to calculate a moving average on a signal.  import matplotlib.pyplot as plt df_NARR_Station[\u0026#39;rollingmean5 Station\u0026#39;]= df_NARR_Station[\u0026#39;Maximum temperature: OBS\u0026#39;].rolling(window=5).mean() df_NARR_Station[\u0026#39;rollingmean5 NARR\u0026#39;]= df_NARR_Station[\u0026#39;Maximum temperature: NARR\u0026#39;].rolling(window=5).mean() df_NARR_Station.tail()     Maximum temperature: OBS Maximum temperature: NARR rollingmean5 Station rollingmean5 NARR     2017-12-27 00:00:00 -17.6 -18.819 -8.04 -8.31893   2017-12-28 00:00:00 -20.5 -19.3721 -11.12 -11.0995   2017-12-29 00:00:00 -18.4 -16.4532 -13.88 -13.6478   2017-12-30 00:00:00 -17.3 -16.2088 -16.12 -15.6138   2017-12-31 00:00:00 -18.3 -16.7728 -18.42 -17.5252    df_NARR_Station[\u0026#39;2017-01\u0026#39;:\u0026#39;2017-12\u0026#39;].plot(figsize=(16,8)) plt.xlabel(\u0026#34;Temps\u0026#34;) plt.ylabel(\u0026#34;Température\u0026#34;) plt.title(\u0026#34;Maximum temperature time serie: Station MTL vs NARR\u0026#34;, y=1.05) plt.savefig(\u0026#34;figures/NARR_time_Serie_temperature2.png\u0026#34;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) # bbox_inches= : option qui permet de propostionner le graphique lors de l\u0026#39;enregistrement plt.show() ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"a457cdd35b9332a6dccc91e35455b160","permalink":"/courses/tutorial_python_netcdf/1-netcdf_tutorial/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python_netcdf/1-netcdf_tutorial/","section":"courses","summary":"Network Common Data Form (Netcdf) is a way to create, access, and share scientific data in a format that is self-documenting and transparent for many types of machines.\nThe Netcdf file itself has information describing the data it contains.\nA NetCDF file has dimensions, variables, and attributes.\nHere is a small example of a NetCDF file, to illustrate these concepts of dimensions, variables and attributes.\nThe notation used to describe this NetCDF file is called Network Common Data Language (CDL).","tags":null,"title":"1 Netcdf Part 1","type":"docs"},{"authors":null,"categories":null,"content":"Instead of extracting a grid point, we may want to extract a domain / box delimited by latitudes and longitudes.\nIn this example we will work with the annual mean of daily maximum temperature data from several regional models of CORDEX-NAM44.\nWe will extract all the grid points from the region between 47degN and 51degN of latitude and between -72degW and -64degW of longitude .\nWe will then plot the inter-annual variability of the mean annual temperature anomalies.\n# we first inmport librairies import netCDF4 import numpy as np import pandas as pd from datetime import datetime import matplotlib.pylab as plt import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) import seaborn as sns from matplotlib import gridspec # path and variable name rep=\u0026#39;./DATA/Inter_annual_anomaly/\u0026#39; variable_in = \u0026#39;Mean_tasmax\u0026#39; # list of periods we want to use list_period = [\u0026#39;2011-2040\u0026#39;,\u0026#39;2041-2070\u0026#39;,\u0026#39;2071-2100\u0026#39;] # list of models list_rcp45 = [\u0026#39;CRCM5-v1_NAM-44_ll_CCCma-CanESM2_rcp45\u0026#39;, \u0026#39;CRCM5-v1_NAM-44_ll_MPI-M-MPI-ESM-LR_rcp45\u0026#39; ] # Area to extract latbounds = [ 47 , 51 ] lonbounds = [ -72 , -64 ] We will make a loop over each model and period to make the extraction:\n#  df_rcp45 = [] matrix_45 = [] for period in list_period: globals()[\u0026#39;flattened_list_\u0026#39;+period] = [] # we define a global variable  for i in range(0,len(list_rcp45)): filename= rep +\u0026#39;anomalie_\u0026#39; + list_rcp45[i] + \u0026#39;_\u0026#39; + variable_in + \u0026#39;_\u0026#39; + period + \u0026#39;_1971-2000.nc\u0026#39; nc = netCDF4.Dataset(filename) # we here read netcdf values var = nc.variables[variable_in][:] lats = nc.variables[\u0026#39;lat\u0026#39;][:]; lons = nc.variables[\u0026#39;lon\u0026#39;][:] # in this part, we extract our domain  subset = ((lats \u0026gt; latbounds[0]) \u0026amp; (lats \u0026lt; latbounds[1]) \u0026amp; (lons \u0026gt; lonbounds[0]) \u0026amp; (lons \u0026lt; lonbounds[1])) data=pd.DataFrame(var[:,subset], dtype=\u0026#39;float\u0026#39;) globals()[\u0026#39;flattened_list_\u0026#39;+period].append(data.mean(axis=1)) df_rcp45.append(pd.DataFrame(globals()[\u0026#39;flattened_list_\u0026#39;+period]).T) df_rcp45 = pd.concat(df_rcp45) df_rcp45.head()     0 1     0 1.72084 -0.288691   1 1.4762 0.885357   2 0.581686 0.541863   3 0.390671 -0.0141989   4 1.14842 1.29417    We can add datetime index in our DataFrame.\nTIME=[] for y in range(int(list_period[0].split(\u0026#39;-\u0026#39;)[0]),int(list_period[-1].split(\u0026#39;-\u0026#39;)[-1])+1,1): TIME.append(datetime.strptime(str(y), \u0026#39;%Y\u0026#39;)) df_rcp45[\u0026#39;Date\u0026#39;] = TIME df_rcp45.index = df_rcp45[\u0026#39;Date\u0026#39;] df_rcp45 = df_rcp45.drop([\u0026#34;Date\u0026#34;], axis=1) df_rcp45.head()    Date 0 1     2011-01-01 00:00:00 1.72084 -0.288691   2012-01-01 00:00:00 1.4762 0.885357   2013-01-01 00:00:00 0.581686 0.541863   2014-01-01 00:00:00 0.390671 -0.0141989   2015-01-01 00:00:00 1.14842 1.29417    We then want, for each year, the intra-model variability. For this, we will calculate the minimum and maximum values by applying the .apply method.\ndf_rcp45[\u0026#39;min\u0026#39;] = df_rcp45.apply(np.min, axis=1) df_rcp45[\u0026#39;max\u0026#39;] = df_rcp45.apply(np.max, axis=1) df_rcp45[\u0026#39;mean\u0026#39;] = df_rcp45.apply(np.mean, axis=1) df_rcp45.head()    Date 0 1 min max mean     2011-01-01 00:00:00 1.72084 -0.288691 -0.288691 1.72084 0.716076   2012-01-01 00:00:00 1.4762 0.885357 0.885357 1.4762 1.18078   2013-01-01 00:00:00 0.581686 0.541863 0.541863 0.581686 0.561774   2014-01-01 00:00:00 0.390671 -0.0141989 -0.0141989 0.390671 0.188236   2015-01-01 00:00:00 1.14842 1.29417 1.14842 1.29417 1.2213    We will make the same work with RCMs in future condition with rcp8.5 scenario and RCMs in historical conditions.\nlist_rcp85 = [\u0026#39;CRCM5-v1_NAM-44_ll_CCCma-CanESM2_rcp85\u0026#39;,\u0026#39;CRCM5-v1_NAM-44_ll_MPI-M-MPI-ESM-MR_rcp85\u0026#39;] df_rcp85 = [] matrix_85 = [] for period in list_period: globals()[\u0026#39;flattened_list_\u0026#39;+period] = [] for i in range(0,len(list_rcp85)): filename= rep + \u0026#39;anomalie_\u0026#39; + list_rcp85[i] + \u0026#39;_\u0026#39; + variable_in + \u0026#39;_\u0026#39; + period + \u0026#39;_1971-2000.nc\u0026#39; nc = netCDF4.Dataset(filename) var = nc.variables[variable_in][:] lats = nc.variables[\u0026#39;lat\u0026#39;][:]; lons = nc.variables[\u0026#39;lon\u0026#39;][:] subset = ((lats \u0026gt; latbounds[0]) \u0026amp; (lats \u0026lt; latbounds[1]) \u0026amp; (lons \u0026gt; lonbounds[0]) \u0026amp; (lons \u0026lt; lonbounds[1])) #mask = np.where(subset) data=pd.DataFrame(var[:,subset], dtype=\u0026#39;float\u0026#39;) globals()[\u0026#39;flattened_list_\u0026#39;+period].append(data.mean(axis=1)) df_rcp85.append(pd.DataFrame(globals()[\u0026#39;flattened_list_\u0026#39;+period]).T) df_rcp85 = pd.concat(df_rcp85) df_rcp85[\u0026#39;Date\u0026#39;] = TIME df_rcp85.index = df_rcp85[\u0026#39;Date\u0026#39;] df_rcp85 = df_rcp85.drop([\u0026#34;Date\u0026#34;], axis=1) df_rcp85[\u0026#39;min\u0026#39;] = df_rcp85.apply(np.min, axis=1) df_rcp85[\u0026#39;max\u0026#39;] = df_rcp85.apply(np.max, axis=1) df_rcp85[\u0026#39;mean\u0026#39;] = df_rcp85.apply(np.mean, axis=1) ### historical RCMs list_histo = [\u0026#39;CRCM5-v1_NAM-44_ll_CCCma-CanESM2_historical\u0026#39;, \u0026#39;CRCM5-v1_NAM-44_ll_MPI-M-MPI-ESM-LR_historical\u0026#39;] df_histo = [] globals()[\u0026#39;flattened_list_\u0026#39;+period] = [] for i in range(0,len(list_histo)): filename= rep + \u0026#39;anomalie_\u0026#39; + list_histo[i] + \u0026#39;_\u0026#39; + variable_in + \u0026#39;_1971-2000_1971-2000.nc\u0026#39; nc = netCDF4.Dataset(filename) var = nc.variables[variable_in][:] lats = nc.variables[\u0026#39;lat\u0026#39;][:]; lons = nc.variables[\u0026#39;lon\u0026#39;][:] subset = ((lats \u0026gt; latbounds[0]) \u0026amp; (lats \u0026lt; latbounds[1]) \u0026amp; (lons \u0026gt; lonbounds[0]) \u0026amp; (lons \u0026lt; lonbounds[1])) #mask = np.where(subset) data=pd.DataFrame(var[:,subset], dtype=\u0026#39;float\u0026#39;) globals()[\u0026#39;flattened_list_\u0026#39;+period].append(data.mean(axis=1)) df_histo.append(pd.DataFrame(globals()[\u0026#39;flattened_list_\u0026#39;+period]).T) TIME=[] for y in range(1971,2001,1): TIME.append(datetime.strptime(str(y), \u0026#39;%Y\u0026#39;)) df_histo = pd.concat(df_histo) df_histo[\u0026#39;Date\u0026#39;] = TIME df_histo.index = df_histo[\u0026#39;Date\u0026#39;] df_histo = df_histo.drop([\u0026#34;Date\u0026#34;], axis=1) df_histo[\u0026#39;min\u0026#39;] = df_histo.apply(np.min, axis=1) df_histo[\u0026#39;max\u0026#39;] = df_histo.apply(np.max, axis=1) df_histo[\u0026#39;mean\u0026#39;] = df_histo.apply(np.mean, axis=1) We put all results in the same DataFrame:\nresult = [] result = pd.DataFrame({\u0026#39;min_rcp45\u0026#39;: df_rcp45[\u0026#39;min\u0026#39;], \u0026#39;max_rcp45\u0026#39;: df_rcp45[\u0026#39;max\u0026#39;],\u0026#39;mean_rcp45\u0026#39;: df_rcp45[\u0026#39;mean\u0026#39;], \u0026#39;min_rcp85\u0026#39;: df_rcp85[\u0026#39;min\u0026#39;], \u0026#39;max_rcp85\u0026#39;: df_rcp85[\u0026#39;max\u0026#39;],\u0026#39;mean_rcp85\u0026#39;: df_rcp85[\u0026#39;mean\u0026#39;], \u0026#39;min_histo\u0026#39;: df_histo[\u0026#39;min\u0026#39;], \u0026#39;max_histo\u0026#39;: df_histo[\u0026#39;max\u0026#39;],\u0026#39;mean_histo\u0026#39;: df_histo[\u0026#39;mean\u0026#39;]}, columns = [\u0026#39;min_rcp45\u0026#39;,\u0026#39;max_rcp45\u0026#39;,\u0026#39;mean_rcp45\u0026#39;,\u0026#39;min_rcp85\u0026#39;,\u0026#39;max_rcp85\u0026#39;,\u0026#39;mean_rcp85\u0026#39;,\u0026#39;min_histo\u0026#39;,\u0026#39;max_histo\u0026#39;,\u0026#39;mean_histo\u0026#39;]) result.tail()    Date min_rcp45 max_rcp45 mean_rcp45 min_rcp85 max_rcp85 mean_rcp85 min_histo max_histo mean_histo     2096-01-01 00:00:00 1.89008 4.49628 3.19318 5.44184 8.03633 6.73908 nan nan nan   2097-01-01 00:00:00 3.86389 5.06262 4.46325 6.10506 7.35563 6.73034 nan nan nan   2098-01-01 00:00:00 2.28996 4.60365 3.4468 6.58306 7.14778 6.86542 nan nan nan   2099-01-01 00:00:00 2.94984 4.26892 3.60938 6.40518 7.04316 6.72417 nan nan nan   2100-01-01 00:00:00 2.99867 3.27685 3.13776 6.16794 6.96608 6.56701 nan nan nan    We can now plot our inter-annual variability:\ncolor = [\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;] fig = plt.figure(figsize=(10, 6)) gs = gridspec.GridSpec(1, 2, width_ratios=[6, 1]) gs.update( wspace=0.04) ax1 = plt.subplot(gs[0]) plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] #  plt.plot(result.index.year, result[\u0026#39;mean_histo\u0026#39;][:], label=\u0026#39;RCMs historical\u0026#39;, linewidth=2, c=color[0]) plt.plot(result.index.year, result[\u0026#39;mean_rcp45\u0026#39;][:], label=\u0026#39;RCMs scenario rcp 4.5\u0026#39;, linewidth=2, c=color[1]) plt.plot(result.index.year, result[\u0026#39;mean_rcp85\u0026#39;][:], label=\u0026#39;RCMs scenario rcp 8.5\u0026#39;, linewidth=2, c=color[2]) plt.fill_between(result.index.year,result[\u0026#39;min_histo\u0026#39;],result[\u0026#39;max_histo\u0026#39;], color = color[0], alpha=.2) plt.fill_between(result.index.year,result[\u0026#39;min_rcp45\u0026#39;],result[\u0026#39;max_rcp45\u0026#39;], color = color[1], alpha=.2) plt.fill_between(result.index.year,result[\u0026#39;min_rcp85\u0026#39;],result[\u0026#39;max_rcp85\u0026#39;], color = color[2], alpha=.2) plt.legend(loc=\u0026#34;upper left\u0026#34;, markerscale=1., scatterpoints=1, fontsize=20) plt.xticks(range(result.index.year[0]-1, result.index.year[-1]+1, 10), fontsize=14) plt.yticks( fontsize=14) ax1.grid(axis = \u0026#34;x\u0026#34;, linestyle = \u0026#34;--\u0026#34;, color=\u0026#39;black\u0026#39;, linewidth=0.25, alpha=0.5) ax1.grid(axis = \u0026#34;y\u0026#34;, linestyle = \u0026#34;--\u0026#34;, color=\u0026#39;black\u0026#39;, linewidth=0.25, alpha=0.5) plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\u0026#34;right\u0026#34;) plt.xlabel(\u0026#39;Date\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.ylabel(\u0026#39;°C\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.title(\u0026#39;Annual change in daily maximum temperature: (1971-2100) compared with normal (1971-2000)\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) ax1.set_facecolor(\u0026#39;white\u0026#39;) plt.yticks( fontsize=14) plt.show() We would add next to our chart a boxplot on all models for the period 1971-2000 and 2071-2100 only. We will extract these periods from our matrix result = []\nlist_to_remove = [\u0026#39;min\u0026#39;,\u0026#39;max\u0026#39;,\u0026#39;mean\u0026#39;] df_histo = df_histo.drop(list_to_remove, axis=1) df_rcp45 = df_rcp45.drop(list_to_remove, axis=1) df_rcp85 = df_rcp85.drop(list_to_remove, axis=1) df_histo = df_histo.loc[\u0026#39;1971\u0026#39; : \u0026#39;2010\u0026#39;].stack() df_rcp45 = df_rcp45.loc[\u0026#39;2071\u0026#39; : \u0026#39;2100\u0026#39;].stack() df_rcp85 = df_rcp85.loc[\u0026#39;2071\u0026#39; : \u0026#39;2100\u0026#39;].stack() matrix_box = pd.DataFrame({\u0026#39;RCMs_histo\u0026#39;: df_histo, \u0026#39;RCMs_rcp45\u0026#39;: df_rcp45,\u0026#39;RCMs_rcp85\u0026#39;: df_rcp85}, columns = [\u0026#39;RCMs_histo\u0026#39;,\u0026#39;RCMs_rcp45\u0026#39;,\u0026#39;RCMs_rcp85\u0026#39;]) matrix_box.head()     RCMs_histo RCMs_rcp45 RCMs_rcp85     (Timestamp(\u0026lsquo;1971-01-01 00:00:00\u0026rsquo;), 0) -1.34157 nan nan   (Timestamp(\u0026lsquo;1971-01-01 00:00:00\u0026rsquo;), 1) -0.12412 nan nan   (Timestamp(\u0026lsquo;1972-01-01 00:00:00\u0026rsquo;), 0) -0.641267 nan nan   (Timestamp(\u0026lsquo;1972-01-01 00:00:00\u0026rsquo;), 1) -0.0350673 nan nan   (Timestamp(\u0026lsquo;1973-01-01 00:00:00\u0026rsquo;), 0) 0.0541643 nan nan    color = [\u0026#39;black\u0026#39;,\u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;] fig = plt.figure(figsize=(22, 12)) gs = gridspec.GridSpec(1, 2, width_ratios=[6, 1]) gs.update( wspace=0.04) ax1 = plt.subplot(gs[0]) plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] #  plt.plot(result.index.year, result[\u0026#39;mean_histo\u0026#39;][:], label=\u0026#39;RCMs historical\u0026#39;, linewidth=2, c=color[0]) plt.plot(result.index.year, result[\u0026#39;mean_rcp45\u0026#39;][:], label=\u0026#39;RCMs scenario rcp 4.5\u0026#39;, linewidth=2, c=color[1]) plt.plot(result.index.year, result[\u0026#39;mean_rcp85\u0026#39;][:], label=\u0026#39;RCMs scenario rcp 8.5\u0026#39;, linewidth=2, c=color[2]) plt.fill_between(result.index.year,result[\u0026#39;min_histo\u0026#39;],result[\u0026#39;max_histo\u0026#39;], color = color[0], alpha=.2) plt.fill_between(result.index.year,result[\u0026#39;min_rcp45\u0026#39;],result[\u0026#39;max_rcp45\u0026#39;], color = color[1], alpha=.2) plt.fill_between(result.index.year,result[\u0026#39;min_rcp85\u0026#39;],result[\u0026#39;max_rcp85\u0026#39;], color = color[2], alpha=.2) plt.legend(loc=\u0026#34;upper left\u0026#34;, markerscale=1., scatterpoints=1, fontsize=20) #ax.set_xlim(result.index.year[0], result.index.year[-1]) plt.xticks(range(result.index.year[0]-1, result.index.year[-1]+1, 10), fontsize=14) plt.yticks( fontsize=14) ax1.grid(axis = \u0026#34;x\u0026#34;, linestyle = \u0026#34;--\u0026#34;, color=\u0026#39;black\u0026#39;, linewidth=0.25, alpha=0.5) ax1.grid(axis = \u0026#34;y\u0026#34;, linestyle = \u0026#34;--\u0026#34;, color=\u0026#39;black\u0026#39;, linewidth=0.25, alpha=0.5) plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\u0026#34;right\u0026#34;) plt.xlabel(\u0026#39;Date\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.ylabel(\u0026#39;°C\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.title(\u0026#39;Annual change in daily maximum temperature: (1971-2100) compared with normal (1971-2000)\u0026#39;, fontsize=20, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) my_pal = {\u0026#34;RCMs_histo\u0026#34;: \u0026#34;grey\u0026#34;, \u0026#34;RCMs_rcp45\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;RCMs_rcp85\u0026#34;:\u0026#34;red\u0026#34;} ax2 = plt.subplot(gs[1]) #ax2 = matrix_box.boxplot(column=[\u0026#39;RCMs_histo\u0026#39;, \u0026#39;RCMs_rcp45\u0026#39;, \u0026#39;RCMs_rcp85\u0026#39;]) ax2 = sns.boxplot(data=matrix_box, palette=my_pal) # Add transparency to colors for patch in ax2.artists: r, g, b, a = patch.get_facecolor() patch.set_facecolor((r, g, b, .2)) plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\u0026#34;right\u0026#34;) ax1.set_facecolor(\u0026#39;white\u0026#39;) ax2.set_facecolor(\u0026#39;white\u0026#39;) ax2.spines[\u0026#39;top\u0026#39;].set_visible(False) ax2.spines[\u0026#39;bottom\u0026#39;].set_visible(False) ax2.spines[\u0026#39;right\u0026#39;].set_visible(False) ax2.spines[\u0026#39;left\u0026#39;].set_visible(False) medians = matrix_box.median().values median_labels = [str(np.round(s, 2))+\u0026#39;°C\u0026#39; for s in medians] pos = range(len(medians)) i=0 for tick,label in zip(pos,ax2.get_xticklabels()): ax2.text(pos[tick], medians[tick] + 0.1, median_labels[tick], horizontalalignment=\u0026#39;center\u0026#39;, size=\u0026#39;medium\u0026#39;, color = color[i], weight=\u0026#39;semibold\u0026#39;) i+=1 x1, x2, x3 = 0, 1, 2 ax2.text(x1, matrix_box.min().min().round()-0.15 , \u0026#34;1971-2000\u0026#34;, ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;, size=\u0026#39;medium\u0026#39;, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) ax2.text((x2+x3)*.5, matrix_box.min().median()-0.4 , \u0026#34;2071-2100\u0026#34;, ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;, size=\u0026#39;medium\u0026#39;, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.yticks( fontsize=14) plt.savefig(\u0026#39;./figures/VI_YEAR_Mean_tasmax.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, format=\u0026#39;png\u0026#39;, dpi=1000) plt.show() ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"fec61d30f4b6458db6e7354598fb7734","permalink":"/courses/tutorial_python_netcdf/2-netcdf_tutorial_2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python_netcdf/2-netcdf_tutorial_2/","section":"courses","summary":"Instead of extracting a grid point, we may want to extract a domain / box delimited by latitudes and longitudes.\nIn this example we will work with the annual mean of daily maximum temperature data from several regional models of CORDEX-NAM44.\nWe will extract all the grid points from the region between 47degN and 51degN of latitude and between -72degW and -64degW of longitude .\nWe will then plot the inter-annual variability of the mean annual temperature anomalies.","tags":null,"title":"2 Netcdf Part 2","type":"docs"},{"authors":null,"categories":null,"content":"The Cartopy python library allows you to analyze, process and plot georeferenced data with the help of Matplotlib.\nhttps://scitools.org.uk/cartopy/docs/latest/#\nTo install the library under the prompt of anaconda: conda install -c conda-forge cartopy\n1 - Example of drawing a 2D Netcdf field  We will work climatology (1981-2010) monthly minimum daily temperature At first we will import the Python libraries and open the Netcdf file  The Dataset class of the netCDF4 module is used to open and read Netcdf files. Here we will store the Netcdf file in the variable nc_fid.\nfrom netCDF4 import Dataset import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) filename=\u0026#39;./DATA/ANUSPLIN/ANUSPLIN_NLDAS_10km_CLIMATO_TASMIN_1981_2010_06.nc\u0026#39; nc_fid=Dataset(filename,\u0026#39;r\u0026#39;) nc_fid.variables OrderedDict([('lon', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 lon(y, x) units: degrees_east long_name: Longitude CoordinateAxisType: Lon unlimited dimensions: current shape = (1068, 420) filling on, default _FillValue of 9.969209968386869e+36 used), ('lat', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 lat(y, x) units: degrees_north long_name: Latitude CoordinateAxisType: Lat unlimited dimensions: current shape = (1068, 420) filling on, default _FillValue of 9.969209968386869e+36 used), ('time', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float64 time(time) long_name: Time delta_t: units: Days unlimited dimensions: current shape = (1,) filling on, default _FillValue of 9.969209968386869e+36 used), ('TASMIN', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 TASMIN(time, y, x) long_name: Daily minimum temperature units: Celcius missing_value: -999.0 coordinates: lon lat unlimited dimensions: current shape = (1, 1068, 420) filling on, default _FillValue of 9.969209968386869e+36 used)])  We will read TASMIN, lat and lon variables.\nlats = nc_fid.variables[\u0026#39;lat\u0026#39;][:] lons = nc_fid.variables[\u0026#39;lon\u0026#39;][:] time = nc_fid.variables[\u0026#39;time\u0026#39;][:] Vals = nc_fid.variables[\u0026#39;TASMIN\u0026#39;][:].squeeze() print( Vals.shape) (1068, 420)  We will now call the cartopy and matplotlib libraries from Python to create a graphical instance. The carto library brings functions to visually enrich maps made with cartopy: adding a scale \u0026hellip;\nimport matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np import matplotlib as mpl from carto import scale_bar We create an instance of Cartopy to create a map. We choose here our projection, grid orientation and total coverage.\nHere is a very useful link to choose a projection type: https://scitools.org.uk/cartopy/docs/latest/crs/projections.html\nprojections = [ccrs.PlateCarree(), ccrs.Robinson(), ccrs.Mercator(), ccrs.Orthographic(), ccrs.InterruptedGoodeHomolosine(), ccrs.LambertConformal(), ] for proj in projections: plt.figure() ax = plt.axes(projection=proj) ax.stock_img() ax.coastlines() ax.set_title(f\u0026#39;{type(proj)}\u0026#39;) In this example, we will work with a so-called \u0026ldquo;LambertConformal\u0026rdquo; projection. To create a regional map, we use the GeoAxis set_extent method to limit the size of our region.\nfig=plt.figure(figsize=(5,3), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-100,-60,18,62]) We will add some products to our map with Cartopy's cartopy.feature method: https://scitools.org.uk/cartopy/docs/v0.14/matplotlib/feature_interface.html\n         cartopy.feature.BORDERS Borders   cartopy.feature.COASTLINE Coast   cartopy.feature.LAKES Lakes   cartopy.feature.LAND Continents   cartopy.feature.OCEAN Ocean   cartopy.feature.RIVERS Rivers   cartopy.feature.STATES States    We will also add the provinces with Cartopy's NaturalEarthFeature class: https://scitools.org.uk/cartopy/docs/v0.16/matplotlib/feature_interface.html#cartopy.feature.NaturalEarthFeature\nWe work here by adding layers.\n# adding caost: ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); plt.show() # adding land and oceanss: ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) plt.show() # adding lakes and rivers ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) plt.show() # Adding states ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) plt.show() We can now fill our map with our georeferenced field with the method pcolormesh of matplotlib: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pcolormesh.html\nHere Lat Lon coordinates are in 2D, so in each grid point we know the latitude and longitude of our fields.\nIt is possible that for a netcdf file Lat and Lon are in 1D. We must then write them in 2D with the following command:\n-lon, lat = np.meshgrid (lons, lats)\nWe will also produce our own color palette. Here is a useful link to select our colors and create our palette: http://colorbrewer2.org/#type=diverging\u0026amp;scheme=RdYlBu\u0026amp;n=8\nYou can also use one of the color palettes predefined by Matplotlip: https://matplotlib.org/examples/color/colormaps_reference.html\nWe will use to draw our map type pcolormesh matplotib. Other types of plots are available: https://matplotlib.org/basemap/users/examples.html\nTo improve the reading of our map, we can at this level add:\n a legend under the map: cbar.set_label add a color bar with a display interval: plt.colorbar  Y=np.array([[77,0,111],[115,14,181],[160,17,222],[195,14,240],\\ [0,0,93],[21,38,177],[33,95,227],[32,162,247],[59,224,248],[202,255,250],\\ [4,255,179],[37,181,139],[32,132,81],[72,162,60],[157,240,96],[213,255,166],\\ [241,247,132],[248,185,68],[255,124,4],[235,78,14],[215,32,24],[189,24,40],[162,16,56],[135,16,65],[107,15,73]])/255. colbar=mpl.colors.ListedColormap(Y) fig=plt.figure(figsize=(10,6), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) mm = ax.pcolormesh(lons,\\ lats,\\ Vals,\\ vmin=-28,\\ vmax=28, \\ transform=ccrs.PlateCarree(),\\ cmap=colbar ) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); plt.show() If the pallet is not suitable, it is possible to use a palette of Matplotlib.\nfig=plt.figure(figsize=(10,6), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) mm = ax.pcolormesh(lons,\\ lats,\\ Vals,\\ vmin=-28,\\ vmax=28, \\ transform=ccrs.PlateCarree(),\\ cmap=\u0026#39;jet\u0026#39; ) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); plt.show() We can finally add: - a title to our chart: plt.title - a color bar plt.colorbar - a title to our color bar: plt.xlabel - latitudes and longitudes with the function: gridlines () - a scale bar: scale_bar ()\nThen save our chart with the command: plt.savefig\nY=np.array([[77,0,111],[115,14,181],[160,17,222],[195,14,240],\\ [0,0,93],[21,38,177],[33,95,227],[32,162,247],[59,224,248],[202,255,250],\\ [4,255,179],[37,181,139],[32,132,81],[72,162,60],[157,240,96],[213,255,166],\\ [241,247,132],[248,185,68],[255,124,4],[235,78,14],[215,32,24],[189,24,40],[162,16,56],[135,16,65],[107,15,73]])/255. colbar=mpl.colors.ListedColormap(Y) fig=plt.figure(figsize=(22,12), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) mm = ax.pcolormesh(lons,\\ lats,\\ Vals,\\ vmin=-28,\\ vmax=28, \\ transform=ccrs.PlateCarree(),\\ cmap=colbar ) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) ax.gridlines() # Standard 6,000 km scale bar. scale_bar(ax, (0.85, 0.2), 500 ,plot_kwargs = dict(linestyle=\u0026#39;dashed\u0026#39;, color=\u0026#39;black\u0026#39;)) cbar = plt.colorbar(mm, orientation=\u0026#39;horizontal\u0026#39;, shrink=0.75, drawedges=\u0026#39;True\u0026#39;, ticks=np.arange(-28, 28.1, 4),extend=\u0026#39;both\u0026#39;) cbar.set_label(u\u0026#39;\\nProjection = Lambert Conformal Conic / Lambert Conique Conforme\\nResolution: 5 Arcs-Minutes (10 km)\\nData provided by Natural Resources Canada and the University of Washington/ Données fournies par Ressources Naturelles Canada et l Université de Washington\u0026#39;, size=\u0026#39;medium\u0026#39;) # Affichage de la légende de la barre de couleur cbar.ax.tick_params(labelsize=17) plt.xlabel(u\u0026#39;\\n\\n\\nTemperature / Température (°C)\u0026#39;,size=\u0026#39;x-large\u0026#39;) string_title=u\u0026#39;Daily Minimum Temperature / Température Minimale Quotidienne\\nin June / en Juin \\nMonthly Mean Climatology / Climatologie Moyenne Mensuelle (1981 - 2010)\\n\\n\u0026#39; plt.title(string_title, size=\u0026#39;xx-large\u0026#39;) plt.savefig(\u0026#39;./figures/ANUSPLIN_NLDAS_10km_CLIM_TASMIN_06_1981-2010.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() 2 - Another example 2D Netcdf  We will work with the climatology of the total monthly precipitation accumulation (1981-2010) for the month of December.  from netCDF4 import Dataset import matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np from carto import scale_bar rep_data=\u0026#39;./DATA/ANUSPLIN/\u0026#39; fic=rep_data+\u0026#39;ANUSPLIN_NLDAS_10km_CLIMATO_PrecTOT_1981_2010_12.nc\u0026#39; dset=Dataset(fic) precip=dset.variables[\u0026#39;PrecTOT\u0026#39;][:].squeeze() lon=dset.variables[\u0026#39;lon\u0026#39;][:].squeeze() lat=dset.variables[\u0026#39;lat\u0026#39;][:].squeeze() ## Interrogeons un peu chaque variable print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;dimension de precipitation = \u0026#39;,precip.shape) print(\u0026#39;Min de precip est = \u0026#39;, np.nanmin(precip)) print(\u0026#39;Max de precip est = \u0026#39;, np.nanmax(precip)) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;dimension de latitude = \u0026#39;,lat.shape) print(\u0026#39;Min de lat est = \u0026#39;, np.min(lat)) print(\u0026#39;Max de lat est = \u0026#39;, np.max(lat)) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;-----------------------------------------\u0026#39;) print(\u0026#39;dimension de longitude = \u0026#39;,lon.shape) print(\u0026#39;Min de lon est = \u0026#39;, np.min(lon)) print(\u0026#39;Max de lon est = \u0026#39;, np.max(lon)) print(\u0026#39;-----------------------------------------\u0026#39;) ----------------------------------------- dimension de precipitation = (1068, 420) Min de precip est = 2.2327309 Max de precip est = 809.1944 ----------------------------------------- ----------------------------------------- dimension de latitude = (1068, 420) Min de lat est = 25.125 Max de lat est = 60.041664 ----------------------------------------- ----------------------------------------- dimension de longitude = (1068, 420) Min de lon est = -140.95833 Max de lon est = -52.04167 -----------------------------------------  fig = plt.figure(figsize=(22,12)) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ## Choisissons une colormap cmap0=plt.cm.jet_r cmap0.set_under(\u0026#39;w\u0026#39;) ## on met en blanc les valeurs inferieures au min de clev cmap0.set_over(\u0026#39;darkblue\u0026#39;) ## bleu fonce pour les valeurs extremes de pluie mm = ax.pcolormesh(lon,\\ lat,\\ precip,\\ vmin=0,\\ vmax=150, \\ transform=ccrs.PlateCarree(),\\ cmap=cmap0 ) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); ax.gridlines() # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) fig.canvas.draw() # Standard 6,000 km scale bar. scale_bar(ax, (0.85, 0.2), 500 ,plot_kwargs = dict(linestyle=\u0026#39;dashed\u0026#39;, color=\u0026#39;black\u0026#39;)) cbar = plt.colorbar(mm, shrink=0.75, drawedges=\u0026#39;True\u0026#39;, ticks=np.arange(0, 150.1, 20),extend=\u0026#39;both\u0026#39;) cbar.ax.tick_params(labelsize=17) plt.xlabel(u\u0026#39;\\Precipitation (mm)\u0026#39;,size=\u0026#39;x-large\u0026#39;) string_title=u\u0026#39;Precipitation climatology ANUSPLIN: December 1981-2010\u0026#39; plt.title(string_title, size=\u0026#39;xx-large\u0026#39;) plt.savefig(\u0026#39;./figures/My_2Dlalon_plot.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1, dpi=150) plt.show() plt.close() 3 - Cartopy: example with overplot Sometimes we want to have many plots on the same picture.\n We will work the climatologies of the total monthly accumulation of precipitation (1981-2010) for all the months of the year.  We will produce the same previous operation but for each month using a loop for.\n## we import librairies that we need from netCDF4 import Dataset import matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np from carto import scale_bar month_name=[\u0026#34;Jan\u0026#34;,\u0026#34;Feb\u0026#34;,\u0026#34;Mar\u0026#34;,\u0026#34;Apr\u0026#34;,\u0026#34;May\u0026#34;,\u0026#34;June\u0026#34;,\u0026#34;Jul\u0026#34;,\u0026#34;Aug\u0026#34;,\u0026#34;Sep\u0026#34;,\u0026#34;Oct\u0026#34;,\u0026#34;Nov\u0026#34;,\u0026#34;Dec\u0026#34;] rep_data=\u0026#39;./DATA/ANUSPLIN/\u0026#39; fig=plt.figure(figsize=(12,15)) clevs=np.arange(5,150.1,5) ## Colormap we will use cmap0=plt.cm.jet_r cmap0.set_under(\u0026#39;w\u0026#39;) cmap0.set_over(\u0026#39;darkblue\u0026#39;) for imonth in np.arange(1,13): # loop over 12 months ax=fig.add_subplot(4,3,imonth, projection=ccrs.LambertConformal()) fic=rep_data+\u0026#39;ANUSPLIN_NLDAS_10km_CLIMATO_PrecTOT_1981_2010_\u0026#39;+\u0026#39;{:02d}\u0026#39;.format(imonth)+\u0026#39;.nc\u0026#39; dset=Dataset(fic) precip=dset.variables[\u0026#39;PrecTOT\u0026#39;][:].squeeze() lon=dset.variables[\u0026#39;lon\u0026#39;][:].squeeze() lat=dset.variables[\u0026#39;lat\u0026#39;][:].squeeze() ax.set_extent([-130,-60,18,62]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) mm = ax.pcolormesh(lon,\\ lat,\\ precip,\\ vmin=0,\\ vmax=150, \\ transform=ccrs.PlateCarree(),\\ cmap=cmap0 ) plt.title(\u0026#39;Normale \u0026#39;+ month_name[imonth-1],fontsize=12) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.stock_img(); ax.gridlines(); # Standard 6,000 km scale bar. scale_bar(ax, (0.85, 0.2), 500 ,plot_kwargs = dict(linestyle=\u0026#39;dashed\u0026#39;, color=\u0026#39;black\u0026#39;)) plt.savefig(\u0026#39;./figures/My_2Dlalon_multipanel_plot.png\u0026#39;) plt.show() 4 - Cartopy: MODIS https://lance-modis.eosdis.nasa.gov/imagery/gallery/2012270-0926/Miriam.A2012270.2050.2km.jpg\nhttps://lance-modis.eosdis.nasa.gov/imagery/gallery/2012270-0926/Miriam.A2012270.2050.txt\nfig = plt.figure(figsize=(12, 20)) fname = \u0026#39;./figures/Miriam.A2012270.2050.1km.jpg\u0026#39; img_extent = (-120.67660000000001, -106.32104523100001, 13.2301484511245, 30.766899999999502) img = plt.imread(fname) ax = plt.axes(projection=ccrs.PlateCarree()) # set a margin around the data ax.set_xmargin(0.05) ax.set_ymargin(0.10) # ajout de l\u0026#39;image ax.imshow(img, origin=\u0026#39;upper\u0026#39;, extent=img_extent, transform=ccrs.PlateCarree()) ax.coastlines(resolution=\u0026#39;50m\u0026#39;, color=\u0026#39;black\u0026#39;, linewidth=1) # ajout d\u0026#39;une ville ax.plot(-117.1625, 32.715, \u0026#39;bo\u0026#39;, markersize=7, transform=ccrs.Geodetic()) ax.text(-117, 33, \u0026#39;San Diego\u0026#39;, transform=ccrs.Geodetic()) Text(-117, 33, 'San Diego')  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"e3391e1e386ab479d03a830a4007e130","permalink":"/courses/tutorial_python_netcdf/3-cartopy_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python_netcdf/3-cartopy_library/","section":"courses","summary":"The Cartopy python library allows you to analyze, process and plot georeferenced data with the help of Matplotlib.\nhttps://scitools.org.uk/cartopy/docs/latest/#\nTo install the library under the prompt of anaconda: conda install -c conda-forge cartopy\n1 - Example of drawing a 2D Netcdf field  We will work climatology (1981-2010) monthly minimum daily temperature At first we will import the Python libraries and open the Netcdf file  The Dataset class of the netCDF4 module is used to open and read Netcdf files.","tags":null,"title":"3 Cartopy","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, we will use the features of the Python xarray library to process and analyze Netcdf files.\nTo install the library under anaconda:\n$ conda install xarray\nHere is an example of structure of a Netcdf file under xarray:\nDataArray xarray.DataArray is xarray’s implementation of a labeled, multi-dimensional array. It has several key properties: | | | |\u0026ndash;|\u0026ndash;| | values| a numpy.ndarray holding the array’s values | | dims| dimension names for each axis (e.g., (\u0026lsquo;x\u0026rsquo;, \u0026lsquo;y\u0026rsquo;, \u0026lsquo;z\u0026rsquo;,\u0026lsquo;time\u0026rsquo;)) | | coords| a dict-like container of arrays (coordinates) that label each point (e.g., 1-dimensional arrays of numbers, datetime objects or strings) | | attrs| an OrderedDict to hold arbitrary metadata (attributes) |\nDataSet xarray.Dataset is xarray’s multi-dimensional equivalent of a DataFrame. It is a dict-like container of labeled arrays (DataArray objects) with aligned dimensions. It is designed as an in-memory representation of the data model from the netCDF file format.\nxarray.DataSet is a collection of DataArrays. Each NetCDF file contains a DataSet.\n1- Open a Netcdf file We will open and store the data of a Netcdf file in a Dataset.\nFirst we need to import librairies and create aliases.\nimport xarray as xr import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) from matplotlib import pyplot as plt %matplotlib inline plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8,5)  To import and store as dataset only one Netcdf file:  We will work with temperature fields from cera20c reanalysis.\nunique_dataDIR = \u0026#39;./DATA/CERA20C/cera20c_member0_TAS_197101_day.nc\u0026#39; TAS = xr.open_dataset(unique_dataDIR) TAS \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 31) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-01-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] ... t2m (time, latitude, longitude) float32 ... Attributes: CDI: Climate Data Interface version 1.6.9 (http://mpimet.mpg.de/... Conventions: CF-1.6 history: Thu Oct 25 14:29:40 2018: cdo daymean ./TAS/cera20c_member0... CDO: Climate Data Operators version 1.6.9 (http://mpimet.mpg.de/...   If we want to import several Netcdf files:  Here, we want to store all files\u0026rsquo; names starting with \u0026lsquo;cera20c_member0_TAS_\u0026rsquo; and located in \u0026lsquo;./data/cera20c/\u0026rsquo; path.\nmulti_dataDIR = \u0026#39;./DATA/CERA20C/cera20c_member0_TAS_*.nc\u0026#39; TAS2 = xr.open_mfdataset(multi_dataDIR) TAS2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; Attributes: CDI: Climate Data Interface version 1.6.9 (http://mpimet.mpg.de/... Conventions: CF-1.6 history: Thu Oct 25 14:29:40 2018: cdo daymean ./TAS/cera20c_member0... CDO: Climate Data Operators version 1.6.9 (http://mpimet.mpg.de/...   Combine Netcdf:  To combine variables and coordinates between multiple DataArray and/or Dataset objects, use merge(). It can merge a list of Dataset, DataArray or dictionaries of objects convertible to DataArray objects:\nmulti_dataDIR = \u0026#39;./DATA/CERA20C/cera20c_member0_TAS_*.nc\u0026#39; TAS2 = xr.open_mfdataset(multi_dataDIR) multi_dataDIR2 = \u0026#39;./DATA/CERA20C/cera20c_member0_SIC_*.nc\u0026#39; SIC2 = xr.open_mfdataset(multi_dataDIR2) DS_new = xr.merge([TAS2,SIC2]) DS_new \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt;  2- Exploring the data We can quickly explore our datasets by using some methods of the xarray library:\n DS.var DS.dims DS.coords DS.attrs  DS_new.var \u0026lt;bound method ImplementsDatasetReduce._reduce_method.\u0026lt;locals\u0026gt;.wrapped_func of \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt;\u0026gt;  DS_new.dims Frozen(SortedKeysDict({'longitude': 288, 'latitude': 145, 'time': 365, 'bnds': 2}))  DS_new.coords Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00  DS_new.attrs OrderedDict()  3- Basic operations with Xarray:  Select a date:  We can use .sel() method to select one timestamp from our Dataset.\nDS_date = DS_new.sel(time=\u0026#39;1971-01-01\u0026#39;) DS_date \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 1) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(1, 2), chunksize=(1, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(1, 145, 288), chunksize=(1, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(1, 145, 288), chunksize=(1, 145, 288)\u0026gt;   Select time range  We can select a time range with slicing\nDS_date_range = DS_new.sel(time=slice(\u0026#39;1971-06-01\u0026#39;, \u0026#39;1971-08-31\u0026#39;)) DS_date_range \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 92) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-06-01T10:30:00 ... 1971-08-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(92, 2), chunksize=(30, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(92, 145, 288), chunksize=(30, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(92, 145, 288), chunksize=(30, 145, 288)\u0026gt;   Export a dataset  We can export our dataset into dataframe and then use Pandas library to make analysis.\ndf = DS_date_range.to_dataframe() df.head()     time_bnds t2m siconc     (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-01 10:30:00\u0026rsquo;)) 1971-06-01 00:00:00 266.296 0.990234   (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-02 10:30:00\u0026rsquo;)) 1971-06-02 00:00:00 264.76 0.988525   (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-03 10:30:00\u0026rsquo;)) 1971-06-03 00:00:00 265.866 0.987502   (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-04 10:30:00\u0026rsquo;)) 1971-06-04 00:00:00 265.947 0.98738   (0, 90.0, 0.0, Timestamp(\u0026lsquo;1971-06-05 10:30:00\u0026rsquo;)) 1971-06-05 00:00:00 266.126 0.987152    df.describe()     t2m siconc     count 7.68384e+06 7.68384e+06   mean 267.002 0.141752   std 25.852 0.312001   min 192.562 0   25% 273.221 0   50% 285.734 0   75% 296.388 0   max 314.804 1     Time mean  Mean_array = DS_date_range.mean(dim=\u0026#39;time\u0026#39;) Mean_array.values \u0026lt;bound method Mapping.values of \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 Data variables: t2m (latitude, longitude) float32 dask.array\u0026lt;shape=(145, 288), chunksize=(145, 288)\u0026gt; siconc (latitude, longitude) float32 dask.array\u0026lt;shape=(145, 288), chunksize=(145, 288)\u0026gt;\u0026gt;   To save our results into csv:  Mean_array.t2m.to_dataframe().to_csv(\u0026#39;./DATA/CERA20C_T2m_mean.csv\u0026#39;)  Mean over all latitudes and longitudes grid points:  DS_date_range.t2m.mean(dim=(\u0026#39;latitude\u0026#39;, \u0026#39;longitude\u0026#39;)) \u0026lt;xarray.DataArray 't2m' (time: 92)\u0026gt; dask.array\u0026lt;shape=(92,), dtype=float32, chunksize=(30,)\u0026gt; Coordinates: * time (time) datetime64[ns] 1971-06-01T10:30:00 ... 1971-08-31T10:30:00  DS_date_range.t2m.mean(dim=(\u0026#39;latitude\u0026#39;, \u0026#39;longitude\u0026#39;)).plot() [\u0026lt;matplotlib.lines.Line2D at 0xabe86a0\u0026gt;]   To save into csv:  DS_date_range.t2m.mean(dim=(\u0026#39;time\u0026#39;, \u0026#39;longitude\u0026#39;)).to_dataframe().to_csv(\u0026#39;./DATA/CERA20C_T2m_2Dmean.csv\u0026#39;)  Quick plot with Xarray  import cartopy.crs as ccrs fig=plt.figure(figsize=(10,10), frameon=True) ax = plt.axes(projection=ccrs.Orthographic(-80, 35)) Mean_array.t2m.plot.contourf(ax=ax, transform=ccrs.PlateCarree()); ax.set_global(); ax.coastlines();  Basic operations with Xarray:  In this example, we will mean DS_date_range over time and apply a substration to change units.\ncentigrade = DS_date_range.t2m.mean(dim=\u0026#39;time\u0026#39;) - 273.16 centigrade.values array([[ -1.2229004, -1.2229004, -1.2229004, ..., -1.2229004, -1.2229004, -1.2229004], [ -1.3348389, -1.3311157, -1.3274231, ..., -1.3488159, -1.3442383, -1.3395386], [ -1.6027222, -1.5914001, -1.5804138, ..., -1.6451721, -1.631012 , -1.6168518], ..., [-58.547928 , -58.5663 , -58.58455 , ..., -58.314026 , -58.391983 , -58.46997 ], [-59.556473 , -59.577957 , -59.59955 , ..., -59.478424 , -59.50441 , -59.53041 ], [-60.739105 , -60.739105 , -60.739105 , ..., -60.739105 , -60.739105 , -60.739105 ]], dtype=float32)  fig=plt.figure(figsize=(10,10), frameon=True) ax = plt.axes(projection=ccrs.Orthographic(-80, 35)) centigrade.plot.contourf(ax=ax, transform=ccrs.PlateCarree()); ax.set_global(); ax.coastlines();  Groupby() method:  groupdby() method with Xarray is very usefull to group our datasets by month, season, year\u0026hellip; and then apply function to compute indices.\nDS_new \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt;  # monthly mean: DS_month = DS_new.groupby(\u0026#39;time.month\u0026#39;).mean(\u0026#39;time\u0026#39;) DS_month #DS_month.to_dataframe() \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288, month: 12) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * month (month) int64 1 2 3 4 5 6 7 8 9 10 11 12 Data variables: t2m (month, latitude, longitude) float32 dask.array\u0026lt;shape=(12, 145, 288), chunksize=(1, 145, 288)\u0026gt; siconc (month, latitude, longitude) float32 dask.array\u0026lt;shape=(12, 145, 288), chunksize=(1, 145, 288)\u0026gt;  We can use this method to compute climatology and then anomalies.\nclimatology = DS_new.groupby(\u0026#39;time.month\u0026#39;).mean(\u0026#39;time\u0026#39;) anomalies = DS_new.groupby(\u0026#39;time.month\u0026#39;) - climatology anomalies \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288, time: 365) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 month (time) int64 1 1 1 1 1 1 1 1 1 1 ... 12 12 12 12 12 12 12 12 12 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 145, 288), chunksize=(31, 145, 288)\u0026gt;  # seaon mean: DS_season = DS_new.groupby(\u0026#39;time.season\u0026#39;).mean(\u0026#39;time\u0026#39;) DS_season \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288, season: 4) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * season (season) object 'DJF' 'JJA' 'MAM' 'SON' Data variables: t2m (season, latitude, longitude) float32 dask.array\u0026lt;shape=(4, 145, 288), chunksize=(1, 145, 288)\u0026gt; siconc (season, latitude, longitude) float32 dask.array\u0026lt;shape=(4, 145, 288), chunksize=(1, 145, 288)\u0026gt;  # year mean: DS_year = DS_new.groupby(\u0026#39;time.year\u0026#39;).mean(\u0026#39;time\u0026#39;) DS_year \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288, year: 1) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * year (year) int64 1971 Data variables: t2m (year, latitude, longitude) float32 dask.array\u0026lt;shape=(1, 145, 288), chunksize=(1, 145, 288)\u0026gt; siconc (year, latitude, longitude) float32 dask.array\u0026lt;shape=(1, 145, 288), chunksize=(1, 145, 288)\u0026gt;   to select a specific season:  DS_winter = DS_season.sel(season=\u0026#39;DJF\u0026#39;) DS_winter \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288) Coordinates: * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 season \u0026lt;U3 'DJF' Data variables: t2m (latitude, longitude) float32 dask.array\u0026lt;shape=(145, 288), chunksize=(145, 288)\u0026gt; siconc (latitude, longitude) float32 dask.array\u0026lt;shape=(145, 288), chunksize=(145, 288)\u0026gt;  In the example below, we will group the xarray.DataArray data by season, calculate the average, apply a simple arrhythmic operation and plot the resulting fields for each season.\nDS_Season = DS_new.t2m.groupby(\u0026#39;time.season\u0026#39;).mean(\u0026#39;time\u0026#39;)- 273.15 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(9,5)) j = 0 for i, season in enumerate((\u0026#39;DJF\u0026#39;, \u0026#39;MAM\u0026#39;, \u0026#39;JJA\u0026#39;, \u0026#39;SON\u0026#39;)): if season ==\u0026#39;JJA\u0026#39;: j += 1 i = 0 elif season ==\u0026#39;SON\u0026#39;: i = 1 DS_Season.sel(season=season).plot.pcolormesh( ax=axes[i, j], vmin=-30, vmax=30, cmap=\u0026#39;Spectral_r\u0026#39;, add_colorbar=True, extend=\u0026#39;both\u0026#39;) for ax in axes.flat: ax.axes.get_xaxis().set_ticklabels([]) ax.axes.get_yaxis().set_ticklabels([]) ax.axes.axis(\u0026#39;tight\u0026#39;) plt.tight_layout() fig.suptitle(\u0026#39;Seasonal Surface Air Temperature\u0026#39;, fontsize=16, y=1.02) Text(0.5, 1.02, 'Seasonal Surface Air Temperature')  lat_bnd = [80, 50] lon_bnd = [250, 310] DS_Season = DS_new.sel(longitude=slice(*lon_bnd), latitude=slice(*lat_bnd),).siconc.groupby(\u0026#39;time.season\u0026#39;).mean(\u0026#39;time\u0026#39;) *100 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16,10)) j = 0 for i, season in enumerate((\u0026#39;DJF\u0026#39;, \u0026#39;MAM\u0026#39;, \u0026#39;JJA\u0026#39;, \u0026#39;SON\u0026#39;)): if season ==\u0026#39;JJA\u0026#39;: j += 1 i = 0 elif season ==\u0026#39;SON\u0026#39;: i = 1 DS_Season.sel(season=season).plot.pcolormesh( ax=axes[i, j], vmin=0, vmax=100, cmap=\u0026#39;Spectral_r\u0026#39;, add_colorbar=True, extend=\u0026#39;both\u0026#39;) for ax in axes.flat: ax.axes.get_xaxis().set_ticklabels([]) ax.axes.get_yaxis().set_ticklabels([]) ax.axes.axis(\u0026#39;tight\u0026#39;) plt.tight_layout() fig.suptitle(\u0026#39;Seasonal Sea Ice Concentration\u0026#39;, fontsize=16, y=1.02) Text(0.5, 1.02, 'Seasonal Sea Ice Concentration')  To save our result into Netcdf:\nDS_season = DS_new.groupby(\u0026#39;time.season\u0026#39;).mean(\u0026#39;time\u0026#39;) dataDIR = \u0026#39;./DATA/CERA20C_season.nc\u0026#39; DS_Season.to_netcdf(dataDIR) 4- Select grid points from Netcdf file using Xarray In the previous section we applied the .sel () method to work on the time dimension. This method can be used on spatial dimensions to extract points or study areas from our netcdf file.\nGridpoint: to extract the closest grid point of a latitude / longitude: lati = 45.5 loni = 269.2 data = DS_new.sel(longitude=loni , latitude=lati , method=\u0026#39;nearest\u0026#39;) data \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, time: 365) Coordinates: longitude float32 268.75 latitude float32 45.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time) float32 dask.array\u0026lt;shape=(365,), chunksize=(31,)\u0026gt; siconc (time) float32 dask.array\u0026lt;shape=(365,), chunksize=(31,)\u0026gt;  data[\u0026#39;t2m\u0026#39;] = data[\u0026#39;t2m\u0026#39;] - 273.15 data.t2m \u0026lt;xarray.DataArray 't2m' (time: 365)\u0026gt; dask.array\u0026lt;shape=(365,), dtype=float32, chunksize=(31,)\u0026gt; Coordinates: longitude float32 268.75 latitude float32 45.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00  We can convert our selection into a DataFrame and then use Pandas to analyse our results.\ndf = data.t2m.to_dataframe() fig = plt.figure(figsize=(16,8)) df[\u0026#39;t2m\u0026#39;].plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x13660400\u0026gt;  Gridpoints: to extract a list of points lats = [20.0,50.0,90.0] lons = [60.0,80.0,120.0] data = DS_new.sel(longitude=lons , latitude=lats , method=\u0026#39;nearest\u0026#39;) data[\u0026#39;t2m\u0026#39;] = data[\u0026#39;t2m\u0026#39;]-273.15 data \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 3, longitude: 3, time: 365) Coordinates: * longitude (longitude) float32 60.0 80.0 120.0 * latitude (latitude) float32 20.0 50.0 90.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 3, 3), chunksize=(31, 3, 3)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 3, 3), chunksize=(31, 3, 3)\u0026gt;  fig = plt.figure(figsize=(16,8)) data.t2m.sel(longitude=60.0, latitude=[20.0,50.0,90.0]).plot.line(x=\u0026#39;time\u0026#39;) [\u0026lt;matplotlib.lines.Line2D at 0xdf31ac8\u0026gt;, \u0026lt;matplotlib.lines.Line2D at 0xdd0bf28\u0026gt;, \u0026lt;matplotlib.lines.Line2D at 0xdd0b4a8\u0026gt;]  To extract an area or subdomain delimited by latitude and longitude values: .slicing() lat_bnd = [80, 50] lon_bnd = [250, 310] area = DS_new.sel(longitude=slice(*lon_bnd), latitude=slice(*lat_bnd),) area \u0026lt;xarray.Dataset\u0026gt; Dimensions: (bnds: 2, latitude: 25, longitude: 49, time: 365) Coordinates: * longitude (longitude) float32 250.0 251.25 252.5 ... 307.5 308.75 310.0 * latitude (latitude) float32 80.0 78.75 77.5 76.25 ... 52.5 51.25 50.0 * time (time) datetime64[ns] 1971-01-01T10:30:00 ... 1971-12-31T10:30:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) datetime64[ns] dask.array\u0026lt;shape=(365, 2), chunksize=(31, 2)\u0026gt; t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 25, 49), chunksize=(31, 25, 49)\u0026gt; siconc (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 25, 49), chunksize=(31, 25, 49)\u0026gt;  area.longitude.values array([250. , 251.25, 252.5 , 253.75, 255. , 256.25, 257.5 , 258.75, 260. , 261.25, 262.5 , 263.75, 265. , 266.25, 267.5 , 268.75, 270. , 271.25, 272.5 , 273.75, 275. , 276.25, 277.5 , 278.75, 280. , 281.25, 282.5 , 283.75, 285. , 286.25, 287.5 , 288.75, 290. , 291.25, 292.5 , 293.75, 295. , 296.25, 297.5 , 298.75, 300. , 301.25, 302.5 , 303.75, 305. , 306.25, 307.5 , 308.75, 310. ], dtype=float32)  To visualize our area::\nimport cartopy.crs as ccrs import cartopy.feature as cfeat def make_figure(): fig = plt.figure(figsize=(22, 12)) ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree()) # generate a basemap with country borders, oceans and coastlines ax.add_feature(cfeat.LAND) ax.add_feature(cfeat.OCEAN) ax.add_feature(cfeat.COASTLINE) ax.add_feature(cfeat.BORDERS, linestyle=\u0026#39;dotted\u0026#39;) return fig, ax make_figure(); _, ax = make_figure() # plot the temperature field area.t2m[0].plot() \u0026lt;matplotlib.collections.QuadMesh at 0xb2e9240\u0026gt;  To mask an area delimited by a Shapefile: To do this, we need to import two more librairies:\n Geopandas: conda install -c conda-forge geopandas osgeo: conda install -c conda-forge gdal  The next function will open a shapefile, read the polygons and make a mask from each grid points of our Netcdf inside the polygons.\nfrom osgeo import ogr import geopandas as gpd import numpy as np def get_mask(lons2d, lats2d, shp_path=\u0026#34;\u0026#34;, polygon_name=None): \u0026#34;\u0026#34;\u0026#34;Assumes that the shape file contains polygons in lat lon coordinates:param lons2d::param lats2d::param shp_path::rtype : np.ndarrayThe mask is 1 for the points inside of the polygons\u0026#34;\u0026#34;\u0026#34; ds = ogr.Open(shp_path) \u0026#34;\u0026#34;\u0026#34;:type : ogr.DataSource\u0026#34;\u0026#34;\u0026#34; xx = lons2d.copy() yy = lats2d # set longitudes to be from -180 to 180 xx[xx \u0026gt; 180] -= 360 mask = np.zeros(lons2d.shape, dtype=int) nx, ny = mask.shape pt = ogr.Geometry(ogr.wkbPoint) for i in range(ds.GetLayerCount()): layer = ds.GetLayer(i) \u0026#34;\u0026#34;\u0026#34;:type : ogr.Layer\u0026#34;\u0026#34;\u0026#34; for j in range(layer.GetFeatureCount()): feat = layer.GetFeature(j) \u0026#34;\u0026#34;\u0026#34;:type : ogr.Feature\u0026#34;\u0026#34;\u0026#34; # Select polygons by the name property if polygon_name is not None: if not feat.GetFieldAsString(\u0026#34;NAME\u0026#34;) == polygon_name: continue g = feat.GetGeometryRef() \u0026#34;\u0026#34;\u0026#34;:type : ogr.Geometry\u0026#34;\u0026#34;\u0026#34; assert isinstance(g, ogr.Geometry) for pi in range(nx): for pj in range(ny): pt.SetPoint_2D(0, float(xx[pi, pj]), float(yy[pi, pj])) mask[pi, pj] += int(g.Contains(pt)) return mask We first read the Netcdf file and store informations in a Xarray.dataset.\nds = xr.open_dataset(\u0026#39;./DATA/CERA20C/cera20c_member0_TAS_197101_day.nc\u0026#39;) We then need to extract latitudes and longitudes values and compute a 2D matrix.\nImp_Lats = ds[\u0026#39;latitude\u0026#39;].values Imp_Lons = ds[\u0026#39;longitude\u0026#39;].values lon2d, lat2d = np.meshgrid(Imp_Lons, Imp_Lats) ds_mean = ds.mean(\u0026#39;time\u0026#39;) - 273.15 ds_mean \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288) Coordinates: * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 Data variables: t2m (latitude, longitude) float32 -34.067184 ... -25.249802  We open the shape file with Geopandas library.\nshapes = gpd.read_file(\u0026#34;./DATA/Shapefiles/Countries_Final-polygon.shp\u0026#34;) list(shapes.columns.values) ['FIPS', 'ISO2', 'ISO3', 'UN', 'NAME', 'AREA', 'POP2005', 'REGION', 'SUBREGION', 'LON', 'LAT', 'layer', 'path', 'geometry']  shapes.head() shapes.loc[27, \u0026#39;geometry\u0026#39;] shapes.plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xd8b4b70\u0026gt;  We want in our study to extract information inside Mexico shapefile.\nmask=get_mask(lon2d,lat2d,shp_path=\u0026#34;./DATA/Shapefiles/Countries_Final-polygon.shp\u0026#34;, polygon_name=\u0026#39;Mexico\u0026#39;) np.max(mask) 1  We will convert our mask into numpy 2D array. We'll be later able to apply this matrix to mask our Netcdf file.\nnp.save(\u0026#39;DATA/Mexico.npy\u0026#39;,mask) # saving our mask in numpy.array We will mask our area using .where() method.\nds_mask = ds_mean.where(mask == 1) ds_mask.to_netcdf(\u0026#39;DATA/Mexico.nc\u0026#39;) # we want to save our shapefile mask in Netcdf format ds_mask \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 145, longitude: 288) Coordinates: * latitude (latitude) float32 90.0 88.75 87.5 86.25 ... -87.5 -88.75 -90.0 * longitude (longitude) float32 0.0 1.25 2.5 3.75 ... 356.25 357.5 358.75 Data variables: t2m (latitude, longitude) float32 nan nan nan nan ... nan nan nan nan  np.max(ds_mask.t2m) \u0026lt;xarray.DataArray 't2m' ()\u0026gt; array(24.010315)  _, ax = make_figure() # plot the temperature field lat_bnd = [35, 0] lon_bnd = [240, 280] ds_mask.t2m.sel(longitude=slice(*lon_bnd), latitude=slice(*lat_bnd),).plot.pcolormesh(vmin=0, vmax=30, cmap=\u0026#39;Spectral\u0026#39;,add_colorbar=True, extend=\u0026#39;both\u0026#39;) \u0026lt;matplotlib.collections.QuadMesh at 0xdc65400\u0026gt;  5- Last example using Xarray: In this section, We will calculate the seasonal accumulation of the precipitation, extract a region, plot the domain and record our result in Netcdf:\n# Let\u0026#39;s open cera20c_enda_ep_PR_*.nc netcdf files  multi_dataDIR = \u0026#39;./DATA/CERA20C/cera20c_enda_ep_PR_*.nc\u0026#39; array = xr.open_mfdataset(multi_dataDIR) array.tp \u0026lt;xarray.DataArray 'tp' (time: 365, latitude: 181, longitude: 360)\u0026gt; dask.array\u0026lt;shape=(365, 181, 360), dtype=float32, chunksize=(31, 181, 360)\u0026gt; Coordinates: * longitude (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0 * latitude (latitude) float32 90.0 89.0 88.0 87.0 ... -88.0 -89.0 -90.0 * time (time) datetime64[ns] 1971-01-02T18:00:00 ... 1972-01-01T18:00:00 Attributes: units: m long_name: Total precipitation  All Netcdf files are stored in DataArray container, we can now group our Datasets by season, apply a simple sum() method over time and then change units from meters to mm.\narray_season = array.groupby(\u0026#39;time.season\u0026#39;).sum(\u0026#39;time\u0026#39;)*1000 array_season \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 181, longitude: 360, season: 4) Coordinates: * latitude (latitude) float32 90.0 89.0 88.0 87.0 ... -88.0 -89.0 -90.0 * season (season) object 'DJF' 'JJA' 'MAM' 'SON' * longitude (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0 Data variables: tp (season, latitude, longitude) float32 dask.array\u0026lt;shape=(4, 181, 360), chunksize=(1, 181, 360)\u0026gt;  We want to extract a specific domain delimited: - latitude boundaries: 50N to 70N - longitude boudaries: 250E to 310E\nWe finally want to extract winter season.\nlat_bnd = [70, 50] lon_bnd = [250, 310] subset_season_DJF = array_season.sel(season = \u0026#39;DJF\u0026#39;, longitude=slice(*lon_bnd), latitude=slice(*lat_bnd),) subset_season_DJF \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 21, longitude: 61) Coordinates: * latitude (latitude) float32 70.0 69.0 68.0 67.0 ... 53.0 52.0 51.0 50.0 season \u0026lt;U3 'DJF' * longitude (longitude) float32 250.0 251.0 252.0 253.0 ... 308.0 309.0 310.0 Data variables: tp (latitude, longitude) float32 dask.array\u0026lt;shape=(21, 61), chunksize=(21, 61)\u0026gt;  Let's save the Dataset to Netcdf.\ndataDIR = \u0026#39;./DATA/subset_season.nc\u0026#39; subset_season_DJF.to_netcdf(dataDIR) We can call our make_figure() function to quick plot our Dataset.\n_, ax = make_figure() # plot the temperature field subset_season_DJF.tp.plot.pcolormesh(vmin=0, vmax=200, cmap=\u0026#39;Spectral\u0026#39;,add_colorbar=True, extend=\u0026#39;both\u0026#39;) \u0026lt;matplotlib.collections.QuadMesh at 0x130aedd8\u0026gt;  array_season \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 181, longitude: 360, season: 4) Coordinates: * latitude (latitude) float32 90.0 89.0 88.0 87.0 ... -88.0 -89.0 -90.0 * season (season) object 'DJF' 'JJA' 'MAM' 'SON' * longitude (longitude) float32 0.0 1.0 2.0 3.0 ... 356.0 357.0 358.0 359.0 Data variables: tp (season, latitude, longitude) float32 dask.array\u0026lt;shape=(4, 181, 360), chunksize=(1, 181, 360)\u0026gt;  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"987eed7f83bed8904091e698a9a5a7b7","permalink":"/courses/tutorial_python_netcdf/4-xarray_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python_netcdf/4-xarray_library/","section":"courses","summary":"In this tutorial, we will use the features of the Python xarray library to process and analyze Netcdf files.\nTo install the library under anaconda:\n$ conda install xarray\nHere is an example of structure of a Netcdf file under xarray:\nDataArray xarray.DataArray is xarray’s implementation of a labeled, multi-dimensional array. It has several key properties: | | | |\u0026ndash;|\u0026ndash;| | values| a numpy.ndarray holding the array’s values | | dims| dimension names for each axis (e.","tags":null,"title":"4 Xarray","type":"docs"},{"authors":[],"categories":[],"content":"﻿\nExtract hourly, daily and monthly climate data from ECCC datamart In this post, we will see how to automatically download all weather data for one station from environment and climate change canada using Python librairies.\nHere's the URL to get historical data.\nWe will use a daily updated list of Climate stations in the National Archive available on this link.\nAs usual, we import our python librairies:\nimport os import wget import numpy as np import shutil import xml.etree.ElementTree as ET import pandas as pd First, we can open and read the list of Climate stations using Pandas:\ndf = pd.read_csv(\u0026#39;J:/Donnees_Stations/Donnees_EC/Codes/Station_Inventory_EN.csv\u0026#39;, sep=\u0026#39;,\u0026#39;, skiprows=3) df.head() | | Name | Province | Climate ID | Station ID | WMO ID | TC ID | Latitude (Decimal Degrees) | Longitude (Decimal Degrees) | Latitude | Longitude | Elevation (m) | First Year | Last Year | HLY First Year | HLY Last Year | DLY First Year | DLY Last Year | MLY First Year | MLY Last Year | |---:|:-----------------------|:-----------------|-------------:|-------------:|---------:|--------:|-----------------------------:|------------------------------:|-----------:|------------:|----------------:|-------------:|------------:|-----------------:|----------------:|-----------------:|----------------:|-----------------:|----------------:| | 0 | ACTIVE PASS | BRITISH COLUMBIA | 1010066 | 14 | nan | nan | 48.87 | -123.28 | 485200000 | -1231700000 | 4 | 1984 | 1996 | nan | nan | 1984 | 1996 | 1984 | 1996 | | 1 | ALBERT HEAD | BRITISH COLUMBIA | 1010235 | 15 | nan | nan | 48.4 | -123.48 | 482400000 | -1232900000 | 17 | 1971 | 1995 | nan | nan | 1971 | 1995 | 1971 | 1995 | | 2 | BAMBERTON OCEAN CEMENT | BRITISH COLUMBIA | 1010595 | 16 | nan | nan | 48.58 | -123.52 | 483500000 | -1233100000 | 85.3 | 1961 | 1980 | nan | nan | 1961 | 1980 | 1961 | 1980 | | 3 | BEAR CREEK | BRITISH COLUMBIA | 1010720 | 17 | nan | nan | 48.5 | -124 | 483000000 | -1240000000 | 350.5 | 1910 | 1971 | nan | nan | 1910 | 1971 | 1910 | 1971 | | 4 | BEAVER LAKE | BRITISH COLUMBIA | 1010774 | 18 | nan | nan | 48.5 | -123.35 | 483000000 | -1232100000 | 61 | 1894 | 1952 | nan | nan | 1894 | 1952 | 1894 | 1952 |  In this Dataframe, we have all informations to select and work with climate stations:\n Name : name of the station Province: province of the station Climate ID: Climate ID number Station ID: Station ID number Latitude: latitude of the station in decimal degrees Longitude: longitude of the station in decimal degrees First Year: first year of record Last Year: last year of record HLY First Year: first year of hourly record HLY Last Year: last year of hourly record DLY First Year: first year of daily record DLY Last Year: last year of daily record MLY First Year: first year of monthly record MLY Last Year: last year of monthly record  We can easily create 3 dataframes to distinguish hourly, daily and monthly datasets:\ndf_hourly = df[[\u0026#39;Name\u0026#39;, \u0026#39;Province\u0026#39;,\u0026#39;Climate ID\u0026#39;,\u0026#39;Station ID\u0026#39;,\u0026#39;Latitude (Decimal Degrees)\u0026#39;, \u0026#39;Longitude (Decimal Degrees)\u0026#39;,\u0026#39;HLY First Year\u0026#39;,\u0026#39;HLY Last Year\u0026#39;]].dropna() df_daily = df[[\u0026#39;Name\u0026#39;, \u0026#39;Province\u0026#39;,\u0026#39;Climate ID\u0026#39;,\u0026#39;Station ID\u0026#39;,\u0026#39;Latitude (Decimal Degrees)\u0026#39;, \u0026#39;Longitude (Decimal Degrees)\u0026#39;,\u0026#39;DLY First Year\u0026#39;,\u0026#39;DLY Last Year\u0026#39;]].dropna() df_monthly = df[[\u0026#39;Name\u0026#39;, \u0026#39;Province\u0026#39;,\u0026#39;Climate ID\u0026#39;,\u0026#39;Station ID\u0026#39;,\u0026#39;Latitude (Decimal Degrees)\u0026#39;, \u0026#39;Longitude (Decimal Degrees)\u0026#39;,\u0026#39;MLY First Year\u0026#39;,\u0026#39;MLY Last Year\u0026#39;]].dropna() We will use wget python command to download data, example: wget.download(\u0026lsquo;http://climate.weather.gc.ca/climate_data/bulk_data_e.html format=xml\u0026amp;stationID='+str(int(id_stat))+\u0026rsquo;\u0026amp;Year='+str(year)+\u0026rsquo;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=2\u0026rsquo;) With:\n year = year of the record month = year of the record format= [csv|xml]: the format output timeframe = 1: for hourly data timeframe = 2: for daily data timeframe = 3 for monthly data Day: the value of the \u0026ldquo;day\u0026rdquo; variable is not used and can be an arbitrary value For another station, change the value of the variable stationID For the data in XML format, change the value of the variable format to xml in the URL.  1- Working with hourly data: df_hourly.head() | | Name | Province | Climate ID | Station ID | Latitude (Decimal Degrees) | Longitude (Decimal Degrees) | HLY First Year | HLY Last Year | |---:|:------------------|:-----------------|-------------:|-------------:|-----------------------------:|------------------------------:|-----------------:|----------------:| | 28 | DISCOVERY ISLAND | BRITISH COLUMBIA | 1012475 | 27226 | 48.42 | -123.23 | 1997 | 2020 | | 39 | ESQUIMALT HARBOUR | BRITISH COLUMBIA | 1012710 | 52 | 48.43 | -123.44 | 1994 | 2020 | | 49 | KELP REEFS | BRITISH COLUMBIA | 1013998 | 10853 | 48.55 | -123.24 | 1997 | 2020 | | 53 | MALAHAT | BRITISH COLUMBIA | 1014818 | 10730 | 48.58 | -123.58 | 1991 | 1992 | | 54 | MALAHAT | BRITISH COLUMBIA | 1014820 | 65 | 48.57 | -123.53 | 1994 | 2020 |  We want to extract hourly observations from DISCOVERY ISLAND in 2019. So we will select id_stat = 27226, year = 2019 and timeframe = 1 .\n- We want to work in csv format. id_stat = 27226 year = 2019 wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=1\u0026#39;) 'en_climate_hourly_BC_1012475_001-2019_P1H (1).csv'  df = pd.read_csv(\u0026#39;en_climate_hourly_BC_1012475_001-2019_P1H.csv\u0026#39;, sep=\u0026#39;,\u0026#39;, skiprows=0) df.head(2) | | Longitude (x) | Latitude (y) | Station Name | Climate ID | Date/Time | Year | Month | Day | Time | Temp (°C) | Temp Flag | Dew Point Temp (°C) | Dew Point Temp Flag | Rel Hum (%) | Rel Hum Flag | Wind Dir (10s deg) | Wind Dir Flag | Wind Spd (km/h) | Wind Spd Flag | Visibility (km) | Visibility Flag | Stn Press (kPa) | Stn Press Flag | Hmdx | Hmdx Flag | Wind Chill | Wind Chill Flag | Weather | |---:|----------------:|---------------:|:-----------------|-------------:|:-----------------|-------:|--------:|------:|:-------|------------:|------------:|----------------------:|----------------------:|--------------:|---------------:|---------------------:|----------------:|------------------:|----------------:|------------------:|------------------:|------------------:|-----------------:|-------:|------------:|-------------:|------------------:|----------:| | 0 | -123.23 | 48.42 | DISCOVERY ISLAND | 1012475 | 2019-01-01 00:00 | 2019 | 1 | 1 | 00:00 | 5.2 | nan | 0.7 | nan | 73 | nan | 17 | nan | 5 | nan | nan | nan | 102.87 | nan | nan | nan | nan | nan | nan | | 1 | -123.23 | 48.42 | DISCOVERY ISLAND | 1012475 | 2019-01-01 01:00 | 2019 | 1 | 1 | 01:00 | 5.2 | nan | 0.7 | nan | 72 | nan | 17 | nan | 5 | nan | nan | nan | 102.87 | nan | nan | nan | nan | nan | nan |  url_template = \u0026#34;http://climate.weather.gc.ca/climateData/bulkdata_e.html?format=csv\u0026amp;stationID=5415\u0026amp;Year={year}\u0026amp;Month={month}\u0026amp;timeframe=1\u0026amp;submit=Download+Data\u0026#34; url_template 'http://climate.weather.gc.ca/climateData/bulkdata_e.html?format=csv\u0026amp;stationID=5415\u0026amp;Year={year}\u0026amp;Month={month}\u0026amp;timeframe=1\u0026amp;submit=Download+Data'  - If we work in xml format: tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=1\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() Here is the structure of xml file and informations we could find in \u0026lsquo;stationinformation\u0026rsquo; child:\nstationsinfo=root.find(\u0026#39;stationinformation\u0026#39;) stationsinfo[0].text, \u0026#39;latitude\u0026#39;, stationsinfo[3].text, \u0026#39;longitude\u0026#39;, stationsinfo[4].text ############################################################### ######### stationsinfo[0].text : station name ######### stationsinfo[1].text : province ######### stationsinfo[2].text : provider ######### stationsinfo[3].text : latitude  ######### stationsinfo[4].text : longitude ('DISCOVERY ISLAND', 'latitude', '48.42', 'longitude', '-123.23')  Climate records are available in \u0026lsquo;station data\u0026rsquo; child:\nLet's extract Temperature data for one year:\nstationsdata = root.findall(\u0026#39;.//stationdata\u0026#39;) result = [] for stationdata in stationsdata :# loop over hourly values or child  champs=stationdata.find(\u0026#39;temp\u0026#39;) tmp=np.array(result,\u0026#34;float\u0026#34;) result.append(champs.text) tmp=np.array(result,\u0026#34;float\u0026#34;) To loop over years available for this station:\nid_stat = 27226 yi = 2015 yf =2020 dataset = [] for year in range(yi,yf+1): ### loop over years for month in range(1,13): tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=\u0026#39;+str(month)+\u0026#39;\u0026amp;Day=14\u0026amp;timeframe=1\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() stationsdata = root.findall(\u0026#39;.//stationdata\u0026#39;) temperature = [] date = [] data = [] for stationdata in stationsdata: date.append(stationdata.attrib[\u0026#39;year\u0026#39;]+ \u0026#39;-\u0026#39;+stationdata.attrib[\u0026#39;month\u0026#39;]+ \u0026#39;-\u0026#39;+stationdata.attrib[\u0026#39;day\u0026#39;]+ \u0026#39;\u0026#39;+stationdata.attrib[\u0026#39;hour\u0026#39;]+\u0026#39;:00\u0026#39;) champs=stationdata.find(\u0026#39;temp\u0026#39;) tmp = np.array(champs.text,\u0026#34;float\u0026#34;) if tmp == \u0026#39;nan\u0026#39;: temperature.append(np.nan) else: temperature.append(tmp) # loop over days in one year data = {\u0026#39;date\u0026#39;:date, \u0026#39;air_temperature\u0026#39;:temperature} data = pd.DataFrame(data) dataset.append(data) dataset = pd.concat(dataset) from tabulate import tabulate print(tabulate(dataset.head(), headers=\u0026#39;keys\u0026#39;, tablefmt=\u0026#39;pipe\u0026#39;)) | | date | air_temperature | |---:|:--------------|------------------:| | 0 | 2015-1-1 0:00 | 3 | | 1 | 2015-1-1 1:00 | 2.9 | | 2 | 2015-1-1 2:00 | 3 | | 3 | 2015-1-1 3:00 | 3.1 | | 4 | 2015-1-1 4:00 | 3.1 |  To save our dataframe into csv format:\ndataset.to_csv(\u0026#34;./Hourly_dataset.csv\u0026#34;, index = False, header = True, sep = \u0026#39;,\u0026#39;) We Can make quick plot using plotly.express:\nimport plotly.express as px import plotly fig = px.line(dataset, x=\u0026#39;date\u0026#39;, y=\u0026#39;air_temperature\u0026#39;) fig.update_layout(title_text=\u0026#39;Hourly temperature record\u0026#39;, xaxis_rangeslider_visible=True) Follow this link for interactive plot:\nTo save our file in html:\nimport plotly.io as pio pio.write_html(fig, file = \u0026#39;file1.html\u0026#39;, auto_open = True) To save our file in png:\nfig.write_image(\u0026#34;file1.png\u0026#34;) 2- Working with daily data: We will do the same job with daily mean temperature for one Climate station:\ndf_daily.head() | | Name | Province | Climate ID | Station ID | Latitude (Decimal Degrees) | Longitude (Decimal Degrees) | DLY First Year | DLY Last Year | |---:|:-----------------------|:-----------------|-------------:|-------------:|-----------------------------:|------------------------------:|-----------------:|----------------:| | 0 | ACTIVE PASS | BRITISH COLUMBIA | 1010066 | 14 | 48.87 | -123.28 | 1984 | 1996 | | 1 | ALBERT HEAD | BRITISH COLUMBIA | 1010235 | 15 | 48.4 | -123.48 | 1971 | 1995 | | 2 | BAMBERTON OCEAN CEMENT | BRITISH COLUMBIA | 1010595 | 16 | 48.58 | -123.52 | 1961 | 1980 | | 3 | BEAR CREEK | BRITISH COLUMBIA | 1010720 | 17 | 48.5 | -124 | 1910 | 1971 | | 4 | BEAVER LAKE | BRITISH COLUMBIA | 1010774 | 18 | 48.5 | -123.35 | 1894 | 1952 |  tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=2\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() Here is the structure of xml file and informations we could find in \u0026lsquo;stationinformation\u0026rsquo; child:\nClimate records are available in \u0026lsquo;station data\u0026rsquo; child:\nurl_template = \u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=2\u0026#39; url_template 'http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=5415\u0026amp;Year=2020\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=2'  We want to extract 4 specific years for Climate station with ID=27226:\nid_stat = 27226 df_daily[df_daily[\u0026#39;Station ID\u0026#39;] == id_stat][\u0026#39;DLY First Year\u0026#39;] yi = 2015 yf = 2020 dataset = [] for year in range(yi,yf+1): ### loop over years tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=1\u0026amp;Day=14\u0026amp;timeframe=2\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() stationsdata = root.findall(\u0026#39;.//stationdata\u0026#39;) temperature = [] date = [] data = [] for stationdata in stationsdata: date.append(stationdata.attrib[\u0026#39;year\u0026#39;]+ \u0026#39;-\u0026#39;+stationdata.attrib[\u0026#39;month\u0026#39;]+ \u0026#39;-\u0026#39;+stationdata.attrib[\u0026#39;day\u0026#39;]) champs=stationdata.find(\u0026#39;meantemp\u0026#39;) tmp = np.array(champs.text,\u0026#34;float\u0026#34;) if tmp == \u0026#39;nan\u0026#39;: temperature.append(np.nan) else: temperature.append(tmp) # loop over days in one year data = {\u0026#39;date\u0026#39;:date, \u0026#39;temperature\u0026#39;:temperature} data = pd.DataFrame(data) dataset.append(data) dataset = pd.concat(dataset) dataset.head() | | date | temperature | |---:|:---------|--------------:| | 0 | 2015-1-1 | nan | | 1 | 2015-1-2 | nan | | 2 | 2015-1-3 | nan | | 3 | 2015-1-4 | nan | | 4 | 2015-1-5 | nan |  We Can make quick plot using plotly.express:\nimport plotly.express as px fig = px.line(dataset, x=\u0026#39;date\u0026#39;, y=\u0026#39;temperature\u0026#39;) fig.update_layout(title_text=\u0026#39;Time Series with Rangeslider\u0026#39;, xaxis_rangeslider_visible=True) Follow this link for interactive plot:\ndataset.to_csv(\u0026#34;./Daily_dataset.csv\u0026#34;, index = False, header = True, sep = \u0026#39;,\u0026#39;) import plotly.io as pio pio.write_html(fig, file = \u0026#39;file2.html\u0026#39;, auto_open = True) fig.write_image(\u0026#34;file2.png\u0026#34;) 3- Working with monthly data: We will do the same job with monthly total precipitation for one Climate station:\ndf_monthly.head() | | Name | Province | Climate ID | Station ID | Latitude (Decimal Degrees) | Longitude (Decimal Degrees) | MLY First Year | MLY Last Year | |---:|:-----------------------|:-----------------|-------------:|-------------:|-----------------------------:|------------------------------:|-----------------:|----------------:| | 0 | ACTIVE PASS | BRITISH COLUMBIA | 1010066 | 14 | 48.87 | -123.28 | 1984 | 1996 | | 1 | ALBERT HEAD | BRITISH COLUMBIA | 1010235 | 15 | 48.4 | -123.48 | 1971 | 1995 | | 2 | BAMBERTON OCEAN CEMENT | BRITISH COLUMBIA | 1010595 | 16 | 48.58 | -123.52 | 1961 | 1980 | | 3 | BEAR CREEK | BRITISH COLUMBIA | 1010720 | 17 | 48.5 | -124 | 1910 | 1971 | | 4 | BEAVER LAKE | BRITISH COLUMBIA | 1010774 | 18 | 48.5 | -123.35 | 1894 | 1952 |  tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=3\u0026#39;) url_template = \u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=3\u0026#39; url_template tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() url_template 'http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=27226\u0026amp;Year=2020\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=3'  Climate records are available in \u0026lsquo;station data\u0026rsquo; child:\nWe want to dowload monthly total precipitation for Climate station with ID=5415.\nid_stat = 5415 df_monthly[df_monthly[\u0026#39;Station ID\u0026#39;] == id_stat][\u0026#39;MLY First Year\u0026#39;] yi = int(df_monthly[df_monthly[\u0026#39;Station ID\u0026#39;] == id_stat][\u0026#39;MLY First Year\u0026#39;].values) yf = int(df_monthly[df_monthly[\u0026#39;Station ID\u0026#39;] == id_stat][\u0026#39;MLY Last Year\u0026#39;].values) dataset = [] tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=2020\u0026amp;Month=1\u0026amp;Day=14\u0026amp;timeframe=3\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() stationsdata = root.findall(\u0026#39;.//stationdata\u0026#39;) totprecip = [] date = [] data = [] for stationdata in stationsdata: date.append(stationdata.attrib[\u0026#39;year\u0026#39;]+ \u0026#39;-\u0026#39;+stationdata.attrib[\u0026#39;month\u0026#39;]) champs=stationdata.find(\u0026#39;totprecip\u0026#39;) tmp = np.array(champs.text,\u0026#34;float\u0026#34;) if tmp == \u0026#39;nan\u0026#39;: totprecip.append(np.nan) else: totprecip.append(tmp) # loop over days in one year data = {\u0026#39;date\u0026#39;:date, \u0026#39;totprecip\u0026#39;:totprecip} data = pd.DataFrame(data) dataset.append(data) dataset = pd.concat(dataset) dataset = dataset.drop_duplicates(\u0026#39;date\u0026#39;,keep=\u0026#39;last\u0026#39;) dataset[\u0026#39;date\u0026#39;] = pd.to_datetime(dataset[\u0026#39;date\u0026#39;], format=\u0026#39;%Y-%m\u0026#39;) import plotly.graph_objects as go fig = go.Figure(data=[ go.Bar(name=\u0026#39;totprecip1\u0026#39;, x=dataset[dataset[\u0026#39;date\u0026#39;].dt.month == 7][\u0026#39;date\u0026#39;], y=dataset[dataset[\u0026#39;date\u0026#39;].dt.month == 7][\u0026#39;totprecip\u0026#39;]) ]) fig.update_layout( title=\u0026#39;July monthly precipitation\u0026#39;, xaxis_tickfont_size=14, yaxis=dict( title=\u0026#39;Total precipitation (mm)\u0026#39;, titlefont_size=16, tickfont_size=14, ), barmode=\u0026#39;group\u0026#39;, bargap=0.15, # gap between bars of adjacent location coordinates. bargroupgap=0.1 # gap between bars of the same location coordinate. ) Follow this link for interactive plot:\ndataset[dataset[\u0026#39;date\u0026#39;].dt.month == 7].to_csv(\u0026#34;./Monthly_dataset.csv\u0026#34;, index = False, header = True, sep = \u0026#39;,\u0026#39;) import plotly.io as pio pio.write_html(fig, file = \u0026#39;file2.html\u0026#39;, auto_open = True) fig.write_image(\u0026#34;file2.png\u0026#34;) ","date":1583858297,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583858297,"objectID":"2cac93e4db0fb932c2ff9f6fc5be9903","permalink":"/post/working_with_eccc_climate_data/","publishdate":"2020-03-10T09:38:17-07:00","relpermalink":"/post/working_with_eccc_climate_data/","section":"post","summary":"﻿\nExtract hourly, daily and monthly climate data from ECCC datamart In this post, we will see how to automatically download all weather data for one station from environment and climate change canada using Python librairies.\nHere's the URL to get historical data.\nWe will use a daily updated list of Climate stations in the National Archive available on this link.\nAs usual, we import our python librairies:\nimport os import wget import numpy as np import shutil import xml.","tags":[],"title":"Working_with_ECCC_climate_data","type":"post"},{"authors":["Salah Uddin Khan","Nicholas H. Ogden","Aamir Fazil","Philippe Gachon","Guillaume Dueymes","Amy L. Greer","Victoria Ng"],"categories":["2"],"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"974ca606514104176a1205faa68c01d7","permalink":"/publication/article10/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/article10/","section":"publication","summary":"Aedes aegypti and Ae. albopictus are mosquito vectors of more than 22 arboviruses that infect humans. We used boosted regression trees to assess climatic suitability for Ae. albopictus and Ae. aegypti mosquitoes in Canada and the United States (US) under current and future projected climate. Models for both species were mostly influenced by minimum daily temperature and demonstrated high accuracy for predicting their geographic ranges under current climate. Northward range expansion of suitable niche for both species was projected under future climate. Much of the US and parts of southern Canada are projected to be suitable for both species by 2100; with Ae. albopictus projected to expand its range north earlier this century and further north than Ae. aegypti. An associated increased risk of local transmission of Aedestransmitted arboviruses in the projected ecological niche means that surveillance for these vectors and the pathogens that they carry would be prudent..","tags":[],"title":"Current and projected distributions of Aedes aegypti and Ae. albopictus in Canada and the U.","type":"publication"},{"authors":[],"categories":[],"content":"﻿\nNetcdf: Select closest points using CKDTree In this post, we are going to define an algorithm to locate the closest points to a reference point, by using coordinate transformations, k-dimensional trees, and xarray pointwise indexing.\nTo select closest grid points, we will use here one approach using cKDTree class from scipy.spatial package.\nDifferent method are considered here:\n- nearest neighbour - nearest neighbours, weighting with the inverse of distance squared: - \\begin{equation*} $T_t = \\frac{\\sum_{i=1}^{m}T_{s,i}w_i}{\\sum_{i=1}^{m}w_i}$, $w_i = 1/d_i^2$) \\end{equation*} - query ball point : Find all pairs of points whose distance is at most one distance  Let's import Python librairies #for Netcdf manipulation #import netCDF4 #from netCDF4 import Dataset import xarray as xr #for array manipulation import numpy as np import pandas as pd #for plotting import cartopy.crs as ccrs import cartopy.feature as cfeature import matplotlib.pylab as plt #for interpolation from scipy.spatial import cKDTree Loading Netcdf files We are going to use daily ERA5 reanalysis data from April 1st to 31th of october 2018.\nunique_dataDIR = \u0026#39;K:/PROJETS/PROJET_FIRE_INDEX/ERA5_FWI_Netcdf/ERA5_FWI_QC_SNOW_New_2018_from_4_to_8.nc\u0026#39; TAS = xr.open_dataset(unique_dataDIR) TAS.lon.shape, TAS.lat.shape, ((121,), (77,))  Regrid 1D latitude and longitude to 2D grid.\nlon2d, lat2d = np.meshgrid(TAS.lon, TAS.lat) lon2d.shape, lat2d.shape ((77, 121), (77, 121))  Let’s have a quick look at our dataset, we will plot one day in our time range:\nfig = plt.figure(figsize=(12,6)) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-75,-60,50,52]) ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) cmap0=plt.cm.jet cmap0.set_under(\u0026#39;darkblue\u0026#39;) cmap0.set_over(\u0026#39;darkred\u0026#39;) mm = ax.contourf(TAS.lon,\\ TAS.lat,\\ TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI,\\ vmin=0,\\ vmax=20, \\ transform=ccrs.PlateCarree(),\\ levels=np.arange(0, 20, 1.0),\\ cmap=cmap0 ) ax.scatter(lon2d, lat2d, transform=ccrs.PlateCarree(), s=5) \u0026lt;matplotlib.collections.PathCollection at 0xc7d6828\u0026gt;  The blue dots in the plot identify the grid points.\nWhat we want to achieve is to come up with a way to compare distances between a reference point (defined by latitude and longitude values), and all grid points, and pick the minimum value.\nBut first, we need to measure distances between two points on our planet, given their geodetic (latitude/longitude/altitude) coordinates.\nIn this post, we are going to use the cartesian or ECEF (“earth-centered, earth-fixed”) geographic coordinate system, which represents positions (in meters) as X, Y, and Z coordinates, approximating the earth surface as an ellipsoid of revolution (close enough for our purposes).\nOnce we convert our latitude/longitude coordinates to cartesian coordinates, measuring the distance between two points is as simple as computing the Euclidean distance between them.\nThe conversion between cartesian and geodetic coordinates latitude, longitude is done according to:\nWhere is the local curvature of the ellipsoid along the first vertical, and where e, the first eccentricity, and a, the semi-major axis, are the parameters defining the ellipsoid.\nFinding the closest\nOur problem falls into the class of nearest neighbour searches. A common approach when it comes to finding the nearest neighbour in a number of points with k dimensions is to use a KD-tree, or k-dimensional tree. SciPy library provides a very efficient KD-tree implementation . Once we have constructed our tree, all we have to do is to populate it with a (n,m) shaped array of points and then query it the nearest neighbor to a reference point.\nWe will have to make use of some NumPy acrobatics to reshape our data structures from a two-dimensional grid to a one-dimensional array, and to convert the returned one-dimensional index to a set of two indices on our original grid.\nclass KDTreeIndex(): \u0026#34;\u0026#34;\u0026#34;A KD-tree implementation for fast point lookup on a 2D gridKeyword arguments: dataset -- a xarray DataArray containing lat/lon coordinates(named \u0026#39;lat\u0026#39;and \u0026#39;lon\u0026#39;respectively) \u0026#34;\u0026#34;\u0026#34; def transform_coordinates(self, coords): \u0026#34;\u0026#34;\u0026#34;Transform coordinates from geodetic to cartesianKeyword arguments:coords - a set of lan/lon coordinates (e.g. a tuple or an array of tuples)\u0026#34;\u0026#34;\u0026#34; # WGS 84 reference coordinate system parameters A = 6378.137 # major axis [km]  E2 = 6.69437999014e-3 # eccentricity squared  coords = np.asarray(coords).astype(np.float) # is coords a tuple? Convert it to an one-element array of tuples if coords.ndim == 1: coords = np.array([coords]) # convert to radiants lat_rad = np.radians(coords[:,0]) lon_rad = np.radians(coords[:,1]) # convert to cartesian coordinates r_n = A / (np.sqrt(1 - E2 * (np.sin(lat_rad) ** 2))) x = r_n * np.cos(lat_rad) * np.cos(lon_rad) y = r_n * np.cos(lat_rad) * np.sin(lon_rad) z = r_n * (1 - E2) * np.sin(lat_rad) return np.column_stack((x, y, z)) def __init__(self, dataset): # store original dataset shape self.shape = dataset.shape lon2d, lat2d = np.meshgrid(dataset.lon, dataset.lat) # reshape and stack coordinates coords = np.column_stack((lat2d.ravel(), lon2d.ravel())) # construct KD-tree self.tree = cKDTree(self.transform_coordinates(coords)) def query(self, point, k): \u0026#34;\u0026#34;\u0026#34;Query the kd-tree for nearest neighbour.Keyword arguments:point -- a (lat, lon) tuple or array of tuples\u0026#34;\u0026#34;\u0026#34; d, inds = self.tree.query(self.transform_coordinates(point),k=k) w = 1.0 / d**2 # regrid to 2D grid indslatlon = np.unravel_index(inds, self.shape) return d, inds, w, indslatlon def query_ball_point(self, point, radius): \u0026#34;\u0026#34;\u0026#34;Query the kd-tree for all point within distance radius of point(s) xKeyword arguments:point -- a (lat, lon) tuple or array of tuplesradius -- the search radius (km)\u0026#34;\u0026#34;\u0026#34; index = self.tree.query_ball_point(self.transform_coordinates(point), radius) # regrid to 2D grid  index = np.unravel_index(index[0], self.shape) # return DataArray indexers return xr.DataArray(index[0], dims=\u0026#39;lat\u0026#39;), \\ xr.DataArray(index[1], dims=\u0026#39;lon\u0026#39;) We will construct our cKDTree object using one time step from our xarray.\nlatlon_tree = KDTreeIndex(TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI) We can now query the tree the nearest grid point to one reference point:\nNearest neighbour k=1: Let's select reference point defined by: - latitude = 50 degN - longitude = -68.1461 degE\nWe can now query the tree the nearest point :\nsite = (50, -68.1461) d, inds, w, indslatlon = latlon_tree.query(site, k=1) print(inds) print(indslatlon) [5871] (array([48], dtype=int64), array([63], dtype=int64))  latpt = indslatlon[0][0] lonpt = indslatlon[1][0] print(latpt, lonpt) 48 63  The query method actually returns:\n  two array objects a latitude and a longitude indexer\n  one 1D indexer\n  one distance value\n  one corresponding weighting.\n*inds value (ie 5871) represents the index found for our one-dimensional array.\n*indslatlon are the corresponding longitude and latitude values for the closest point found.\n  To extract the xarray field value using our previous index, first we need to convert a two-dimensional grid to a one-dimensional array and then select index value.\nTAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI.values.flatten()[inds] array([10.517308])  TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI[latpt,lonpt] \u0026lt;xarray.DataArray 'FWI' ()\u0026gt; array(10.517308) Coordinates: lon float64 -68.25 lat float32 50.0 time datetime64[ns] 2018-08-01  Let's make a quick look:\nfig = plt.figure(figsize=(25,12)) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-72,-65,49,51]) # ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac  ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ## Choisissons une colormap cmap0=plt.cm.jet cmap0.set_under(\u0026#39;darkblue\u0026#39;) ## on met en blanc les valeurs inferieures au min de clev cmap0.set_over(\u0026#39;darkred\u0026#39;) ## bleu fonce pour les valeurs extremes de pluie mm = ax.contourf(TAS.lon,\\ TAS.lat,\\ TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI,\\ vmin=0,\\ vmax=20, \\ transform=ccrs.PlateCarree(),\\ levels=np.arange(0, 20, 1.0),\\ cmap=cmap0 ) ax.scatter(lon2d, lat2d, transform=ccrs.PlateCarree(), s=5, label=\u0026#39;ERA5 grid points\u0026#39;) ax.scatter(TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI[latpt,lonpt].lon, TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI[latpt,lonpt].lat, color=\u0026#39;r\u0026#39;, transform=ccrs.PlateCarree(), s=100, label=\u0026#39;closest grid point\u0026#39;) ax.scatter(-68.1461, 50, transform=ccrs.PlateCarree(), marker=\u0026#39;x\u0026#39;, color=\u0026#39;black\u0026#39;, s=300, label=\u0026#39;reference point\u0026#39;) plt.legend(loc=\u0026#39;upper left\u0026#39;, numpoints=1, ncol=3, fontsize=20) \u0026lt;matplotlib.legend.Legend at 0xc7d6358\u0026gt;  Nearest neighbours k=10 We will now make the same exercice using k=10.\nsite = (50, -68.1461) d, inds, w, indslatlon = ground_pixel_tree.query(site, k=10) print(d) print(w) print(inds) print(indslatlon) [[ 7.44918778 10.47474677 25.37308687 28.39863145 28.78330094 28.79213291 29.70566615 29.72368634 37.59953757 37.68733316]] [[0.01802114 0.00911408 0.00155329 0.00123995 0.00120703 0.00120629 0.00113324 0.00113187 0.00070735 0.00070406]] [[5871 5872 5870 5873 5750 5992 5751 5993 5749 5991]] (array([[48, 48, 48, 48, 47, 49, 47, 49, 47, 49]], dtype=int64), array([[63, 64, 62, 65, 63, 63, 64, 64, 62, 62]], dtype=int64))  TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI.values.flatten()[inds] array([[10.517308 , 9.87137321, 8.57146189, 11.99096501, 9.32552318, 10.0954969 , 9.84150618, 11.8046335 , 9.26170112, 8.3454113 ]])  latpts = indslatlon[0][0] lonpts = indslatlon[1][0] print(latpts, lonpts) [48 48 48 48 47 49 47 49 47 49] [63 64 62 65 63 63 64 64 62 62]  We can know Interpolate our field using inverse distance weighting, using 10 nearest neighbours (k=10):\n(i.e. Tt=∑mi=1Ts,iwi∑mi=1wi, wi=1/d2i).\nnp.sum(w * TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI.values.flatten()[inds], axis=1) / np.sum(w, axis=1) array([10.21868978])  Let's make a quick look:\nlatpts = indslatlon[0][0] lonpts = indslatlon[1][0] TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI[latpts,lonpts].lon \u0026lt;xarray.DataArray 'lon' (lon: 10)\u0026gt; array([-68.25, -68. , -68.5 , -67.75, -68.25, -68.25, -68. , -68. , -68.5 , -68.5 ]) Coordinates: * lon (lon) float64 -68.25 -68.0 -68.5 -67.75 ... -68.0 -68.0 -68.5 -68.5 time datetime64[ns] 2018-08-01  fig = plt.figure(figsize=(25,12)) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-72,-65,49,51]) # ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac  ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ## Choisissons une colormap cmap0=plt.cm.jet cmap0.set_under(\u0026#39;darkblue\u0026#39;) ## on met en blanc les valeurs inferieures au min de clev cmap0.set_over(\u0026#39;darkred\u0026#39;) ## bleu fonce pour les valeurs extremes de pluie mm = ax.contourf(TAS.lon,\\ TAS.lat,\\ TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI,\\ vmin=0,\\ vmax=20, \\ transform=ccrs.PlateCarree(),\\ levels=np.arange(0, 20, 1.0),\\ cmap=cmap0 ) ax.scatter(lon2d, lat2d, transform=ccrs.PlateCarree(), s=5, label=\u0026#39;ERA5 grid points\u0026#39;) ax.scatter(TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI[latpts,lonpts].lon, TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI[latpts,lonpts].lat, color=\u0026#39;r\u0026#39;, transform=ccrs.PlateCarree(), s=100, label=\u0026#39;closest grid points\u0026#39;) ax.scatter(-68.1461, 50, transform=ccrs.PlateCarree(), marker=\u0026#39;x\u0026#39;, color=\u0026#39;black\u0026#39;, s=300, label=\u0026#39;reference point\u0026#39;) plt.legend(loc=\u0026#39;upper left\u0026#39;, numpoints=1, ncol=3, fontsize=20) \u0026lt;matplotlib.legend.Legend at 0xcb78ac8\u0026gt;  Now we will find all grid points within a given distance from a reference point, by using the query_ball_point method in SciPy’s KD-tree implementation.\nLet’s find out which ground pixels fall into a 700km radius from our reference point:\nball_point_index = ground_pixel_tree.query_ball_point(site, 50) lons = TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI[ball_point_index].lon lats = TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI[ball_point_index].lat fig = plt.figure(figsize=(25,12)) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-72,-65,49,51]) # ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac  ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ## Choisissons une colormap cmap0=plt.cm.jet cmap0.set_under(\u0026#39;darkblue\u0026#39;) ## on met en blanc les valeurs inferieures au min de clev cmap0.set_over(\u0026#39;darkred\u0026#39;) ## bleu fonce pour les valeurs extremes de pluie mm = ax.contourf(TAS.lon,\\ TAS.lat,\\ TAS.sel(time=\u0026#39;2018-08-01\u0026#39;).FWI,\\ vmin=0,\\ vmax=20, \\ transform=ccrs.PlateCarree(),\\ levels=np.arange(0, 20, 1.0),\\ cmap=cmap0 ) ax.scatter(lon2d, lat2d, transform=ccrs.PlateCarree(), s=5, label=\u0026#39;ERA5 grid points\u0026#39;) ax.scatter(lons, lats, color=\u0026#39;r\u0026#39;, transform=ccrs.PlateCarree(), s=100, label=\u0026#39;grid points within 50 km from reference point\u0026#39;) ax.scatter(-68.1461, 50, transform=ccrs.PlateCarree(), marker=\u0026#39;x\u0026#39;, color=\u0026#39;black\u0026#39;, s=300, label=\u0026#39;reference point\u0026#39;) plt.legend(loc=\u0026#39;upper left\u0026#39;, numpoints=1, ncol=3, fontsize=20) \u0026lt;matplotlib.legend.Legend at 0xcb86b38\u0026gt;  ","date":1582751429,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582751429,"objectID":"f6edd5f82e559908fa0dc543da1dd416","permalink":"/post/ckdtree_netcdf/","publishdate":"2020-02-26T13:10:29-08:00","relpermalink":"/post/ckdtree_netcdf/","section":"post","summary":"﻿\nNetcdf: Select closest points using CKDTree In this post, we are going to define an algorithm to locate the closest points to a reference point, by using coordinate transformations, k-dimensional trees, and xarray pointwise indexing.\nTo select closest grid points, we will use here one approach using cKDTree class from scipy.spatial package.\nDifferent method are considered here:\n- nearest neighbour - nearest neighbours, weighting with the inverse of distance squared: - \\begin{equation*} $T_t = \\frac{\\sum_{i=1}^{m}T_{s,i}w_i}{\\sum_{i=1}^{m}w_i}$, $w_i = 1/d_i^2$) \\end{equation*} - query ball point : Find all pairs of points whose distance is at most one distance  Let's import Python librairies #for Netcdf manipulation #import netCDF4 #from netCDF4 import Dataset import xarray as xr #for array manipulation import numpy as np import pandas as pd #for plotting import cartopy.","tags":[],"title":"CKDTree_Netcdf","type":"post"},{"authors":[],"categories":[],"content":"﻿\nWorking with XML (Extensible Markup Language) file using Python Part 1: Surface Weather Observation XML (SW-OB-XML) SW-OB-XML For xml files, we will use a package called xml which will allow us to decrypt the xml file.\nPandas does not directly have tools to transform xml into DataFrame because the semi-structured aspect of xml forces us to perform a few steps before filling out a DataFrame.\nWe will first download an xml file located in this directory:\nhttps://dd.weather.gc.ca/observations/doc/\nWe are going to work here with the meteorological ground (SWOB-ML) and marine observations data found on the ECCC Datamart.\n Data are available at the following links:\n  Earth observations:   https://dd.meteo.gc.ca/observations/swob-ml/AAAAMMJJ/XXXX/\nWhere:\n  AAAAMMJJ: observations date, UTC\n  XXXX: station ID (OMM ID)\n  Marine observations:   https://dd.meteo.gc.ca/observations/swob-ml/marine/moored-buoys/YYYMMDD/XXXXXXX\noù:\n  AAAAMMJJ: observations date, UTC\n  XXXXXXX: station ID (OMM ID), can be 5 or 7 digits\n  Partner observations:   https://dd.meteo.gc.ca/observations/swob-ml/partners/NETWORK/YYYMMDD/MSC-ID\noù:\n NETWORK: acronym of the partner network YYYYMMDD: observations date, UTC MSC-ID: ID  ################################################\nDescription  ################################################\nThe markup language for ground weather (SWOB Met-ML) and marine observations is an XML format for the distribution of weather data observed by Environment and Climate Change Canada and its partners.\nThe list of observation stations is available here:\nhttps://dd.meteo.gc.ca/observations/doc/swob-xml_station_list.csv\nThe list of marine observation stations is available here:\nhttps://dd.meteo.gc.ca/observations/doc/swob-xml_marine_station_list.csv\nThe list of partner stations is available here:\nhttps://dd.meteo.gc.ca/observations/doc/swob-xml_partner_station_list.csv\nThe format specification is available in the following directory: https://dd.meteo.gc.ca/observations/doc/\nHere is the structure of a SWOB file:\nFirst, we import python librairies and define the date we want to extract:\n# importations des librairies  from datetime import datetime, timedelta import pandas as pd import xml.etree.ElementTree as ET import wget import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import os from os.path import exists now = datetime.utcnow() - timedelta(hours=1) year = str(now.year) month = str(now.month) day = str(now.day) hour = f\u0026#39;{now.hour:02d}\u0026#39; day = f\u0026#39;{now.day:02d}\u0026#39; month = f\u0026#39;{now.month:02d}\u0026#39; date = year+month+day date '20200212'  We will list the stations we want to extract: example for the province of Quebec and Ontario.\nstations = pd.read_table(\u0026#34;swob-xml_station_list.csv\u0026#34;, sep=\u0026#34;,\u0026#34;) stations_quebec = stations.loc[(stations[\u0026#34;Province/Territory\u0026#34;]==\u0026#39;Quebec\u0026#39;) ,:] stations_ontario = stations.loc[(stations[\u0026#34;Province/Territory\u0026#34;]==\u0026#39;Ontario\u0026#39;) ,:] stations_mask=pd.concat([stations_quebec,stations_ontario],axis=0) stations_mask_sans_NaN = stations_mask.dropna() #stations_mask_sans_NaN_AUTO = stations_mask_sans_NaN.loc[stations_mask_sans_NaN[\u0026#34;AUTO/MAN\u0026#34;]==\u0026#39;AUTO\u0026#39;,:] stations_mask_sans_NaN.head()     IATA_ID Name WMO_ID MSC_ID Latitude Longitude Elevation(m) Data_Provider Dataset/Network AUTO/MAN Province/Territory     48 CGAH LA GRANDE IV 71823 7093GJ5 53.75 -73.6828 306.3 MSC CA AUTO Quebec   49 CGBC BAIE-COMEAU 71890 7040442 49.1351 -68.2024 22.6 MSC CA AUTO Quebec   50 CGCH CHIBOUGAMAU-CHAPAIS 73011 7091410 49.7773 -74.5304 389.3 MSC CA AUTO Quebec   51 CGCL CAUSAPSCAL AIRPORT 73005 7051201 48.3088 -67.2528 123 MSC CA AUTO Quebec   52 CGGA GASPE AIRPORT 73116 7052603 48.7833 -64.4833 40 MSC CA      Working with one station Let's work with CWTQ station ID:\nimport requests stid = \u0026#39;CWTQ\u0026#39; tmp_file = \u0026#39;https://dd.weather.gc.ca/observations/swob-ml/\u0026#39;+date+\u0026#39;/\u0026#39;+stid+\u0026#39;/\u0026#39;+year+\u0026#39;-\u0026#39;+month+\u0026#39;-\u0026#39;+day+\u0026#39;-\u0026#39;+hour+\u0026#39;00-\u0026#39;+stid+\u0026#39;-AUTO-swob.xml\u0026#39; tmp_file 'https://dd.weather.gc.ca/observations/swob-ml/20200212/CWTQ/2020-02-12-1900-CWTQ-AUTO-swob.xml'  wget.download(tmp_file) 100% [............................................................] 9408 / 9408 '2020-02-12-1900-CWTQ-AUTO-swob.xml'  tree = ET.parse(year+\u0026#39;-\u0026#39;+month+\u0026#39;-\u0026#39;+day+\u0026#39;-\u0026#39;+hour+\u0026#39;00-\u0026#39;+stid+\u0026#39;-AUTO-swob.xml\u0026#39;) root=tree.getroot() children = root.getchildren() Here is the root of our data to extract:\nroot \u0026lt;Element '{http://www.opengis.net/om/1.0}ObservationCollection' at 0x000000000950ED18\u0026gt;  children [\u0026lt;Element '{http://www.opengis.net/om/1.0}member' at 0x000000000950EE08\u0026gt;]  Number of elements in our file:\n# total amount of items print(len(root)) 1  for child in root.iter('*'):\nprint(child.items()) #print(child.keys())  for child in root.iter('*'):\nprint(child) print(child.attrib) print(child.attrib.get('value')) print(child.attrib.get('name'))  The list of variables is available on page 40 of the document: \u0026mdash; SWOB-ML_Product_User_Guide_v8.2_f.pdf\ndata=[] date_tm=[] avg_air_temp_pst1hr=[] max_air_temp_pst1hr=[] min_air_temp_pst1hr=[] max_rel_hum_pst1hr=[] min_rel_hum_pst1hr=[] stn_pres=[] mslp=[] pcpn_amt_pst1hr=[] wnd_dir_10m_pst1hr_max_spd=[] max_wnd_spd_10m_pst1hr=[] avg_wnd_spd_10m_pst10mts=[] avg_snw_dpth_pst5mts_1=[] avg_snw_dpth_pst5mts_2=[] avg_snw_dpth_pst5mts_3=[] for child in root.iter(\u0026#39;*\u0026#39;): if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;date_tm\u0026#39;: # date of actual observation date_tm.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_air_temp_pst1hr\u0026#39;: # average air temperature in the last hour  avg_air_temp_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;min_air_temp_pst1hr\u0026#39;: # minimum air temperature in the last hour min_air_temp_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;max_air_temp_pst1hr\u0026#39;: # maximum air temperature in the last hour  max_air_temp_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;max_rel_hum_pst1hr\u0026#39;: # maximum relative humidity in the past hour max_rel_hum_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;min_rel_hum_pst1hr\u0026#39;: # minimum relative humidity in the past hour min_rel_hum_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;stn_pres\u0026#39;: # pressure at the station (start of the hour) stn_pres.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;pcpn_amt_pst1hr\u0026#39;: # amount of precipitation in the past hour pcpn_amt_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_wnd_spd_10m_pst10mts\u0026#39;: # average wind speed at 10 m for 10 min avg_wnd_spd_10m_pst10mts.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;max_wnd_spd_10m_pst1hr\u0026#39;: # maximum wind speed at 10 m for one hour max_wnd_spd_10m_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;wnd_dir_10m_pst1hr_max_spd\u0026#39;: # Instant wind direction at 10 m for maximum hourly wind speed, min 0 to 60 wnd_dir_10m_pst1hr_max_spd.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;mslp\u0026#39;: # average sea level pressure mslp.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_snw_dpth_pst5mts_1\u0026#39;: avg_snw_dpth_pst5mts_1.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_snw_dpth_pst5mts_2\u0026#39;: avg_snw_dpth_pst5mts_2.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_snw_dpth_pst5mts_3\u0026#39;: avg_snw_dpth_pst5mts_3.append(child.get(\u0026#39;value\u0026#39;)) data_dict = {\u0026#39;date_tm\u0026#39;:date_tm, \u0026#39;avg_air_temp_pst1hr\u0026#39;:avg_air_temp_pst1hr, \u0026#39;min_air_temp_pst1hr\u0026#39;:min_air_temp_pst1hr, \u0026#39;max_air_temp_pst1hr\u0026#39;: max_air_temp_pst1hr, \u0026#39;max_rel_hum_pst1hr\u0026#39;:max_rel_hum_pst1hr, \u0026#39;min_rel_hum_pst1hr\u0026#39;:min_rel_hum_pst1hr, \u0026#39;stn_pres\u0026#39;:stn_pres, \u0026#39;mslp\u0026#39;:mslp, \u0026#39;pcpn_amt_pst1hr\u0026#39;:pcpn_amt_pst1hr, \u0026#39;avg_wnd_spd_10m_pst10mts\u0026#39;:avg_wnd_spd_10m_pst10mts, \u0026#39;wnd_dir_10m_pst1hr_max_spd\u0026#39;:wnd_dir_10m_pst1hr_max_spd, \u0026#39;max_wnd_spd_10m_pst1hr\u0026#39;:max_wnd_spd_10m_pst1hr, \u0026#39;avg_snw_dpth_pst5mts_1\u0026#39;:avg_snw_dpth_pst5mts_1, \u0026#39;avg_snw_dpth_pst5mts_2\u0026#39;:avg_snw_dpth_pst5mts_2, \u0026#39;avg_snw_dpth_pst5mts_3\u0026#39;:avg_snw_dpth_pst5mts_3 } data= pd.DataFrame(data_dict) data     date_tm avg_air_temp_pst1hr min_air_temp_pst1hr max_air_temp_pst1hr max_rel_hum_pst1hr min_rel_hum_pst1hr stn_pres mslp pcpn_amt_pst1hr avg_wnd_spd_10m_pst10mts wnd_dir_10m_pst1hr_max_spd max_wnd_spd_10m_pst1hr avg_snw_dpth_pst5mts_1 avg_snw_dpth_pst5mts_2 avg_snw_dpth_pst5mts_3     0 2020-02-18T20:00:00.000Z -0.7 -1.2 -0.3 86 83 1006.6 1010.7 0 40.8 151 56.4 37 33 37    Reading the station list stations = pd.read_table(\u0026#34;swob-xml_station_list.csv\u0026#34;, sep=\u0026#34;,\u0026#34;) stations.head()     IATA_ID Name WMO_ID MSC_ID Latitude Longitude Elevation(m) Data_Provider Dataset/Network AUTO/MAN Province/Territory     0 4400488 East Chedabucto Bay 44488 9302000 45.445 -60.9538 0 nan nan AUTO nan   1 4400489 West Chedabucto Bay 44489 9302001 45.4869 -61.141 0 nan nan AUTO nan   2 4400490 West Bay of Fundy 44490 9300300 44.6606 -66.3686 0 nan nan AUTO nan   3 4600303 Southern Georgia Strait 46303 9102000 49.025 -123.43 0 nan nan AUTO nan   4 4600304 Entrance to English Bay 46304 9102001 49.3017 -123.357 0 nan nan AUTO nan    Filter stations over a specific province stations_mask = stations.loc[stations[\u0026#34;Province/Territory\u0026#34;]==\u0026#39;Quebec\u0026#39;,:] stations_mask.head()     IATA_ID Name WMO_ID MSC_ID Latitude Longitude Elevation(m) Data_Provider Dataset/Network AUTO/MAN Province/Territory     48 CGAH LA GRANDE IV 71823 7093GJ5 53.75 -73.6828 306.3 MSC CA AUTO Quebec   49 CGBC BAIE-COMEAU 71890 7040442 49.1351 -68.2024 22.6 MSC CA AUTO Quebec   50 CGCH CHIBOUGAMAU-CHAPAIS 73011 7091410 49.7773 -74.5304 389.3 MSC CA AUTO Quebec   51 CGCL CAUSAPSCAL AIRPORT 73005 7051201 48.3088 -67.2528 123 MSC CA AUTO Quebec   52 CGGA GASPE AIRPORT 73116 7052603 48.7833 -64.4833 40 MSC CA AUTO Quebec    #stations_mask_sans_NaN = stations_mask.dropna()  #stations_mask_sans_NaN We want to work with only stations with automatic recording:\nstations_mask_AUTO = stations_mask.loc[stations_mask[\u0026#34;AUTO/MAN\u0026#34;]==\u0026#39;AUTO\u0026#39;,:] stations_mask_AUTO.head()     IATA_ID Name WMO_ID MSC_ID Latitude Longitude Elevation(m) Data_Provider Dataset/Network AUTO/MAN Province/Territory     48 CGAH LA GRANDE IV 71823 7093GJ5 53.75 -73.6828 306.3 MSC CA AUTO Quebec   49 CGBC BAIE-COMEAU 71890 7040442 49.1351 -68.2024 22.6 MSC CA AUTO Quebec   50 CGCH CHIBOUGAMAU-CHAPAIS 73011 7091410 49.7773 -74.5304 389.3 MSC CA AUTO Quebec   51 CGCL CAUSAPSCAL AIRPORT 73005 7051201 48.3088 -67.2528 123 MSC CA AUTO Quebec   52 CGGA GASPE AIRPORT 73116 7052603 48.7833 -64.4833 40 MSC CA AUTO Quebec    Example of a loop on all stations in Quebec:\nfor index, row in stations_mask_AUTO.iterrows(): stid = row[\u0026#39;IATA_ID\u0026#39;] tmp_file = \u0026#39;https://dd.weather.gc.ca/observations/swob-ml/\u0026#39;+date+\u0026#39;/\u0026#39;+stid+\u0026#39;/\u0026#39;+year+\u0026#39;-\u0026#39;+month+\u0026#39;-\u0026#39;+day+\u0026#39;-\u0026#39;+hour+\u0026#39;00-\u0026#39;+stid+\u0026#39;-AUTO-swob.xml\u0026#39; r = requests.get(tmp_file) if r.status_code != 200: continue root = ET.fromstring(r.content) data=[] date_tm=[] avg_air_temp_pst1hr=[] max_air_temp_pst1hr=[] min_air_temp_pst1hr=[] max_rel_hum_pst1hr=[] min_rel_hum_pst1hr=[] stn_pres=[] mslp=[] pcpn_amt_pst1hr=[] wnd_dir_10m_pst1hr_max_spd=[] max_wnd_spd_10m_pst1hr=[] avg_wnd_spd_10m_pst10mts=[] avg_snw_dpth_pst5mts_1=[] avg_snw_dpth_pst5mts_2=[] avg_snw_dpth_pst5mts_3=[] for child in root.iter(\u0026#39;*\u0026#39;): if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;date_tm\u0026#39;: # date-heure d’observation réelle date_tm.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_air_temp_pst1hr\u0026#39;: # température de l\u0026#39;air moyenne au cours de la dernière heure (minutes 00 à 60) avg_air_temp_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;min_air_temp_pst1hr\u0026#39;: # température de l\u0026#39;air minimale au cours de la dernière heure (minutes 00 à 60) min_air_temp_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;max_air_temp_pst1hr\u0026#39;: # température de l\u0026#39;air maximale au cours de la dernière heure (minutes 00 à 60)  max_air_temp_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;max_rel_hum_pst1hr\u0026#39;: # humidité relative maximale au cours de la dernière heure  max_rel_hum_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;min_rel_hum_pst1hr\u0026#39;:# humidité relative minimale au cours de la dernière heure min_rel_hum_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;stn_pres\u0026#39;: # pression à la station (début de l’heure) stn_pres.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;pcpn_amt_pst1hr\u0026#39;: # quantité de précipitation au cours de la dernière heure pcpn_amt_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_wnd_spd_10m_pst10mts\u0026#39;: # vitesse moyenne du vent à 10 m pendant 10 min avg_wnd_spd_10m_pst10mts.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;max_wnd_spd_10m_pst1hr\u0026#39;: # vitesse maximale du vent à 10 m pendant une heure max_wnd_spd_10m_pst1hr.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;wnd_dir_10m_pst1hr_max_spd\u0026#39;: #Direction instantanée du vent à 10 m pour une vitesse horaire maximale du vent, min 0 à 60 wnd_dir_10m_pst1hr_max_spd.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;mslp\u0026#39;: # pression moyenne au niveau de la mer mslp.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_snw_dpth_pst5mts_1\u0026#39;: avg_snw_dpth_pst5mts_1.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_snw_dpth_pst5mts_2\u0026#39;: avg_snw_dpth_pst5mts_2.append(child.get(\u0026#39;value\u0026#39;)) if child.attrib.get(\u0026#39;name\u0026#39;) == \u0026#39;avg_snw_dpth_pst5mts_3\u0026#39;: avg_snw_dpth_pst5mts_3.append(child.get(\u0026#39;value\u0026#39;)) data_dict = {\u0026#39;date_tm\u0026#39;:date_tm, \u0026#39;avg_air_temp_pst1hr\u0026#39;:avg_air_temp_pst1hr, \u0026#39;min_air_temp_pst1hr\u0026#39;:min_air_temp_pst1hr, \u0026#39;max_air_temp_pst1hr\u0026#39;: max_air_temp_pst1hr, \u0026#39;max_rel_hum_pst1hr\u0026#39;:max_rel_hum_pst1hr, \u0026#39;min_rel_hum_pst1hr\u0026#39;:min_rel_hum_pst1hr, \u0026#39;stn_pres\u0026#39;:stn_pres, \u0026#39;mslp\u0026#39;:mslp, \u0026#39;pcpn_amt_pst1hr\u0026#39;:pcpn_amt_pst1hr, \u0026#39;avg_wnd_spd_10m_pst10mts\u0026#39;:avg_wnd_spd_10m_pst10mts, \u0026#39;wnd_dir_10m_pst1hr_max_spd\u0026#39;:wnd_dir_10m_pst1hr_max_spd, \u0026#39;max_wnd_spd_10m_pst1hr\u0026#39;:max_wnd_spd_10m_pst1hr, \u0026#39;avg_snw_dpth_pst5mts_1\u0026#39;:avg_snw_dpth_pst5mts_1, \u0026#39;avg_snw_dpth_pst5mts_2\u0026#39;:avg_snw_dpth_pst5mts_2, \u0026#39;avg_snw_dpth_pst5mts_3\u0026#39;:avg_snw_dpth_pst5mts_3 } for key,value in data_dict.items(): if not value: data_dict[key] = \u0026#39;NaN\u0026#39; data= pd.DataFrame(data_dict) os.makedirs(\u0026#39;D:/Utilisateurs/guillaume/Documents/GitHub/Python_XML/SWOB/\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;/\u0026#39;, exist_ok=True) # file = \u0026#39;D:/Utilisateurs/guillaume/Documents/GitHub/Python_XML/SWOB/\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;/\u0026#39;+str(int(row[\u0026#39;WMO_ID\u0026#39;]))+\u0026#39;_\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;.csv\u0026#39; file = \u0026#39;D:/Utilisateurs/guillaume/Documents/GitHub/Python_XML/SWOB/\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;/\u0026#39;+str(row[\u0026#39;MSC_ID\u0026#39;])+\u0026#39;_\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;.csv\u0026#39; if exists(file): #print (\u0026#34;File exist\u0026#34;) data.to_csv(r\u0026#39;D:/Utilisateurs/guillaume/Documents/GitHub/Python_XML/SWOB/\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;/\u0026#39;+str(row[\u0026#39;MSC_ID\u0026#39;])+\u0026#39;_\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;.csv\u0026#39;,mode=\u0026#39;a\u0026#39;,index=False,header=False) else: #print (\u0026#34;File not exist\u0026#34;) pd.DataFrame(row).T.to_csv(r\u0026#39;D:/Utilisateurs/guillaume/Documents/GitHub/Python_XML/SWOB/\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;/\u0026#39;+str(row[\u0026#39;MSC_ID\u0026#39;])+\u0026#39;_\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;.csv\u0026#39;,index=False) data.to_csv(r\u0026#39;D:/Utilisateurs/guillaume/Documents/GitHub/Python_XML/SWOB/\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;/\u0026#39;+str(row[\u0026#39;MSC_ID\u0026#39;])+\u0026#39;_\u0026#39;+now.strftime(\u0026#34;%Y%m\u0026#34;)+\u0026#39;.csv\u0026#39;,mode=\u0026#39;a\u0026#39;,index=False) We will trace the location of the stations: We will start by extracting the latitudes and longitudes:\nlist_lat = stations.loc[stations[\u0026#34;AUTO/MAN\u0026#34;]==\u0026#39;AUTO\u0026#39;,:][\u0026#39;Latitude\u0026#39;].values list_lon = stations.loc[stations[\u0026#34;AUTO/MAN\u0026#34;]==\u0026#39;AUTO\u0026#39;,:][\u0026#39;Longitude\u0026#39;].values import cartopy.crs as ccrs import cartopy.feature as cfeature import matplotlib.pylab as plt import numpy as np fig = plt.figure(figsize=(28,16)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.set_extent((-150.0, -50, 40, 90)) ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.gridlines() mm = ax.scatter(list_lon, list_lat, c=\u0026#39;red\u0026#39;, s=100, label=\u0026#39;SWOB Met-ML MSC\u0026#39;) #mm = ax.scatter(dfstat[\u0026#39;lon\u0026#39;].values, dfstat[\u0026#39;lat\u0026#39;].values, c=\u0026#39;green\u0026#39;, s=400, label=\u0026#39;Site\u0026#39;) plt.legend(loc=\u0026#34;best\u0026#34;, markerscale=1., scatterpoints=1, fontsize=40) # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) fig.canvas.draw() plt.savefig(\u0026#39;Localisation_SWOB_CANADA.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() plt.close() Just over Quebec: list_lat = stations.loc[(stations[\u0026#34;AUTO/MAN\u0026#34;]==\u0026#39;AUTO\u0026#39;) \u0026amp; (stations[\u0026#34;Province/Territory\u0026#34;]==\u0026#39;Quebec\u0026#39;),:][\u0026#39;Latitude\u0026#39;].values list_lon = stations.loc[(stations[\u0026#34;AUTO/MAN\u0026#34;]==\u0026#39;AUTO\u0026#39;) \u0026amp; (stations[\u0026#34;Province/Territory\u0026#34;]==\u0026#39;Quebec\u0026#39;),:][\u0026#39;Longitude\u0026#39;].values import cartopy.crs as ccrs import cartopy.feature as cfeature import matplotlib.pylab as plt import numpy as np fig = plt.figure(figsize=(28,16)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.set_extent((-80.0, -50, 40, 70)) ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.gridlines() mm = ax.scatter(list_lon, list_lat, c=\u0026#39;red\u0026#39;, s=100, label=\u0026#39;SWOB Met-ML MSC\u0026#39;) #mm = ax.scatter(dfstat[\u0026#39;lon\u0026#39;].values, dfstat[\u0026#39;lat\u0026#39;].values, c=\u0026#39;green\u0026#39;, s=400, label=\u0026#39;Site\u0026#39;) plt.legend(loc=\u0026#34;best\u0026#34;, markerscale=1., scatterpoints=1, fontsize=40) # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) fig.canvas.draw() plt.savefig(\u0026#39;Localisation_SWOB_Quebec.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() plt.close() If we want to plot by grouping by province: We only want the \u0026ldquo;Automatic\u0026rdquo; stations and group them by province.\ngrouped_dataframe = stations.loc[stations[\u0026#34;AUTO/MAN\u0026#34;]==\u0026#39;AUTO\u0026#39;,:].groupby(\u0026#39;Province/Territory\u0026#39;) grouped_dataframe.count()    Province/Territory IATA_ID Name WMO_ID MSC_ID Latitude Longitude Elevation(m) Data_Provider Dataset/Network AUTO/MAN     Alberta 240 240 196 240 240 240 240 240 240 240   British Columbia 119 119 101 119 119 119 119 119 119 119   Manitoba 47 47 44 47 47 47 47 47 47 47   New Brunswick 21 21 20 21 21 21 21 21 21 21   Newfoundland and Labrador 36 36 34 36 36 36 36 36 36 36   Northwest Territories 49 49 34 49 49 49 49 49 49 49   Nova Scotia 42 42 39 42 42 42 42 42 42 42   Nunavut 63 63 52 63 63 63 63 63 63 63   Ontario 111 111 103 111 111 111 111 111 111 111   Prince Edward Island 7 7 5 7 7 7 7 7 7 7   Quebec 122 122 103 122 122 122 122 122 122 122   Saskatchewan 57 57 56 57 57 57 57 57 57 57   Yukon 23 23 17 23 23 23 23 23 23 23    for name,group in grouped_dataframe: print(name) Alberta British Columbia Manitoba New Brunswick Newfoundland and Labrador Northwest Territories Nova Scotia Nunavut Ontario Prince Edward Island Quebec Saskatchewan Yukon  grouped_dataframe.get_group(\u0026#39;Quebec\u0026#39;).head()     IATA_ID Name WMO_ID MSC_ID Latitude Longitude Elevation(m) Data_Provider Dataset/Network AUTO/MAN Province/Territory     48 CGAH LA GRANDE IV 71823 7093GJ5 53.75 -73.6828 306.3 MSC CA AUTO Quebec   49 CGBC BAIE-COMEAU 71890 7040442 49.1351 -68.2024 22.6 MSC CA AUTO Quebec   50 CGCH CHIBOUGAMAU-CHAPAIS 73011 7091410 49.7773 -74.5304 389.3 MSC CA AUTO Quebec   51 CGCL CAUSAPSCAL AIRPORT 73005 7051201 48.3088 -67.2528 123 MSC CA AUTO Quebec   52 CGGA GASPE AIRPORT 73116 7052603 48.7833 -64.4833 40 MSC CA AUTO Quebec    We create a random color list for all of our provinces.\nimport random number_of_colors = len(grouped_dataframe) color = [\u0026#34;#\u0026#34;+\u0026#39;\u0026#39;.join([random.choice(\u0026#39;0123456789ABCDEF\u0026#39;) for j in range(6)]) for i in range(number_of_colors)] import cartopy.crs as ccrs import cartopy.feature as cfeature import matplotlib.pylab as plt import numpy as np fig = plt.figure(figsize=(28,16)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.set_extent((-150.0, -50, 40, 90)) ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.gridlines() i = 0 for name,group in grouped_dataframe: list_lon= grouped_dataframe.get_group(name)[\u0026#39;Longitude\u0026#39;].values list_lat= grouped_dataframe.get_group(name)[\u0026#39;Latitude\u0026#39;].values mm = ax.scatter(list_lon, list_lat, c=color[i], s=100, label=\u0026#39;SWOB Met-ML MSC: \u0026#39;+ name ) plt.legend(loc=\u0026#34;best\u0026#34;, markerscale=1., scatterpoints=1, fontsize=10) i+=1 # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) fig.canvas.draw() plt.savefig(\u0026#39;Localisation_SWOB_CANADA.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() plt.close() ","date":1582059998,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582059998,"objectID":"13246bb27bf44a0599dc7971b7e4ba11","permalink":"/post/ecc_swob_stations/","publishdate":"2020-02-18T13:06:38-08:00","relpermalink":"/post/ecc_swob_stations/","section":"post","summary":"﻿\nWorking with XML (Extensible Markup Language) file using Python Part 1: Surface Weather Observation XML (SW-OB-XML) SW-OB-XML For xml files, we will use a package called xml which will allow us to decrypt the xml file.\nPandas does not directly have tools to transform xml into DataFrame because the semi-structured aspect of xml forces us to perform a few steps before filling out a DataFrame.\nWe will first download an xml file located in this directory:","tags":[],"title":"ECC_SWOB_Stations","type":"post"},{"authors":[],"categories":[],"content":"﻿\nAbout Vector Data Vector data are composed of discrete geometric locations (x, y values) known as vertices that define the “shape” of the spatial object.\nThe organization of the vertices determines the type of vector that you are working with. There are three types of vector data:\n  Points: Each individual point is defined by a single x, y coordinate. There can be many points in a vector point file.   Lines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each “bend” in the road or stream represents a vertex that has defined x, y location.\n  ![png](./output_20_1.png)   Polygons: A polygon consists of 3 or more vertices that are connected and “closed”. Thus the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons. Occasionally, a polygon can have a hole in the middle of it (like a doughnut), this is something to be aware of but not an issue you will deal with in this tutorial.  ![png](output_30_1.png)  Shapefiles: Points, Lines, and Polygons Geospatial data in vector format are often stored in a shapefile format. Because the structure of points, lines, and polygons are different, each individual shapefile can only contain one vector type (all points, all lines or all polygons). You will not find a mixture of point, line and polygon objects in a single shapefile.\nObjects stored in a shapefile often have a set of associated attributes that describe the data. For example, a line shapefile that contains the locations of streams, might contain the associated stream name, stream “order” and other information about each stream line object.\nA shapefile is created by 3 or more files, all of which must retain the same NAME and be stored in the same file directory, in order for you to be able to work with them.\nShapefile Structure There are 3 key files associated with any and all shapefiles:\n .shp: the file that contains the geometry for all features. .shx: the file that indexes the geometry. .dbf: the file that stores feature attributes in a tabular format.  Sometimes, a shapefile will have other associated files including:\n .prj: the file that contains information on projection format including the coordinate system and projection information. It is a plain text file describing the projection using well-known text (WKT) format. .sbn and .sbx: the files that are a spatial index of the features. .shp.xml: the file that is the geospatial metadata in XML format, (e.g. ISO 19115 or XML format).  We will use the geopandas library to work with vector data in Python. You will also use matplotlib.pyplot to plot your data.\nFirst, we import librairies.\nimport warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) from matplotlib import pyplot as plt %matplotlib inline import geopandas as gpd 1- Point shapefile We can then open our first shapefile which contains point locations\nshapes = gpd.read_file(\u0026#34;shapefiles\\BV_SJ_ponts.shp\u0026#34;) Each object in a shapefile has one or more attributes associated with it. Shapefile attributes are similar to fields or columns in a spreadsheet. Each row in the spreadsheet has a set of columns associated with it that describe the row element. In the case of a shapefile, each row represents a spatial object - for example, a road, represented as a line in a line shapefile, will have one “row” of attributes associated with it. These attributes can include different types of information that describe objects stored within a shapefile.\nshapes.head(2)     Id ORIG_FID OCEAN_EN OCEAN_FR WSCSSDA WSCSDA WSCMDA WSCSSDA_EN WSCSDA_EN WSCMDA_EN WSCSSDA_FR WSCSDA_FR WSCMDA_FR BUFF_DIST ORIG_FID_1 Shape_Leng Shape_Area geometry     0 0 1 Atlantic Ocean OcÃ©an Atlantique 01AE 01A 01 Fish (Maine) Saint John and Southern Bay of Fundy (N.B.) Maritime Provinces Drainage Area Fish (Maine) Saint-Jean et sud de la baie de Fundy (N.-B.) Aire de drainage des provinces Maritimes 1 0 23.6325 6.74042 POINT (-68.31290411677341 47.93509135554499)   1 0 1 Atlantic Ocean OcÃ©an Atlantique 01AE 01A 01 Fish (Maine) Saint John and Southern Bay of Fundy (N.B.) Maritime Provinces Drainage Area Fish (Maine) Saint-Jean et sud de la baie de Fundy (N.-B.) Aire de drainage des provinces Maritimes 1 0 23.6325 6.74042 POINT (-68.10019745512986 47.8385423546759)   2 0 1 Atlantic Ocean OcÃ©an Atlantique 01AE 01A 01 Fish (Maine) Saint John and Southern Bay of Fundy (N.B.) Maritime Provinces Drainage Area Fish (Maine) Saint-Jean et sud de la baie de Fundy (N.-B.) Aire de drainage des provinces Maritimes 1 0 23.6325 6.74042 POINT (-68.05084811719591 47.67967312057342)   3 0 1 Atlantic Ocean OcÃ©an Atlantique 01AE 01A 01 Fish (Maine) Saint John and Southern Bay of Fundy (N.B.) Maritime Provinces Drainage Area Fish (Maine) Saint-Jean et sud de la baie de Fundy (N.-B.) Aire de drainage des provinces Maritimes 1 0 23.6325 6.74042 POINT (-67.94013835894734 47.46553718633378)   4 0 1 Atlantic Ocean OcÃ©an Atlantique 01AE 01A 01 Fish (Maine) Saint John and Southern Bay of Fundy (N.B.) Maritime Provinces Drainage Area Fish (Maine) Saint-Jean et sud de la baie de Fundy (N.-B.) Aire de drainage des provinces Maritimes 1 0 23.6325 6.74042 POINT (-67.79256589502562 47.36265881338679)    Notice that the geopandas data structure is a data.frame that contains a geometry column where the x, y point location values are stored. All of the other shapefile feature attributes are contained in columns, similar to what you may be used to if you’ve used a GIS tool such as ArcGIS or QGIS.\nShapefile Metadata \u0026amp; Attributes When you import the shapefile layer into Python the gpd.read_file() function automatically stores information about the data as attributes. You are particularly interested in the geospatial metadata, describing the format, CRS, extent, and other components of the vector data, and the attributes which describe properties associated with each individual vector object.\nSpatial Metadata Key metadata for all shapefiles include:\n- Object Type: the class of the imported object. - Coordinate Reference System (CRS): the projection of the data. - Extent: the spatial extent (geographic area that the shapefile covers) of the shapefile. Note that the spatial extent for a shapefile represents the extent for ALL spatial objects in the shapefile.  You can view shapefile metadata using the class(), .crs and .total_bounds methods:\ntype(shapes) geopandas.geodataframe.GeoDataFrame  shapes.total_bounds array([-70.40000541, 45.21527633, -65.03721851, 48.06254733])  shapes.crs {'init': 'epsg:4269'}  The CRS for our data is epsg code: 4269.\nshapes.geom_type.head(2) 0 Point 1 Point dtype: object  shapes.geometry.head() 0 POINT (-68.31290411677341 47.93509135554499) 1 POINT (-68.10019745512986 47.8385423546759) 2 POINT (-68.05084811719591 47.67967312057342) 3 POINT (-67.94013835894734 47.46553718633378) 4 POINT (-67.79256589502562 47.36265881338679) Name: geometry, dtype: object  How Many Features Are in Your Shapefile? You can view the number of features (counted by the number of rows in the attribute table) and feature attributes (number of columns) in our data using the pandas .shape method. Note that the data are returned as a vector of two values:\n(rows, columns)\nAlso note that the number of columns includes a column where the geometry (the x, y coordinate locations) are stored.\nshapes.shape (72, 18)  Plot our Shapefile you can visualize the data in your Python geodata.frame object using the .plot() method.\nshapes.loc[:, \u0026#39;geometry\u0026#39;].plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x9b129b0\u0026gt;  2- Linestring shapefile We had previously a POINT object.\nWe will then convert POINT to LINESTRING.\nfrom shapely.geometry import LineString, mapping def point_to_linestring(fili_shps): gdf = gpd.read_file(fili_shps) #POINTS latlon = [mapping(x)[\u0026#39;coordinates\u0026#39;] for x in gdf.geometry] lats = [x[1] for x in latlon] lons = [x[0] for x in latlon] linestr = LineString(zip(lons, lats)) return gpd.GeoDataFrame(index=[0], crs=gdf.crs, geometry=[linestr]) line_shapes = point_to_linestring(\u0026#34;shapefiles\\BV_SJ_ponts.shp\u0026#34;) line_shapes.loc[:, \u0026#39;geometry\u0026#39;].plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xa55f128\u0026gt;  line_shapes     geometry     0 LINESTRING (-68.31290411677341 47.93509135554499, \u0026hellip; )    If we want to save our new shapefile:\nline_shapes.to_file(\u0026#34;shapefiles\\BV_SJ_lines.shp\u0026#34;) And for example we can save latitude and longitude values into csv format:\nimport shapefile import os def shp2csv(shp_file): \u0026#39;\u0026#39;\u0026#39;Outputs a csv file based on input shapefile vertices\u0026#39;\u0026#39;\u0026#39; out = os.path.splitext(shp_file)[0]+\u0026#39;_pnts.csv\u0026#39; with open(out, \u0026#39;w\u0026#39;) as csv: with shapefile.Reader(shp_file) as sf: for shp_rec in sf.shapeRecords(): csv.write(\u0026#39;{}\\n\u0026#39;.format(shp_rec.record)) for pnt in shp_rec.shape.points: csv.write(\u0026#39;{}\\n\u0026#39;.format(pnt)) shp2csv(\u0026#34;shapefiles\\BV_SJ_lines.shp\u0026#34;) import pandas as pd BV_border = pd.read_csv(\u0026#39;.\\shapefiles\\BV_SJ_lines_pnts.csv\u0026#39;, sep=\u0026#39;,\u0026#39;,skiprows = range(0, 1)) BV_border.columns=[\u0026#34;lon\u0026#34;, \u0026#34;lat\u0026#34;] BV_border[\u0026#34;lon\u0026#34;]=BV_border[\u0026#34;lon\u0026#34;].apply(lambda x: x.replace(\u0026#34;(\u0026#34;, \u0026#34;\u0026#34;)).apply(pd.to_numeric,1) BV_border[\u0026#34;lat\u0026#34;]=BV_border[\u0026#34;lat\u0026#34;].apply(lambda x: x.replace(\u0026#34;)\u0026#34;, \u0026#34;\u0026#34;)).apply(pd.to_numeric,1) BV_border.head() BV_border.lon BV_border.append(BV_border, ignore_index=True) BV_border.head()     lon lat     0 -68.1002 47.8385   1 -68.0508 47.6797   2 -67.9401 47.4655   3 -67.7926 47.3627   4 -67.6059 47.4158    import matplotlib.pyplot as plt import cartopy.crs as ccrs from cartopy.io.shapereader import Reader from cartopy.feature import ShapelyFeature fname = r\u0026#39;shapefiles\\BV_SJ_lines.shp\u0026#39; fig=plt.figure(figsize=(10,10), frameon=True) ax = plt.axes(projection=ccrs.Robinson()) ax.set_extent([-70,-64,45,50]) #ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;50m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.gridlines() # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) fig.canvas.draw() colors = [\u0026#39;red\u0026#39;] maskBV = [\u0026#39;BV\u0026#39;] cs = ax.plot(BV_border.lon,BV_border.lat, transform=ccrs.PlateCarree(), color=colors[0], linewidth=2, label=maskBV[0]) shape_feature = ShapelyFeature(Reader(fname).geometries(), ccrs.PlateCarree(), edgecolor=\u0026#39;red\u0026#39;) ax.add_feature(shape_feature, facecolor=\u0026#39;yellow\u0026#39;) plt.legend(loc=\u0026#34;best\u0026#34;, markerscale=2., fontsize=10) plt.savefig(\u0026#39;./figure.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() plt.close() print(\u0026#39;Terminé\u0026#39;) Terminé  3- Polygon shapefile We can now convert our linestring shapefile to a polygon shapefile:\nfrom shapely.geometry import Polygon, mapping def linestring_to_polygon(fili_shps): gdf = gpd.read_file(fili_shps) #LINESTRING geom = [x for x in gdf.geometry] all_coords = mapping(geom[0])[\u0026#39;coordinates\u0026#39;] lats = [x[1] for x in all_coords] lons = [x[0] for x in all_coords] linestr = Polygon(zip(lons, lats)) return gpd.GeoDataFrame(index=[0], crs=gdf.crs, geometry=[linestr]) poly_shapes = linestring_to_polygon(\u0026#34;shapefiles\\BV_SJ_lines.shp\u0026#34;) poly_shapes.to_file(\u0026#39;shapefiles\\BV_SJ_WGS84.shp\u0026#39;) poly_shapes.loc[:, \u0026#39;geometry\u0026#39;].plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xa5f0a90\u0026gt;  poly_shapes     geometry     0 POLYGON ((-68.31290411677341 47.93509135554499, \u0026hellip;))    4- Find if a point in inside a polygon Function bellow will check if a point, defined by a specific latitude and longitude, is inside or not a polygon shapefile.\ndef check(lon, lat, ds_in): lyr_in = ds_in.GetLayer(0) # create point geometry pt = ogr.Geometry(ogr.wkbPoint) pt.SetPoint_2D(0, lon, lat) # go over all the polygons in the layer see if one include the point for feat_in in lyr_in: # roughly subsets features, instead of go over everything ply = feat_in.GetGeometryRef() # test if ply.Contains(pt): # TODO do what you need to do here print(\u0026#39;Point in shapefile polygon\u0026#39;) return(lon, lat) return(True) else: print(\u0026#39;Point not in shapefile polygon\u0026#39;) ds_in = ogr.Open(\u0026#34;shapefiles\\BV_SJ_WGS84.shp\u0026#34;) lon = -68. lat = 47 ind = check(lon, lat, ds_in) ind Point in shapefile polygon (-68.0, 47)  import matplotlib.pyplot as plt import cartopy.crs as ccrs from cartopy.io.shapereader import Reader from cartopy.feature import ShapelyFeature fname = r\u0026#39;shapefiles\\BV_SJ_WGS84.shp\u0026#39; fig=plt.figure(figsize=(28,16), frameon=True) ax = plt.axes(projection=ccrs.Robinson()) ax.set_extent([-70,-64,45,50]) ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.plot(ind[0], ind[1], \u0026#39;go\u0026#39;, ms=20, transform=ccrs.Geodetic()) ax.text(ind[0] + .35, ind[1] + .15, \u0026#39;Point in shapefile polygon\u0026#39;, va=\u0026#39;center\u0026#39;, ha=\u0026#39;right\u0026#39;, color= \u0026#39;green\u0026#39;, transform=ccrs.Geodetic(), fontweight=\u0026#39;bold\u0026#39;) ax.plot(-66, 48.5, \u0026#39;ro\u0026#39;, ms=20, transform=ccrs.Geodetic()) ax.text(-66 + .35, 48.5 + .15, \u0026#39;Point outside shapefile polygon\u0026#39;, va=\u0026#39;center\u0026#39;, ha=\u0026#39;right\u0026#39;, color= \u0026#39;red\u0026#39;, transform=ccrs.Geodetic(), fontweight=\u0026#39;bold\u0026#39;) shape_feature = ShapelyFeature(Reader(fname).geometries(), ccrs.PlateCarree(), edgecolor=\u0026#39;black\u0026#39;) ax.add_feature(shape_feature, facecolor=\u0026#39;yellow\u0026#39;) plt.show() ","date":1581969111,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581969111,"objectID":"4f80d7619a1ed5da2019bf11192286d1","permalink":"/post/shapefiles_in_python/","publishdate":"2020-02-17T11:51:51-08:00","relpermalink":"/post/shapefiles_in_python/","section":"post","summary":"﻿\nAbout Vector Data Vector data are composed of discrete geometric locations (x, y values) known as vertices that define the “shape” of the spatial object.\nThe organization of the vertices determines the type of vector that you are working with. There are three types of vector data:\n  Points: Each individual point is defined by a single x, y coordinate. There can be many points in a vector point file.","tags":[],"title":"Shapefiles_in_python","type":"post"},{"authors":[],"categories":[],"content":"﻿\nFrom shapefiles to Netcdf Mask In this tutorial, we will use shapefiles to create masks over canadian provinces.\nWe will use gridded dataset ANUSPLIN meteorological data.\nAgriculture and Agri-Food Canada have produced daily precipitation, minimum and maximum temperature across Canada (south of 60°N) for climate related application purpose using thin-plate smoothing splines, as implemented in the ANUSPLIN climate modeling software (Hutchinson et al., 2009; McKenney et al., 2011).\nThe so-called ANUSPLIN data uses ground-based observations and generates daily gridded data from 1951 to 2017 on a Lambert conformal conic projection with 5’ arc minutes spacing (equivalent to a resolution of about 10 km). The key strength of this spatial interpolation method is its global dependence on all data, permitting robust and stable determination of spatially varying dependences on elevation. Hutchinson et al. (2009) have shown that while ANUSPLIN fall month’s absolute errors were remarkably small, those of winter months were quite large due to rather difficult observation and measurement conditions.\nData are available on:\nftp://ftp.nrcan.gc.ca/pub/outgoing/canada_daily_grids\nI will present a solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - numpy: for simple array manipulations - geopandas: to open shapefiles - osgeo: - matplotlib: for plotting  1- First we need to import librairies and create aliases. import xarray as xr import numpy as np import regionmask import geopandas as gpd import pandas as pd import matplotlib.pyplot as plt from osgeo import ogr import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) %matplotlib inline Working over canadian province In this example we will use canadian province shapefiles developped by statistics canada.\nShapefiles are available on this website: https://www12.statcan.gc.ca/census-recensement/2011/geo/bound-limit/bound-limit-2016-eng.cfm\nAfter downloaded the shapefile, we must load it using geopandas library:\nPATH_TO_SHAPEFILE = \u0026#39;./Canadian_province/lpr_000b16a_e/lpr_000b16a_e.shp\u0026#39; province = gpd.read_file(PATH_TO_SHAPEFILE) province     PRUID PRNAME PRENAME PRFNAME PREABBR PRFABBR     0 10 Newfoundland and Labrador / Terre-Neuve-et-Labrador Newfoundland and Labrador Terre-Neuve-et-Labrador N.L. T.-N.-L.   1 11 Prince Edward Island / Île-du-Prince-Édouard Prince Edward Island Île-du-Prince-Édouard P.E.I. Î.-P.-É.   2 12 Nova Scotia / Nouvelle-Écosse Nova Scotia Nouvelle-Écosse N.S. N.-É.   3 13 New Brunswick / Nouveau-Brunswick New Brunswick Nouveau-Brunswick N.B. N.-B.   4 24 Quebec / Québec Quebec Québec Que. Qc   5 35 Ontario Ontario Ontario Ont. Ont.   6 46 Manitoba Manitoba Manitoba Man. Man.   7 47 Saskatchewan Saskatchewan Saskatchewan Sask. Sask.   8 48 Alberta Alberta Alberta Alta. Alb.   9 59 British Columbia / Colombie-Britannique British Columbia Colombie-Britannique B.C. C.-B.   10 60 Yukon Yukon Yukon Y.T. Yn   11 61 Northwest Territories / Territoires du Nord-Ouest Northwest Territories Territoires du Nord-Ouest N.W.T. T.N.-O.   12 62 Nunavut Nunavut Nunavut Nvt. Nt    Shapes are here a GeoDataFrame containing all polygons illustrating the province boundaries.\nID_PROV = 5 print(province.PRNAME[ID_PROV]) Ontario  province.loc[:, \u0026#39;geometry\u0026#39;].plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xad67400\u0026gt;  tmpWGS84 = province.to_crs({\u0026#39;proj\u0026#39;:\u0026#39;longlat\u0026#39;, \u0026#39;ellps\u0026#39;:\u0026#39;WGS84\u0026#39;, \u0026#39;datum\u0026#39;:\u0026#39;WGS84\u0026#39;}) tmpWGS84.plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1327ac88\u0026gt;  Now we can load the ANUSPLIN_10km gridded data. The parameter chunks is very important, it defines how big are the “pieces” of data moved from the disk to the memory. With this value the entire computation on a workstation with 32 GB takes a couple of minutes.\nWe will load all the temperature files using Xarray library.\nmodel=\u0026#39;ANUSPLIN_10km_YEAR_Mean_tasmoy_1950-2017\u0026#39; t_in = \u0026#39;J:/DONNEES_AMERIQUE_DU_NORD/ANUSPLIN_10km/Netcdf/INDICES_ANNEES/Mean_tasmoy/\u0026#39; data = t_in + model + \u0026#39;.nc\u0026#39; ds = xr.open_mfdataset(data, chunks = {\u0026#39;time\u0026#39;: 10}) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (time: 68, x: 510, y: 1068) Coordinates: lon (y, x) float32 dask.array\u0026lt;shape=(1068, 510), chunksize=(1068, 510)\u0026gt; lat (y, x) float32 dask.array\u0026lt;shape=(1068, 510), chunksize=(1068, 510)\u0026gt; * time (time) float64 1.0 2.0 3.0 4.0 5.0 ... 64.0 65.0 66.0 67.0 68.0 Dimensions without coordinates: x, y Data variables: Mean_tasmoy (time, y, x) float32 dask.array\u0026lt;shape=(68, 1068, 510), chunksize=(10, 1068, 510)\u0026gt;  Our xarray Dataset contains a single variable (Mean_tasmoy) which is stored as a dask.array. This is the result of loading files with open_mfdataset.\nNow we will use regionmask module to create a gridded mask with the function regions_cls documented here: https://regionmask.readthedocs.io/en/stable/generated/regionmask.Regions_cls.html#regionmask.Regions_cls\nWith this function we will create an object able to mask ANUSPLIN gridded data.\nprovince_mask_poly = regionmask.Regions_cls(name = \u0026#39;PRENAME\u0026#39;, numbers = list(range(0,13)), names = list(tmpWGS84.PRENAME), abbrevs = list(tmpWGS84.PRENAME), outlines = list(tmpWGS84.geometry.values[i] for i in range(0,13))) province_mask_poly 13 'PRENAME' Regions () Newfoundland and Labrador Prince Edward Island Nova Scotia New Brunswick Quebec Ontario Manitoba Saskatchewan Alberta British Columbia Yukon Northwest Territories Nunavut  Now we are ready to apply the mask on the gridded dataset xarray ANUSPLIN.\nWe select only the first timestep to speed up the process.\nmask = province_mask_poly.mask(ds.isel(time = 0), lat_name=\u0026#39;lat\u0026#39;, lon_name=\u0026#39;lon\u0026#39;) mask \u0026lt;xarray.DataArray (y: 1068, x: 510)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * y (y) int64 0 1 2 3 4 5 6 7 ... 1061 1062 1063 1064 1065 1066 1067 * x (x) int64 0 1 2 3 4 5 6 7 8 ... 501 502 503 504 505 506 507 508 509 lat (y, x) float32 83.45833 83.375 83.291664 ... 41.125 41.041668 lon (y, x) float32 -140.95833 -140.95833 ... -52.04167 -52.04167  mask \u0026lt;xarray.DataArray (y: 1068, x: 510)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * y (y) int64 0 1 2 3 4 5 6 7 ... 1061 1062 1063 1064 1065 1066 1067 * x (x) int64 0 1 2 3 4 5 6 7 8 ... 501 502 503 504 505 506 507 508 509 lat (y, x) float32 83.45833 83.375 83.291664 ... 41.125 41.041668 lon (y, x) float32 -140.95833 -140.95833 ... -52.04167 -52.04167  Mask can be saved (for example as a NetCDF) for a later use.\nmask.to_netcdf(\u0026#39;./mask_all_province.nc\u0026#39;) A quick visualisation:\nplt.figure(figsize=(15,8)) ax = plt.axes() mask.plot(ax = ax) province.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;, lw = 1) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1b989828\u0026gt;  2- Extract one province We will sho how to mask Ontario province.\nID_PROV = 5 print(province.PRNAME[ID_PROV]) Ontario  mask \u0026lt;xarray.DataArray (y: 1068, x: 510)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * y (y) int64 0 1 2 3 4 5 6 7 ... 1061 1062 1063 1064 1065 1066 1067 * x (x) int64 0 1 2 3 4 5 6 7 8 ... 501 502 503 504 505 506 507 508 509 lat (y, x) float32 83.45833 83.375 83.291664 ... 41.125 41.041668 lon (y, x) float32 -140.95833 -140.95833 ... -52.04167 -52.04167  out_sel2 = ds.where(mask == ID_PROV) out_sel2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (time: 68, x: 510, y: 1068) Coordinates: * y (y) int64 0 1 2 3 4 5 6 ... 1061 1062 1063 1064 1065 1066 1067 * x (x) int64 0 1 2 3 4 5 6 7 8 ... 502 503 504 505 506 507 508 509 lon (y, x) float32 dask.array\u0026lt;shape=(1068, 510), chunksize=(1068, 510)\u0026gt; lat (y, x) float32 dask.array\u0026lt;shape=(1068, 510), chunksize=(1068, 510)\u0026gt; * time (time) float64 1.0 2.0 3.0 4.0 5.0 ... 64.0 65.0 66.0 67.0 68.0 Data variables: Mean_tasmoy (time, y, x) float32 dask.array\u0026lt;shape=(68, 1068, 510), chunksize=(10, 1068, 510)\u0026gt;  Quick visualisation, we will display the first step of our DataArray masked.\nFor out_sel2 array :\nplt.figure(figsize=(15,8)) ax = plt.axes() out_sel2.Mean_tasmoy.isel(time = 0).plot(ax = ax) province.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x18bc95f8\u0026gt;  out_sel2.to_netcdf(\u0026#39;./Ontario.nc\u0026#39;) import matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np import matplotlib as mpl from netCDF4 import Dataset, num2date import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import datetime import xarray as xr import pandas as pd filename=\u0026#39;./Ontario.nc\u0026#39; nc_fid=Dataset(filename,\u0026#39;r\u0026#39;) data=nc_fid.variables[\u0026#39;Mean_tasmoy\u0026#39;][:].squeeze() lons=nc_fid.variables[\u0026#39;lon\u0026#39;][:].squeeze() lats=nc_fid.variables[\u0026#39;lat\u0026#39;][:].squeeze() data.shape (68, 1068, 510)  Let's use Xarray to compute climatology over 1988-2017.\ndata_m = data[-30:,:,:] clim_89_2017=data_m.mean(axis=0) clim_89_2017.shape (1068, 510)  fig=plt.figure(figsize=(28,16), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-100,-75,40,58]) ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) mm = ax.contourf(lons,\\ lats,\\ clim_89_2017,\\ vmin=-10,\\ vmax=10, \\ transform=ccrs.PlateCarree(),\\ levels=np.arange(-10, 10, 1.),\\ cmap=plt.cm.jet ) ax.gridlines() # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) cbar.set_label(u\u0026#39;\\nProjection = Lambert Conformal Conic \\nResolution: 5 Arcs-Minutes (10 km)\\nData provided by Natural Resources Canada / Created by Guillaume Dueymes\u0026#39;, size=\u0026#39;medium\u0026#39;) # Affichage de la légende de la barre de couleur cbar = plt.colorbar(mm, shrink=0.75, drawedges=\u0026#39;True\u0026#39;,extend=\u0026#39;both\u0026#39;) cbar.ax.tick_params(labelsize=17) plt.xlabel(u\u0026#39;\\n\\n\\nTemperature / Température (°C)\u0026#39;,size=\u0026#39;x-large\u0026#39;) string_title=u\u0026#39;Climate normals of mean annual temperature (°C)\\n1988 - 2017\\n\u0026#39; plt.title(string_title, size=\u0026#39;xx-large\u0026#39;) plt.savefig(\u0026#39;./ANUSPLIN_Ontario_10km_YEAR_CLIM_1988-2017.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() ","date":1581700488,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581700488,"objectID":"7471577190056491409d8fc4776b2f2f","permalink":"/post/province_shapefiles/","publishdate":"2020-02-14T09:14:48-08:00","relpermalink":"/post/province_shapefiles/","section":"post","summary":"﻿\nFrom shapefiles to Netcdf Mask In this tutorial, we will use shapefiles to create masks over canadian provinces.\nWe will use gridded dataset ANUSPLIN meteorological data.\nAgriculture and Agri-Food Canada have produced daily precipitation, minimum and maximum temperature across Canada (south of 60°N) for climate related application purpose using thin-plate smoothing splines, as implemented in the ANUSPLIN climate modeling software (Hutchinson et al., 2009; McKenney et al., 2011).\nThe so-called ANUSPLIN data uses ground-based observations and generates daily gridded data from 1951 to 2017 on a Lambert conformal conic projection with 5’ arc minutes spacing (equivalent to a resolution of about 10 km).","tags":[],"title":"Province_shapefiles","type":"post"},{"authors":[],"categories":[],"content":"﻿\nScraping web data using Python In this tutorial, we will see how to get data from Environment Canada webpage.\nExploring the structure of the ECCC web page We will extract the current weather conditions and weather forecast for Montreal..\nhttps://meteo.gc.ca/city/pages/qc-147_metric_f.html\nimport requests import pandas as pd # we download the web page page = requests.get(\u0026#34;https://meteo.gc.ca/city/pages/qc-147_metric_e.html\u0026#34;) # Using BeautifulSoup to analyze html code from bs4 import BeautifulSoup # we will analyze the previous content with BeautifulSoup soup = BeautifulSoup(page.content,\u0026#39;html.parser\u0026#39;) soup.head() [\u0026lt;meta charset=\u0026quot;utf-8\u0026quot;/\u0026gt;, \u0026lt;meta content=\u0026quot;width=device-width, initial-scale=1\u0026quot; name=\u0026quot;viewport\u0026quot;/\u0026gt;, \u0026lt;meta content=\u0026quot;Environment Canada\u0026quot; name=\u0026quot;dcterms.creator\u0026quot;/\u0026gt;, \u0026lt;meta content=\u0026quot;eng\u0026quot; name=\u0026quot;dcterms.language\u0026quot; title=\u0026quot;ISO639-2\u0026quot;/\u0026gt;, \u0026lt;meta content=\u0026quot;Current conditions and forecasts including 7 day outlook, daily high/low temperature, warnings, chance of precipitation, pressure, humidity/wind chill (when applicable) historical data, normals, record values and sunrise/sunset times\u0026quot; name=\u0026quot;description\u0026quot;/\u0026gt;, \u0026lt;meta content=\u0026quot;meteorology;weather;weather warnings;weather forecasts\u0026quot; name=\u0026quot;dcterms.subject\u0026quot; title=\u0026quot;scheme\u0026quot;/\u0026gt;, \u0026lt;meta content=\u0026quot;2013-04-16\u0026quot; name=\u0026quot;dcterms.issued\u0026quot; title=\u0026quot;W3CDTF\u0026quot;/\u0026gt;, \u0026lt;meta content=\u0026quot;2020-02-07\u0026quot; name=\u0026quot;dcterms.modified\u0026quot; title=\u0026quot;W3CDTF\u0026quot;/\u0026gt;, \u0026lt;title\u0026gt;Montréal, QC - 7 Day Forecast - Environment Canada\u0026lt;/title\u0026gt;, \u0026lt;meta content=\u0026quot;Montréal, QC - 7 Day Forecast - Environment Canada\u0026quot; name=\u0026quot;dcterms.title\u0026quot;/\u0026gt;, \u0026lt;link href=\u0026quot;/template/gcweb/v5.0.1/assets/favicon.ico\u0026quot; rel=\u0026quot;icon\u0026quot; type=\u0026quot;image/x-icon\u0026quot;/\u0026gt;, \u0026lt;link href=\u0026quot;/template/gcweb/v5.0.1/css/theme.min.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt;, \u0026lt;link href=\u0026quot;/v175/css/city/jquery-ui-1.10.3.custom.min.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt;, \u0026lt;noscript\u0026gt;\u0026lt;link href=\u0026quot;/template/gcweb/v5.0.1/css/noscript.min.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt;\u0026lt;/noscript\u0026gt;, \u0026lt;link href=\u0026quot;/template/gcweb/v5.0.1/css/noscript.min.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt;, \u0026lt;link href=\u0026quot;/rss/city/qc-147_e.xml\u0026quot; rel=\u0026quot;alternate\u0026quot; title=\u0026quot;ATOM feed for Montréal - Weather - Environment Canada\u0026quot; type=\u0026quot;application/atom+xml\u0026quot;/\u0026gt;, \u0026lt;link href=\u0026quot;/rss/warning/qc-147_e.xml\u0026quot; rel=\u0026quot;alternate\u0026quot; title=\u0026quot;ATOM feed for Montréal - Warning - Environment Canada\u0026quot; type=\u0026quot;application/atom+xml\u0026quot;/\u0026gt;, \u0026lt;link href=\u0026quot;/v175/css/city/city.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt;, \u0026lt;link href=\u0026quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt;, \u0026lt;link href=\u0026quot;/v175/css/wxotemplate/wxo.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;\u0026gt; \u0026lt;link href=\u0026quot;/v175/css/wxotemplate/print-sm.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt; \u0026lt;!--[if lte IE 9]\u0026gt;\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;/v175/css/wxotemplate/wxo-ie8.css\u0026quot; /\u0026gt;\u0026lt;![endif]--\u0026gt; \u0026lt;/link\u0026gt;, \u0026lt;link href=\u0026quot;/v175/css/wxotemplate/print-sm.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot;/\u0026gt;]  soup.title \u0026lt;title\u0026gt;Montréal, QC - 7 Day Forecast - Environment Canada\u0026lt;/title\u0026gt;  1- Reading current conditions Column 1 conditions = soup.find(id=\u0026#34;mainContent\u0026#34;) col1 = conditions.find_all(class_=\u0026#34;dl-horizontal wxo-conds-col1\u0026#34;) today1 = col1[1] #print(today1.prettify()) tmp1 = today1.select(\u0026#34;.mrgn-bttm-0\u0026#34;) print(tmp1) [\u0026lt;dd class=\u0026quot;mrgn-bttm-0\u0026quot;\u0026gt;Light Snow and Blowing Snow\u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-metric-hide\u0026quot;\u0026gt;98.5 \u0026lt;abbr title=\u0026quot;kilopascals\u0026quot;\u0026gt;kPa\u0026lt;/abbr\u0026gt; \u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-imperial-hide wxo-city-hidden\u0026quot;\u0026gt;29.1 inches\u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0\u0026quot;\u0026gt;Falling\u0026lt;/dd\u0026gt;]  short_desc = [tmp1[0].get_text()] short_desc ['Light Snow and Blowing Snow']  pression = [tmp1[1].get_text().replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;)] trend =[ tmp1[3].get_text()] weather = pd.DataFrame({\u0026#34;short_desc\u0026#34;: short_desc, \u0026#34;pression\u0026#34;: pression, \u0026#34;trend\u0026#34;: trend, }) weather     short_desc pression trend     0 Light Snow and Blowing Snow 98.5 kPa Falling    Column 2 col2 = conditions.find_all(class_=\u0026#34;dl-horizontal wxo-conds-col2\u0026#34;) today2 = col2[1] #print(today2.prettify()) tmp2 = today2.select(\u0026#34;.mrgn-bttm-0\u0026#34;) print(tmp2) [\u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-metric-hide\u0026quot;\u0026gt;-6.7°\u0026lt;abbr title=\u0026quot;Celsius\u0026quot;\u0026gt;C\u0026lt;/abbr\u0026gt; \u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-imperial-hide wxo-city-hidden\u0026quot;\u0026gt;19.9° \u0026lt;abbr title=\u0026quot;Fahrenheit\u0026quot;\u0026gt;F\u0026lt;/abbr\u0026gt; \u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-metric-hide\u0026quot;\u0026gt;-8.8°\u0026lt;abbr title=\u0026quot;Celsius\u0026quot;\u0026gt;C\u0026lt;/abbr\u0026gt; \u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-imperial-hide wxo-city-hidden\u0026quot;\u0026gt;16.2°\u0026lt;abbr title=\u0026quot;Fahrenheit\u0026quot;\u0026gt;F\u0026lt;/abbr\u0026gt; \u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0\u0026quot;\u0026gt;85%\u0026lt;/dd\u0026gt;]  temperature = [tmp2[0].get_text().replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;)] rosee = [tmp2[2].get_text().replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;)] humidity = [tmp2[4].get_text().replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;)] weather = pd.DataFrame({\u0026#34;temperature\u0026#34;: temperature, \u0026#34;temperature_rosee\u0026#34;: rosee, \u0026#34;humidity\u0026#34;: humidity, }) weather     temperature temperature_rosee humidity     0 -6.7°C -8.8°C 85%    Column 3 col3 = conditions.find_all(class_=\u0026#34;dl-horizontal wxo-conds-col3\u0026#34;) today3 = col3[1] #print(today2.prettify()) tmp3 = today3.select(\u0026#34;.mrgn-bttm-0\u0026#34;) print(tmp3) [\u0026lt;dd class=\u0026quot;longContent mrgn-bttm-0 wxo-metric-hide\u0026quot;\u0026gt; \u0026lt;abbr title=\u0026quot;North\u0026quot;\u0026gt;N\u0026lt;/abbr\u0026gt; 35 \u0026lt;br class=\u0026quot;visible-xs\u0026quot;/\u0026gt;gust 53 \u0026lt;abbr title=\u0026quot;kilometres per hour\u0026quot;\u0026gt;km/h\u0026lt;/abbr\u0026gt; \u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;longContent mrgn-bttm-0 wxo-imperial-hide wxo-city-hidden\u0026quot;\u0026gt; \u0026lt;abbr title=\u0026quot;North\u0026quot;\u0026gt;N\u0026lt;/abbr\u0026gt; 22 \u0026lt;br class=\u0026quot;visible-xs\u0026quot;/\u0026gt;gust 33 \u0026lt;abbr title=\u0026quot;miles per hour\u0026quot;\u0026gt;mph\u0026lt;/abbr\u0026gt; \u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-metric-hide\u0026quot;\u0026gt;-16\u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-imperial-hide wxo-city-hidden\u0026quot;\u0026gt;3\u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-metric-hide\u0026quot;\u0026gt;0.8 \u0026lt;abbr title=\u0026quot;kilometres\u0026quot;\u0026gt;km\u0026lt;/abbr\u0026gt; \u0026lt;/dd\u0026gt;, \u0026lt;dd class=\u0026quot;mrgn-bttm-0 wxo-imperial-hide wxo-city-hidden\u0026quot;\u0026gt;0.5 mile \u0026lt;/dd\u0026gt;]  wind = [tmp3[0].get_text().replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;)] visibility = [tmp3[2].get_text().replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;)] weather = pd.DataFrame({\u0026#34;short_desc\u0026#34;: short_desc, \u0026#34;pression\u0026#34;: pression, \u0026#34;Trend\u0026#34;: trend, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;temperature_rosee\u0026#34;: rosee, \u0026#34;humidity\u0026#34;: humidity, \u0026#34;wind\u0026#34;: wind, \u0026#34;visibility\u0026#34;: visibility, }) weather     short_desc pression Trend temperature temperature_rosee humidity wind visibility     0 Light Snow and Blowing Snow 98.5 kPa Falling -6.7°C -8.8°C 85% N 35 gust 53 km/h -16    weather.to_csv(\u0026#34;current.csv\u0026#34;, header = True, sep = \u0026#39;,\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) 2- Reading forecasting values period_tags = soup.find(id=\u0026#34;mainContent\u0026#34;) row1 = period_tags.find_all(class_=\u0026#34;div-row div-row1 div-row-head\u0026#34;) row1 [\u0026lt;div class=\u0026quot;div-row div-row1 div-row-head\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;/forecast/hourly/qc-147_metric_e.html\u0026quot;\u0026gt;\u0026lt;strong title=\u0026quot;Friday\u0026quot;\u0026gt;Fri\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt;7 \u0026lt;abbr title=\u0026quot;February\u0026quot;\u0026gt;Feb\u0026lt;/abbr\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;, \u0026lt;div class=\u0026quot;div-row div-row1 div-row-head\u0026quot;\u0026gt; \u0026lt;strong title=\u0026quot;Saturday\u0026quot;\u0026gt;Sat\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt;8 \u0026lt;abbr title=\u0026quot;February\u0026quot;\u0026gt;Feb\u0026lt;/abbr\u0026gt; \u0026lt;/div\u0026gt;, \u0026lt;div class=\u0026quot;div-row div-row1 div-row-head\u0026quot;\u0026gt; \u0026lt;strong title=\u0026quot;Sunday\u0026quot;\u0026gt;Sun\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt;9 \u0026lt;abbr title=\u0026quot;February\u0026quot;\u0026gt;Feb\u0026lt;/abbr\u0026gt; \u0026lt;/div\u0026gt;, \u0026lt;div class=\u0026quot;div-row div-row1 div-row-head\u0026quot;\u0026gt; \u0026lt;strong title=\u0026quot;Monday\u0026quot;\u0026gt;Mon\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt;10 \u0026lt;abbr title=\u0026quot;February\u0026quot;\u0026gt;Feb\u0026lt;/abbr\u0026gt; \u0026lt;/div\u0026gt;, \u0026lt;div class=\u0026quot;div-row div-row1 div-row-head\u0026quot;\u0026gt; \u0026lt;strong title=\u0026quot;Tuesday\u0026quot;\u0026gt;Tue\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt;11 \u0026lt;abbr title=\u0026quot;February\u0026quot;\u0026gt;Feb\u0026lt;/abbr\u0026gt; \u0026lt;/div\u0026gt;, \u0026lt;div class=\u0026quot;div-row div-row1 div-row-head\u0026quot;\u0026gt; \u0026lt;strong title=\u0026quot;Wednesday\u0026quot;\u0026gt;Wed\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt;12 \u0026lt;abbr title=\u0026quot;February\u0026quot;\u0026gt;Feb\u0026lt;/abbr\u0026gt; \u0026lt;/div\u0026gt;, \u0026lt;div class=\u0026quot;div-row div-row1 div-row-head\u0026quot;\u0026gt; \u0026lt;strong title=\u0026quot;Thursday\u0026quot;\u0026gt;Thu\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt;13 \u0026lt;abbr title=\u0026quot;February\u0026quot;\u0026gt;Feb\u0026lt;/abbr\u0026gt; \u0026lt;/div\u0026gt;]  period_tags = soup.find(id=\u0026#34;mainContent\u0026#34;) row1 = period_tags.find_all(class_=\u0026#34;div-row div-row1 div-row-head\u0026#34;) periods = [pt.get_text().replace(\u0026#39;\\xa0\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) for pt in row1] periods ['Fri7 Feb', 'Sat8 Feb', 'Sun9 Feb', 'Mon10 Feb', 'Tue11 Feb', 'Wed12 Feb', 'Thu13 Feb']  period_tags = soup.find(id=\u0026#34;mainContent\u0026#34;) row2 = period_tags.find_all(class_=\u0026#34;div-row div-row2 div-row-data\u0026#34;) row2[1] \u0026lt;div class=\u0026quot;div-row div-row2 div-row-data\u0026quot;\u0026gt; \u0026lt;img alt=\u0026quot;A mix of sun and cloud\u0026quot; class=\u0026quot;center-block\u0026quot; height=\u0026quot;51\u0026quot; src=\u0026quot;/weathericons/02.gif\u0026quot; width=\u0026quot;60\u0026quot;/\u0026gt;\u0026lt;p class=\u0026quot;mrgn-bttm-0\u0026quot;\u0026gt;\u0026lt;span class=\u0026quot;high wxo-metric-hide\u0026quot; title=\u0026quot;max\u0026quot;\u0026gt;-13°\u0026lt;abbr title=\u0026quot;Celsius\u0026quot;\u0026gt;C\u0026lt;/abbr\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;high wxo-imperial-hide wxo-city-hidden\u0026quot; title=\u0026quot;max\u0026quot;\u0026gt;9°\u0026lt;abbr title=\u0026quot;Fahrenheit\u0026quot;\u0026gt;F\u0026lt;/abbr\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026quot;mrgn-bttm-0 pop text-center\u0026quot;\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p class=\u0026quot;mrgn-bttm-0\u0026quot;\u0026gt;A mix of sun and cloud\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt;  period_tags = soup.find(id=\u0026#34;mainContent\u0026#34;) row2 = period_tags.find_all(class_=\u0026#34;div-row div-row2 div-row-data\u0026#34;) temperature_f = [pt.get_text().split(\u0026#39;\\n\u0026#39;)[1][:] for pt in row2] temperature_f ['-5°C23°F', '-13°C9°F', '-6°C21°F', '-3°C27°F', '-2°C28°F', '-6°C21°F', '-6°C21°F']  temperature_2=[] for val in enumerate(temperature_f): temperature_2.append(val[1].split(\u0026#39;C\u0026#39;)[0] + \u0026#39;C\u0026#39;) temperature_2 ['-5°C', '-13°C', '-6°C', '-3°C', '-2°C', '-6°C', '-6°C']  conditions_f = [pt.get_text().split(\u0026#39;\\n\u0026#39;)[3] for pt in row2] conditions_f ['Snow at times heavy and blowing snow', 'A mix of sun and cloud', 'Sunny', 'Snow', 'Periods of snow', 'A mix of sun and cloud', 'Chance of flurries']  weather_f = pd.DataFrame({\u0026#34;Period\u0026#34;: periods, \u0026#34;temperature_f\u0026#34;: temperature_f, \u0026#34;conditions_f\u0026#34;:conditions_f, }) weather_f     Period temperature_f conditions_f     0 Fri7 Feb -5°C23°F Snow at times heavy and blowing snow   1 Sat8 Feb -13°C9°F A mix of sun and cloud   2 Sun9 Feb -6°C21°F Sunny   3 Mon10 Feb -3°C27°F Snow   4 Tue11 Feb -2°C28°F Periods of snow   5 Wed12 Feb -6°C21°F A mix of sun and cloud   6 Thu13 Feb -6°C21°F Chance of flurries    3- Download of weather icons for the next 7 days from urllib import request row3 = period_tags.find_all(class_=\u0026#34;div-row div-row2 div-row-data\u0026#34;) i = 0 for images in row3: s = images.find(\u0026#39;img\u0026#39;) f = open(str(i) + \u0026#39;.gif\u0026#39;, \u0026#39;wb\u0026#39;) f.write(request.urlopen(\u0026#34;https://meteo.gc.ca/weathericons/\u0026#34;+str(s)[str(s).find(\u0026#39;gif\u0026#39;)-3:(str(s).find(\u0026#39;gif\u0026#39;))-1]+\u0026#34;.gif\u0026#34;).read()) f.close() i += 1 weather_f.to_csv(\u0026#34;forecast.csv\u0026#34;, header = True, sep = \u0026#39;,\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) ","date":1581108920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581108920,"objectID":"3e70cabbc9fcb50213c3b77cbfb369b4","permalink":"/post/scrap_eccc/","publishdate":"2020-02-07T12:55:20-08:00","relpermalink":"/post/scrap_eccc/","section":"post","summary":"﻿\nScraping web data using Python In this tutorial, we will see how to get data from Environment Canada webpage.\nExploring the structure of the ECCC web page We will extract the current weather conditions and weather forecast for Montreal..\nhttps://meteo.gc.ca/city/pages/qc-147_metric_f.html\nimport requests import pandas as pd # we download the web page page = requests.get(\u0026#34;https://meteo.gc.ca/city/pages/qc-147_metric_e.html\u0026#34;) # Using BeautifulSoup to analyze html code from bs4 import BeautifulSoup # we will analyze the previous content with BeautifulSoup soup = BeautifulSoup(page.","tags":[],"title":"Scrap_ECCC","type":"post"},{"authors":[],"categories":[],"content":"Cartopy: Add WMS image from Environment Canada In this tutorial, we will use a Web Map Service and Cartopy python library to display products from Environment Canada\nA Web Map Service (WMS) defines an interface that allows a client to get maps of geospatial data and gain detailed information on specific features shown on the map. A \u0026ldquo;map\u0026rdquo; is defined here as a visual representation of geospatial data, not the geospatial data itself.\nEnvironnement Canada generates and archives moasic .png images every 5 minutes. These images are served as a Web Map Service. This notebook shows how to add those images to a cartopy figure axes.\nHere is a link with availables datasets: https://eccc-msc.github.io/open-data/msc-data/readme_fr/\nData Source: https://geo.weather.gc.ca/geomet?service=WMS\u0026amp;version=1.3.0\u0026amp;request=GetCapabilities\u0026amp;layer=RADAR_1KM_RRAI\nfrom datetime import datetime import cartopy.crs as ccrs import cartopy.feature as cfeature import matplotlib.pyplot as plt 1- plot RRAI 1 km for a specific date DATE = datetime(2020, 2, 7, 19, 0) fig = plt.figure(figsize=(14, 8)) ax = fig.add_subplot(1, 1, 1,projection=ccrs.LambertConformal(central_longitude=-100)) ax.add_feature(cfeature.COASTLINE.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.STATES.with_scale(\u0026#39;50m\u0026#39;)) # Date string format for data request strDATE = DATE.strftime(\u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) ax.set_title(\u0026#39;%s\u0026#39; % DATE.strftime(\u0026#39;%H:%M UTC %d%b %Y\u0026#39;), loc=\u0026#39;right\u0026#39;) ax.set_title( \u0026#39;RADAR_1km_RRAI\u0026#39;, loc=\u0026#39;left\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, fontsize=15) ax.add_wms(wms=\u0026#39;https://geo.weather.gc.ca/geomet?service=WMS\u0026amp;version=1.3.0\u0026amp;request=GetCapabilities\u0026amp;layer=RADAR_1KM_RRAI\u0026#39;, layers=\u0026#39;RADAR_1KM_RRAI\u0026#39; , wms_kwargs={\u0026#39;time\u0026#39;:strDATE, \u0026#39;transparent\u0026#39;:True}, zorder=10) ax.stock_img() ax.set_extent((-80, -60, 40, 52)) fig.subplots_adjust(wspace=0.02) 2- Plot the most recent time available Simply leave out the \u0026lsquo;time\u0026rsquo; argument in the wms_kwargs dict. The transparent option should default to True.\nThe timestamp of the image is retrieved from the JSON file at https://mesonet.agron.iastate.edu/data/gis/images/4326/USCOMP/n0q_0.json\n# Get the metadata for the most recent NEXRAD image from JSON file. import requests def get_timestamp(): f = requests.get(\u0026#39;https://mesonet.agron.iastate.edu/data/gis/images/4326/USCOMP/n0q_0.json\u0026#39;).json() validDATE = datetime.strptime(f[\u0026#39;meta\u0026#39;][\u0026#39;valid\u0026#39;], \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) return validDATE print(\u0026#39;Latest Image:\u0026#39;, get_timestamp()) print(\u0026#39;Current Time:\u0026#39;, datetime.utcnow()) diff = (datetime.utcnow()-get_timestamp()) print(\u0026#39;Difference (minutes):\u0026#39;, diff.seconds/60) Latest Image: 2020-02-07 19:15:00 Current Time: 2020-02-07 19:17:32.276824 Difference (minutes): 2.533333333333333  fig = plt.figure(figsize=(14, 8)) ax = fig.add_subplot(1, 1, 1, projection=ccrs.LambertConformal(central_longitude=-100)) ax.add_feature(cfeature.COASTLINE.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.STATES.with_scale(\u0026#39;50m\u0026#39;)) # Date string format for data reques ax.set_title(\u0026#39;Most Recent Time\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, loc=\u0026#39;left\u0026#39;) ax.set_title(\u0026#39;%s\u0026#39; % get_timestamp().strftime(\u0026#39;%H:%M UTC %d%b %Y\u0026#39;), loc=\u0026#39;right\u0026#39;) ax.add_wms(wms=\u0026#39;https://geo.weather.gc.ca/geomet?service=WMS\u0026amp;version=1.3.0\u0026amp;request=GetCapabilities\u0026amp;layer=RADAR_1KM_RRAI\u0026#39;, layers=\u0026#39;RADAR_1KM_RRAI\u0026#39;, wms_kwargs={\u0026#39;transparent\u0026#39;:True}, zorder=10) # ^ For current time (most recent time in last 5 minutes), leave the \u0026#39;time\u0026#39;  # wms_kwargs unassigned. Transparetn should be True by default, but it # doesn\u0026#39;t hurt to be explicit. ax.set_extent((-80, -60, 40, 52)) fig.subplots_adjust(wspace=0.02) 3 fig = plt.figure(figsize=(15, 5)) ax = fig.add_subplot(1, 2, 1, projection=ccrs.LambertConformal(central_longitude=-100)) ax.add_feature(cfeature.COASTLINE.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.STATES.with_scale(\u0026#39;50m\u0026#39;)) # Date string format for data request strDATE = DATE.strftime(\u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) ax.set_title(\u0026#39;Most Recent Time\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, loc=\u0026#39;left\u0026#39;) ax.set_title(\u0026#39;%s\u0026#39; % get_timestamp().strftime(\u0026#39;%H:%M UTC %d%b %Y\u0026#39;), loc=\u0026#39;right\u0026#39;) ax.add_wms(wms=\u0026#39;https://geo.weather.gc.ca/geomet?service=WMS\u0026amp;version=1.3.0\u0026amp;request=GetCapabilities\u0026amp;layer=RADAR_1KM_RRAI\u0026#39;, layers=\u0026#39;RADAR_1KM_RRAI\u0026#39;, wms_kwargs={\u0026#39;transparent\u0026#39;:True}, zorder=10) ax.set_extent((-80, -60, 40, 52)) # ^ If you add a background image or use tiles you neet to set transparent # as True and set the zorder as a higher number, i.e. 10. ax.stock_img() ax.set_extent([-120, -75, 23, 50]) ax = fig.add_subplot(1, 2, 2,projection=ccrs.LambertConformal(central_longitude=-100)) ax.add_feature(cfeature.COASTLINE.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.STATES.with_scale(\u0026#39;50m\u0026#39;)) # Date string format for data reques # https://mesonet.agron.iastate.edu/docs/nexrad_mosaic/ ax.set_title(\u0026#39;Most Recent Time\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, loc=\u0026#39;left\u0026#39;) ax.set_title(\u0026#39;%s\u0026#39; % get_timestamp().strftime(\u0026#39;%H:%M UTC %d%b %Y\u0026#39;), loc=\u0026#39;right\u0026#39;) ax.add_wms(wms=\u0026#39;https://geo.weather.gc.ca/geomet?service=WMS\u0026amp;version=1.3.0\u0026amp;request=GetCapabilities\u0026amp;layer=RADAR_1KM_RRAI\u0026#39;, layers=\u0026#39;RADAR_1KM_RRAI\u0026#39;, wms_kwargs={\u0026#39;transparent\u0026#39;:True}, zorder=10) ax.set_extent((-80, -60, 40, 52)) fig.subplots_adjust(wspace=0.02) An other example using NEXRAD mosaic image from Iowa Environmental Mesonet.\nhttps://mesonet.agron.iastate.edu/docs/nexrad_composites/\nSee the data source URL for dBZ colormaps. You can generate an approximate colormap with MetPy's NWSReflectivity and NWSReflectivityExpanded ctables.\n N0Q: Base Reflectivity - 8 bit/0.5 dbz resolution  Colorbars See the data source website for the real colormaps.\nApproximate colorbars can be made with metpy. The NWSReflectivity colormap is nearly exact for N0R, but NWSReflectivityExpanded is very different for N0Q. When I need it, I probably will make a custom colormap sometime in the future.\nfrom metpy.plots import colortables import numpy as np ctable = \u0026#39;NWSReflectivityExpanded\u0026#39; norm, cmap = colortables.get_with_steps(ctable, 5, 2.5) fig = plt.figure(figsize=(14, 8)) ax = fig.add_subplot(1, 1, 1,projection=ccrs.LambertConformal(central_longitude=-100)) ax.add_feature(cfeature.COASTLINE.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.STATES.with_scale(\u0026#39;50m\u0026#39;)) # Date string format for data reques # https://mesonet.agron.iastate.edu/docs/nexrad_mosaic/ ax.set_title(\u0026#39;Most Recent Time\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, loc=\u0026#39;left\u0026#39;) ax.set_title(\u0026#39;%s\u0026#39; % get_timestamp().strftime(\u0026#39;%H:%M UTC %d%b %Y\u0026#39;), loc=\u0026#39;right\u0026#39;) ax.add_wms(wms=\u0026#39;https://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0q-t.cgi?\u0026#39;, layers=\u0026#39;nexrad-n0q-wmst\u0026#39;, wms_kwargs={\u0026#39;transparent\u0026#39;:True}, zorder=10) # ^ For current time (most recent time in last 5 minutes), leave the \u0026#39;time\u0026#39;  # wms_kwargs unassigned. Transparetn should be True by default, but it # doesn\u0026#39;t hurt to be explicit. norm, cmap = colortables.get_with_steps(ctable, 5, 2.5) fig = plt.figure(figsize=(5,.1)) # Make a placeholder mesh that the colorbar will be made with, # then remove the axis and the image im = plt.pcolormesh(np.ones([1,1]),norm=norm, cmap=cmap) plt.axis(\u0026#39;off\u0026#39;) im.remove() # Add the colorbar cbar_ax = fig.add_axes([0, .5, 1, 3]) cbar = fig.colorbar(im, cax=cbar_ax, orientation=\u0026#39;horizontal\u0026#39;, label=\u0026#39;N0Q: \u0026#39; + ctable) ax.set_extent((-80, -60, 40, 52)) fig.subplots_adjust(wspace=0.02) We can combine these two differents products:\nfig = plt.figure(figsize=(15, 5)) ax = fig.add_subplot(1, 2, 1,projection=ccrs.LambertConformal(central_longitude=-100)) ax.add_feature(cfeature.COASTLINE.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.STATES.with_scale(\u0026#39;50m\u0026#39;)) # Date string format for data request strDATE = DATE.strftime(\u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) ax.set_title(\u0026#39;RADAR_1km_RRAI\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, loc=\u0026#39;left\u0026#39;) ax.set_title(\u0026#39;%s\u0026#39; % get_timestamp().strftime(\u0026#39;%H:%M UTC %d%b %Y\u0026#39;), loc=\u0026#39;right\u0026#39;) ax.add_wms(wms=\u0026#39;https://geo.weather.gc.ca/geomet?service=WMS\u0026amp;version=1.3.0\u0026amp;request=GetCapabilities\u0026amp;layer=RADAR_1KM_RRAI\u0026#39;, layers=\u0026#39;RADAR_1KM_RRAI\u0026#39;, wms_kwargs={\u0026#39;transparent\u0026#39;:True}, zorder=10) # ^ If you add a background image or use tiles you neet to set transparent # as True and set the zorder as a higher number, i.e. 10. ax.set_extent([-120, -75, 23, 50]) ax = fig.add_subplot(1, 2, 2,projection=ccrs.LambertConformal(central_longitude=-100)) ax.add_feature(cfeature.COASTLINE.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.STATES.with_scale(\u0026#39;50m\u0026#39;)) # Date string format for data reques # https://mesonet.agron.iastate.edu/docs/nexrad_mosaic/ ax.set_title(\u0026#39;NOQ\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, loc=\u0026#39;left\u0026#39;) ax.set_title(\u0026#39;%s\u0026#39; % get_timestamp().strftime(\u0026#39;%H:%M UTC %d%b %Y\u0026#39;), loc=\u0026#39;right\u0026#39;) ax.add_wms(wms=\u0026#39;https://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0q-t.cgi?\u0026#39;, layers=\u0026#39;nexrad-n0q-wmst\u0026#39;, wms_kwargs={\u0026#39;transparent\u0026#39;:True}, zorder=10) ax.set_extent((-120, -75, 23, 50)) fig.subplots_adjust(wspace=0.02) Exemple 2 : GDPS fig = plt.figure(figsize=(14, 8)) ax = fig.add_subplot(1, 1, 1, projection=ccrs.LambertConformal(central_longitude=-100)) # Date string format for data reques ax.set_title(\u0026#39;Most Recent Time\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, loc=\u0026#39;left\u0026#39;) ax.set_title(\u0026#39;%s\u0026#39; % get_timestamp().strftime(\u0026#39;%H:%M UTC %d%b %Y\u0026#39;), loc=\u0026#39;right\u0026#39;) ax.add_wms(wms=\u0026#39;https://geo.weather.gc.ca/geomet?service=WMS\u0026amp;version=1.3.0\u0026amp;request=GetCapabilities\u0026amp;layer=GDPS.ETA_TT\u0026#39;, layers=\u0026#39;GDPS.ETA_TT\u0026#39;, wms_kwargs={\u0026#39;transparent\u0026#39;:True}, zorder=10) ax.add_wms(wms=\u0026#39;https://geo.weather.gc.ca/geomet?service=WMS\u0026amp;version=1.3.0\u0026amp;request=GetCapabilities\u0026amp;layer=GDPS.ETA_UU\u0026#39;, layers=\u0026#39;GDPS.ETA_UU\u0026#39;, wms_kwargs={\u0026#39;transparent\u0026#39;:True}, zorder=10) ax.add_feature(cfeature.COASTLINE.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) ax.add_feature(cfeature.STATES.with_scale(\u0026#39;50m\u0026#39;)) ax.set_extent((-120, -75, 23, 50)) ","date":1581108121,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581108121,"objectID":"f8c26280bacc059b6325e5d7013b3849","permalink":"/post/cartopy_wms/","publishdate":"2020-02-07T12:42:01-08:00","relpermalink":"/post/cartopy_wms/","section":"post","summary":"Cartopy: Add WMS image from Environment Canada In this tutorial, we will use a Web Map Service and Cartopy python library to display products from Environment Canada\nA Web Map Service (WMS) defines an interface that allows a client to get maps of geospatial data and gain detailed information on specific features shown on the map. A \u0026ldquo;map\u0026rdquo; is defined here as a visual representation of geospatial data, not the geospatial data itself.","tags":[],"title":"Cartopy_WMS","type":"post"},{"authors":[],"categories":[],"content":"Working with Netcd4-python data: Synoptic map In this tutorial, we will use the features of the Python xarray library to process and analyze Netcdf files. We will then use matplotlib to plot it, and cartopy to map our analyse.\nWe will see the different steps to plot an exemple of synoptic map over North America with: + geopotential 850hpa + Temperature 850hpa + UU,VV 250hpa\nWe fist import the necessary packages. The plt.rcParams[\u0026lsquo;figure.figsize\u0026rsquo;] just sets the size of the inline figures in this notebook to make them larger and easier to read.\nimport xarray as xr import cartopy.crs as ccrs import cartopy.feature as cfeature import matplotlib.pyplot as plt import matplotlib as mpl import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (20, 20) We will work with fields from NARR reanalysis at pressure levels.\nData can be directly download on this link: ftp://ftp.cdc.noaa.gov/Datasets/NARR/pressure/\ntas = xr.open_dataset(\u0026#39;J:/REANALYSES/NARR/3hrs/NARR_air_lc_2018_01_3hrs.nc\u0026#39;) uu = xr.open_dataset(\u0026#39;J:/REANALYSES/NARR/3hrs/NARR_uwnd_lc_2018_01_3hrs.nc\u0026#39;) vv = xr.open_dataset(\u0026#39;J:/REANALYSES/NARR/3hrs/NARR_vwnd_lc_2018_01_3hrs.nc\u0026#39;) omega = xr.open_dataset(\u0026#39;J:/REANALYSES/NARR/Daily/omega/NARR_omega_lc_2018_01_d.nc\u0026#39;) hgt = xr.open_dataset(\u0026#39;J:/REANALYSES/NARR/3hrs/NARR_hgt_lc_2018_01_3hrs.nc\u0026#39;) - Exploring the data We can quickly explore our datasets by using some methods of the xarray library:\n- DS.var - DS.dims = DS.coords - DS.attrs  hgt \u0026lt;xarray.Dataset\u0026gt; Dimensions: (level: 29, time: 248, x: 349, y: 277) Coordinates: * time (time) datetime64[ns] 2018-01-01 ... 2018-01-31T21:00:00 * level (level) float32 1000.0 975.0 950.0 ... 150.0 125.0 100.0 lat (y, x) float32 ... lon (y, x) float32 ... * y (y) float32 0.0 32463.0 64926.0 ... 8927325.0 8959788.0 * x (x) float32 0.0 32463.0 64926.0 ... 11264660.0 11297120.0 Data variables: Lambert_Conformal int32 ... hgt (time, level, y, x) float32 ... Attributes: Conventions: CF-1.2 centerlat: 50.0 centerlon: -107.0 comments: institution: National Centers for Environmental Prediction latcorners: [ 1.000001 0.897945 46.3544 46.63433 ] loncorners: [-145.5 -68.32005 -2.569891 148.6418 ] platform: Model standardpar1: 50.0 standardpar2: 50.000001 title: 8x Daily NARR history: created Mon Jul 18 17:37:00 MDT 2016 by NOAA/ESRL/PSD dataset_title: NCEP North American Regional Reanalysis (NARR) references: https://www.esrl.noaa.gov/psd/data/gridded/data.narr.html source: http://www.emc.ncep.noaa.gov/mmb/rreanl/index.html  hgt.coords Coordinates: * time (time) datetime64[ns] 2018-01-01 ... 2018-01-31T21:00:00 * level (level) float32 1000.0 975.0 950.0 925.0 ... 150.0 125.0 100.0 lat (y, x) float32 ... lon (y, x) float32 ... * y (y) float32 0.0 32463.0 64926.0 ... 8894862.0 8927325.0 8959788.0 * x (x) float32 0.0 32463.0 64926.0 ... 11264660.0 11297120.0  We can quickly explore our datasets by using some methods of the xarray library.\nWe want to analyse a specific date : 13/01/2018 at 12UTC.\ntas.time \u0026lt;xarray.DataArray 'time' (time: 248)\u0026gt; array(['2018-01-01T00:00:00.000000000', '2018-01-01T03:00:00.000000000', '2018-01-01T06:00:00.000000000', ..., '2018-01-31T15:00:00.000000000', '2018-01-31T18:00:00.000000000', '2018-01-31T21:00:00.000000000'], dtype='datetime64[ns]') Coordinates: * time (time) datetime64[ns] 2018-01-01 ... 2018-01-31T21:00:00 Attributes: axis: T coordinate_defines: point delta_t: 0000-00-00 03:00:00 long_name: Time standard_name: time actual_range: [1910952. 1911693.]  #To select a specifi date:  date = \u0026#39;2018-01-13T12\u0026#39; tas.sel(time=date).sel(level=500).isel(x=slice(60,340), y=slice(50,230)) \u0026lt;xarray.Dataset\u0026gt; Dimensions: (x: 280, y: 180) Coordinates: time datetime64[ns] 2018-01-13T12:00:00 level float32 500.0 lat (y, x) float32 ... lon (y, x) float32 ... * y (y) float32 1623150.0 1655613.0 ... 7401564.0 7434027.0 * x (x) float32 1947780.0 1980243.0 ... 10972490.0 11004960.0 Data variables: Lambert_Conformal int32 ... air (y, x) float32 ... Attributes: Conventions: CF-1.2 centerlat: 50.0 centerlon: -107.0 comments: institution: National Centers for Environmental Prediction latcorners: [ 1.000001 0.897945 46.3544 46.63433 ] loncorners: [-145.5 -68.32005 -2.569891 148.6418 ] platform: Model standardpar1: 50.0 standardpar2: 50.000001 title: 8x Daily NARR history: created Mon Jul 18 17:31:43 MDT 2016 by NOAA/ESRL/PSD dataset_title: NCEP North American Regional Reanalysis (NARR) references: https://www.esrl.noaa.gov/psd/data/gridded/data.narr.html source: http://www.emc.ncep.noaa.gov/mmb/rreanl/index.html  hgt850 = hgt[\u0026#39;hgt\u0026#39;].sel(time=date).sel(level=850).isel(x=slice(60,340), y=slice(50,230)) tt850 = tas[\u0026#39;air\u0026#39;].sel(time=date).sel(level=850).isel(x=slice(60,340), y=slice(50,230)) urel250 = uu[\u0026#39;uwnd\u0026#39;].sel(time=date).sel(level=250).isel(x=slice(60,340), y=slice(50,230)) vrel250 = vv[\u0026#39;vwnd\u0026#39;].sel(time=date).sel(level=250).isel(x=slice(60,340), y=slice(50,230)) def plotMap(): #Set the projection information proj = ccrs.LambertConformal(central_longitude=-97.0,central_latitude=53, standard_parallels=[53]) #Create a figure with an axes object on which we will plot. Pass the projection to that axes. fig, ax = plt.subplots(subplot_kw=dict(projection=proj)) #Zoom in #ax.set_extent([-140,-60,10,70]) #Add map features ax.add_feature(cfeature.LAND, facecolor=\u0026#39;0.9\u0026#39;) #Grayscale colors can be set using 0 (black) to 1 (white) ax.add_feature(cfeature.LAKES, alpha=0.9) #Alpha sets transparency (0 is transparent, 1 is solid) ax.add_feature(cfeature.BORDERS, zorder=10) ax.add_feature(cfeature.COASTLINE, zorder=10) #We can use additional features from Natural Earth (http://www.naturalearthdata.com/features/) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;50m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;, zorder=10) #Add lat/lon gridlines every 20° to the map ax.gridlines(xlocs=np.arange(0,361,20), ylocs=np.arange(-80,90,20)) return fig, ax We specify the interval for our height contours. We use numpy's function arange to select a range of contours from 500 to 2000 m, every 60 m.\nA key thing to note is the transform argument at the end of the contour call. This is what tells matplotlib to transform our variable in lat/lon coordinates onto our map projection coordinates.\nWe then plot temperature in filled contours. Matplotlib has many colormaps from which to choose. Here, we use cm.jet_r.\n#Import scipy.ndimage to get the gaussian_filter function import scipy.ndimage as ndimage hght_levels = np.arange(500,2000,60) #Plot a new figure and map axes fig, ax = plotMap() ## Choisissons une colormap cmap0=plt.cm.jet_r cmap0.set_under(\u0026#39;w\u0026#39;) ## on met en blanc les valeurs inferieures au min de clev cmap0.set_over(\u0026#39;darkblue\u0026#39;) #Smooth and re-plot the temperature field tt_smooth = ndimage.gaussian_filter(tt850.values-273.15, sigma=1.5, order=0) tt_contour = ax.contourf(tt850.lon.values, tt850.lat.values, tt_smooth, zorder=2, cmap=cmap0, transform = ccrs.PlateCarree()) #Smooth and re-plot the height field hght_smooth = ndimage.gaussian_filter(hgt850, sigma=3, order=0) hght_contour = ax.contour(hgt850.lon, hgt850.lat, hght_smooth, levels=hght_levels, linewidths=1, colors=\u0026#39;k\u0026#39;,transform = ccrs.PlateCarree()) #Plot contour labels for the heights, leaving a break in the contours for the text (inline=True) plt.clabel(hght_contour, hght_levels, inline=True, fmt=\u0026#39;%1i\u0026#39;, fontsize=12) \u0026lt;a list of 18 text.Text objects\u0026gt;  We can improve our synoptic map with adding wind barbs in Knots and using our own colormap.\n#Import scipy.ndimage to get the gaussian_filter function import scipy.ndimage as ndimage hght_levels = np.arange(500,2000,60) tt_levels = np.arange(-40,40,2) #Plot a new figure and map axes fig, ax = plotMap() ## Choisissons une colormap Y=np.array([[77,0,111],[115,14,181],[160,17,222],[195,14,240],\\ [0,0,93],[21,38,177],[33,95,227],[32,162,247],[59,224,248],[202,255,250],\\ [4,255,179],[37,181,139],[32,132,81],[72,162,60],[157,240,96],[213,255,166],\\ [241,247,132],[248,185,68],[255,124,4],[235,78,14],[215,32,24],[189,24,40],[162,16,56],[135,16,65],[107,15,73]])/255. colbar=mpl.colors.ListedColormap(Y) cmap0=plt.cm.jet_r cmap0.set_under(\u0026#39;w\u0026#39;) ## on met en blanc les valeurs inferieures au min de clev cmap0.set_over(\u0026#39;darkblue\u0026#39;) #Smooth and re-plot the temperature field tt_smooth = ndimage.gaussian_filter(tt850.values-273.15, sigma=1.5, order=0) tt_contour = ax.contourf(tt850.lon.values, tt850.lat.values, tt_smooth, levels=tt_levels, zorder=2, cmap=colbar, transform = ccrs.PlateCarree()) #Smooth and re-plot the height field hght_smooth = ndimage.gaussian_filter(hgt850, sigma=3, order=0) hght_contour = ax.contour(hgt850.lon, hgt850.lat, hght_smooth, levels=hght_levels, linewidths=1, colors=\u0026#39;red\u0026#39;,transform = ccrs.PlateCarree()) #Plot contour labels for the heights, leaving a break in the contours for the text (inline=True) plt.clabel(hght_contour, hght_levels, inline=True, fmt=\u0026#39;%1i\u0026#39;, fontsize=20) #Plot the barbs ax.barbs(urel250.lon.values, urel250.lat.values, urel250.values*1.944, vrel250.values*1.944, regrid_shape=12, zorder=20, transform=ccrs.PlateCarree()) #Create a colorbar and shrink it down a bit. cb = plt.colorbar(tt_contour, shrink=0.5, ticks=np.arange(-40, 40.1, 4)) #Set the title ax.set_title(\u0026#39;850-hPa Heights, 850-hPa Temperature, Wind (kts) \\n\u0026#39;+str(date), fontsize=14) Text(0.5, 1.0, '850-hPa Heights, 850-hPa Temperature, Wind (kts) \\n 2018-01-13T12')  ","date":1581103860,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581103860,"objectID":"78832065b50af1afeeb35bd7923d9cd9","permalink":"/post/narr_synoptic/","publishdate":"2020-02-07T11:31:00-08:00","relpermalink":"/post/narr_synoptic/","section":"post","summary":"Working with Netcd4-python data: Synoptic map In this tutorial, we will use the features of the Python xarray library to process and analyze Netcdf files. We will then use matplotlib to plot it, and cartopy to map our analyse.\nWe will see the different steps to plot an exemple of synoptic map over North America with: + geopotential 850hpa + Temperature 850hpa + UU,VV 250hpa\nWe fist import the necessary packages.","tags":[],"title":"NARR_Synoptic","type":"post"},{"authors":[],"categories":[],"content":"Connexion to Microsoft SQL Server Management Studio This is a short tutorial to connect python to a Miscrosoft SQL Server.\nSteps to connect with pyodbc python library You need to know some informations on your local server, database and tables you want to connect: In our case: - The Server Name is: DESKTOP-BE6M578 - The Database Name is: BDSOPFEU - The Table Name is: dbo.Meteo_stations - The Table dbo.Meteo_stations containes stations information\n1 Retrieve your server name You can get your server name by opening Microsoft SQL Server. You’ll then see the Connect to Server box, where the server name will be displayed.\nIn my case, the server name is: DESKTOP-BE6M578\n2 Retrieve your Database name Next, you’ll need to obtain the database name in which your desired table is stored.\nYou can find the database name under the Object Explorer menu (underneath the Databases section) which is located on the left-hand side of your SQL Server.\nIn our example, the database name is: BDSOPFEU\n3 Get the table name Now you’ll need to get the name of your desired table.\nThe name of your table would also be located under the Object Explorer menu (underneath the Tables section).\nHere, the name of the table is: dbo.Meteo_stations\n4 Connect Python to our Microsoft SQL Server Now we have all the informations about our server and database to connect Python .\nHere is the structure of the code that you may use in Python:\nimport pyodbc conn = pyodbc.connect(\u0026lsquo;Driver={SQL Server};\u0026rsquo; \u0026lsquo;Server=server_name;\u0026rsquo; \u0026lsquo;Database=db_name;\u0026rsquo; \u0026lsquo;Trusted_Connection=yes;')\ncursor = conn.cursor() cursor.execute(\u0026lsquo;SELECT * FROM db_name.Table\u0026rsquo;)\nfor row in cursor: print(row) And this is how the code would look like in Python using our example:\nimport pyodbc connexion = pyodbc.connect(\u0026#39;Driver={SQL Server};\u0026#39; \u0026#39;Server=DESKTOP-BE6M578;\u0026#39; \u0026#39;Database=BDSOPFEU;\u0026#39; \u0026#39;Trusted_Connection=yes;\u0026#39;) cursor = connexion.cursor() cursor.execute(\u0026#39;SELECT * FROM BDSOPFEU.dbo.Meteo_stations\u0026#39;) # to loop over each row #for row in cursor: # print(row) row = cursor.fetchone() print(row) connexion.close() (111, datetime.datetime(1978, 4, 1, 0, 0), 1, 'STE-PERPETUE', '4113-1-5355', 111, datetime.datetime(1989, 10, 31, 0, 0), 400, 'M', 'N', 1, 1, 'MFFP_CEL', None, 13, ' ', None, False, None, None, None)   For having basic informations about our table:  columns = [column[0] for column in cursor.description] columns ['No_station', 'Ouverture', 'Base', 'Nom', 'Quadr', 'Secteur', 'Fermeture', 'Elevation', 'T_hr', 'Vent', 'T_donnees', 'T_station', 'Provenance', 'Stn_mere', 'Hre_levee', 'Memo', 'Contour', 'Active', 'Cladonie', 'Latitude', 'Longitude']  Now, I’ll show you how to get data from SQL to pandas DataFrame.\nimport pyodbc import pandas as pd connexion = pyodbc.connect(\u0026#39;Driver={SQL Server};\u0026#39; \u0026#39;Server=DESKTOP-BE6M578;\u0026#39; \u0026#39;Database=BDSOPFEU;\u0026#39; \u0026#39;Trusted_Connection=yes;\u0026#39;) SQL_Query = pd.read_sql_query( \u0026#39;\u0026#39;\u0026#39;SELECTNom, No_station, Latitude, Longitude FROM dbo.Meteo_stations WHERE Active = \u0026#39;True\u0026#39;\u0026#39;\u0026#39;\u0026#39;, connexion) df = pd.DataFrame(SQL_Query) df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  import pyodbc import pandas as pd connexion = pyodbc.connect(\u0026#39;Driver={SQL Server};\u0026#39; \u0026#39;Server=DESKTOP-BE6M578;\u0026#39; \u0026#39;Database=BDSOPFEU;\u0026#39; \u0026#39;Trusted_Connection=yes;\u0026#39;) SQL_Query = pd.read_sql_query( \u0026#39;\u0026#39;\u0026#39;SELECTDate,Heure,Pluie,Hr,Tsec,Thum,Vv,VvrFROM dbo.Meteo_obs WHERE No_station = 111\u0026#39;\u0026#39;\u0026#39;, connexion) df = pd.DataFrame(SQL_Query) df.tail() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  ","date":1581101857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581101857,"objectID":"748ea3dec686da485b1d1c7c738be15b","permalink":"/post/mss_server/","publishdate":"2020-02-07T10:57:37-08:00","relpermalink":"/post/mss_server/","section":"post","summary":"Connexion to Microsoft SQL Server Management Studio This is a short tutorial to connect python to a Miscrosoft SQL Server.\nSteps to connect with pyodbc python library You need to know some informations on your local server, database and tables you want to connect: In our case: - The Server Name is: DESKTOP-BE6M578 - The Database Name is: BDSOPFEU - The Table Name is: dbo.Meteo_stations - The Table dbo.Meteo_stations containes stations information","tags":[],"title":"MSS_Server","type":"post"},{"authors":[],"categories":[],"content":"From country shapefiles to Netcdf Mask In this tutorial, we will use shapefiles to create mask over specific countries.\nIn order to work with the whole globe, we will use gridded dataset ERA5 meteorological data.\nI will present a simple solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - regionmask: to mask a gridded file according to a shapefile - numpy: for simple array manipulations - geopandas: to open shapefiles - matplotlib: for plotting  1- Import librairies and create aliases. import xarray as xr import numpy as np import regionmask import geopandas as gpd import pandas as pd import matplotlib.pyplot as plt import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) %matplotlib inline Working over countries In this example we will focus on countries in Africa. We will use shapefiles developped in ESRI format from this link:\nhttps://community.esri.com/external-link.jspa?url=http%3A%2F%2Fwww.maplibrary.org%2Flibrary%2Fstacks%2FAfrica%2Findex.htm\nAfter downloaded the shapefile, we must load it using geopandas library:\nPATH_TO_SHAPEFILE = \u0026#39;./Countries/Africa_Countries.shp\u0026#39; countries = gpd.read_file(PATH_TO_SHAPEFILE) countries.head()     ID CODE COUNTRY geometry     0 1 ALG Algeria POLYGON ((-5.7636199999979 25.58624999999302, \u0026hellip;   1 2 ANG Angola POLYGON ((13.36632442474365 -8.32172966003418,\u0026hellip;    countries.shape[0] 762  my_list = list(countries[\u0026#39;CODE\u0026#39;]) my_list_unique = set(list(countries[\u0026#39;CODE\u0026#39;])) indexes = [my_list.index(x) for x in my_list_unique] Shapes are here a GeoDataFrame containing all polygons illustrating the countries boundaries.\nNow we can load the ERA5 gridded data. The parameter chunks is very important, it defines how big are the “pieces” of data moved from the disk to the memory. With this value the entire computation on a workstation with 32 GB takes a couple of minutes.\nWe will load all the temperature files for the year 2018 using Xarray library.\nmodel=\u0026#39;ERA5_T2m_1h\u0026#39; t_in = \u0026#39;J:/REANALYSES/ERA5/T2m_1h/\u0026#39; data = t_in + model + \u0026#39;_2018*_sfc.nc\u0026#39; ds = xr.open_mfdataset(data, chunks = {\u0026#39;time\u0026#39;: 10}) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 0.0 0.25 0.5 0.75 ... 359.25 359.5 359.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  The next function assign_coordswill convert the longitude from the 0-360 range to -180,180\nds = ds.assign_coords(longitude=(((ds.longitude + 180) % 360) - 180)).sortby(\u0026#39;longitude\u0026#39;) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  len(list(countries.geometry.values[i] for i in indexes)) len(list(countries.COUNTRY[indexes])) len(indexes) 52  a = range(0,countries.shape[0]) np.shape(a) (762,)  Our xarray Dataset contains a single variable (t2m) which is stored as a dask.array. This is the result of loading files with open_mfdataset.\nNow we will use regionmask module to create a gridded mask with the function regions_cls documented here: https://regionmask.readthedocs.io/en/stable/generated/regionmask.Regions_cls.html#regionmask.Regions_cls\nWith this function we will create an object able to mask ERA5 gridded data.\ncountries_mask_poly = regionmask.Regions_cls(name = \u0026#39;COUNTRY\u0026#39;, numbers = indexes, names = countries.COUNTRY[indexes], abbrevs = countries.COUNTRY[indexes], outlines = list(countries.geometry.values[i] for i in range(0,countries.shape[0]))) countries_mask_poly 52 'COUNTRY' Regions () Burkina Faso Senegal Botswana Liberia Chad Equatorial Guinea Djibouti Ghana Nigeria Sao Tome and Principe Swaziland Uganda Tanzania Comoros Guinea Algeria Niger Madagascar Burundi Cameroon Mali Zimbabwe Cote d`Ivoire Tunisia Sierra Leone Libya Rwanda Benin Malawi Gabon South Africa Western Sahara Zambia Central African Republic Togo Namibia Gambia Congo-Brazzaville Democratic Republic of Congo Morocco Eritrea Cape Verde Angola Ethiopia Lesotho Egypt Guinea-Bissau Kenya Mozambique Sudan Mauritania Somalia  Now we are ready to apply the mask on the gridded dataset xarray ERA5.\nWe select only the first timestep to speed up the process.\nThis step could take few minutes because of ERA5 resolution and grid : Dimensions: (latitude: 721, longitude: 1440)\nmask = countries_mask_poly.mask(ds.isel(time = 0), lat_name=\u0026#39;latitude\u0026#39;, lon_name=\u0026#39;longitude\u0026#39;) mask \u0026lt;xarray.DataArray 'region' (latitude: 721, longitude: 1440)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75  Mask can be saved (for example as a NetCDF) for a later use.\nmask.to_netcdf(\u0026#39;./mask_Africa_by_countries.nc\u0026#39;) We can use Panoply free software to plot our netcdf file.\nhttps://www.giss.nasa.gov/tools/panoply/\nHere's a quick visualisation using Matplotlib:\nplt.figure(figsize=(16,8)) ax = plt.axes() mask.plot(ax = ax) countries.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;, lw = 1) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xbdf0d68\u0026gt;  Let's now extract one specific country from our mask. We will for example extract informations only over Algeria. Remember, index for Algeria is: 0 .\nmask_algeria = mask.where(mask == 0 ) mask_algeria.to_netcdf(\u0026#39;./mask_Algeria.nc\u0026#39;) plt.figure(figsize=(16,8)) ax = plt.axes() mask_algeria.plot(ax = ax) countries.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;, lw = 1) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7528278\u0026gt;  2- Extract time-series We can now for each country aggregate the grid cells in the national borders. We will first develop two methods to mask our area. Then, we will extract the time series.\n- Method 1: I will focus on Algeria.\nID_COUNTRY = 0 print(countries.COUNTRY[ID_COUNTRY]) Algeria  As first step, I will save the latitude and longitude vectors because I will use it later. Then, I select the mask points where the value is equal to target value (the ID_COUNTRY code). In the numpy array sel_mask all the values are nan except for the selected ones.\nlat = mask.latitude.values lon = mask.longitude.values sel_mask = mask.where(mask == ID_COUNTRY).values sel_mask array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]])  To speed-up the process I want to crop the xarray Dataset selecting the smallest box containing the entire mask. To do this I store in id_lon and id_lat the coordinate points where the mask has at least a non-nan value.\nid_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))] id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))] id_lat array([37. , 36.75, 36.5 , 36.25, 36. , 35.75, 35.5 , 35.25, 35. , 34.75, 34.5 , 34.25, 34. , 33.75, 33.5 , 33.25, 33. , 32.75, 32.5 , 32.25, 32. , 31.75, 31.5 , 31.25, 31. , 30.75, 30.5 , 30.25, 30. , 29.75, 29.5 , 29.25, 29. , 28.75, 28.5 , 28.25, 28. , 27.75, 27.5 , 27.25, 27. , 26.75, 26.5 , 26.25, 26. , 25.75, 25.5 , 25.25, 25. , 24.75, 24.5 , 24.25, 24. , 23.75, 23.5 , 23.25, 23. , 22.75, 22.5 , 22.25, 22. , 21.75, 21.5 , 21.25, 21. , 20.75, 20.5 , 20.25, 20. , 19.75, 19.5 , 19.25, 19. ], dtype=float32)  The Xarray dataset is reduced selecting only the target year and the coordinates containing the target region. Then the dataset is load from the dask array using compute and then filtered using the mask.\nout_sel1 = ds.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == ID_COUNTRY) out_sel1 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 73, longitude: 82, time: 8760) Coordinates: * latitude (latitude) float64 37.0 36.75 36.5 36.25 ... 19.5 19.25 19.0 * longitude (longitude) float64 -8.5 -8.25 -8.0 -7.75 ... 11.25 11.5 11.75 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  - Method 2: We can directly use xarray library to apply netcdf mask with using .where() method and DataArray mask:\nout_sel2 = ds.where(mask == 0) out_sel2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  Quick visualisation, we will display the first step of our DataArray masked.\nFor out_sel2 array :\nplt.figure(figsize=(12,8)) ax = plt.axes() out_sel2.t2m.isel(time = 0).plot(ax = ax) countries.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xe73e550\u0026gt;  For out_sel1 array :\nplt.figure(figsize=(12,8)) ax = plt.axes() out_sel1.t2m.isel(time = 0).plot(ax = ax) countries.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xe8024a8\u0026gt;  Finally we can aggregate by the arithmetic mean using the groupby function to obtain a time-series of national average temperatures.\nx = out_sel1.groupby(\u0026#39;time\u0026#39;).mean() x \u0026lt;xarray.Dataset\u0026gt; Dimensions: (time: 8760) Coordinates: * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time) float32 285.01346 284.65833 283.95526 ... 282.03668 281.461  Then we plot the time-series…\nx.t2m.plot() [\u0026lt;matplotlib.lines.Line2D at 0xe9dc518\u0026gt;]   Let's resample our dataset by day and then compute a daily mean.  x = out_sel1.resample(time = \u0026#39;1D\u0026#39;).mean()-273.15 x \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 73, longitude: 82, time: 365) Coordinates: * longitude (longitude) float64 -8.5 -8.25 -8.0 -7.75 ... 11.25 11.5 11.75 * latitude (latitude) float64 37.0 36.75 36.5 36.25 ... 19.5 19.25 19.0 * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan  daily_mean = x.t2m.mean(dim=(\u0026#39;longitude\u0026#39;,\u0026#39;latitude\u0026#39;)) daily_mean \u0026lt;xarray.DataArray 't2m' (time: 365)\u0026gt; array([14.550091, 13.812102, 13.700798, ..., 10.975014, 11.182918, 10.956429], dtype=float32) Coordinates: * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31  daily_mean.plot() [\u0026lt;matplotlib.lines.Line2D at 0xe243f98\u0026gt;]  And we save it as a csv\ndaily_mean.to_pandas().to_csv(\u0026#39;average-temperature-algeria.csv\u0026#39;, header = [\u0026#39;t2m\u0026#39;]) 2- Extract time-series for one specific localisation In this example, we eant to extract time-series for Alger:\nWith: - longitude = 3.04 - latitude = 36.75\nlati = 36.75 loni = 3.04 data = out_sel1.sel(longitude=loni , latitude=lati , method=\u0026#39;nearest\u0026#39;) data.t2m \u0026lt;xarray.DataArray 't2m' (time: 8760)\u0026gt; array([289.91342, 289.63348, 289.2745 , ..., 285.62613, 284.9182 , 284.80624], dtype=float32) Coordinates: latitude float64 36.75 longitude float64 3.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Attributes: units: K long_name: 2 metre temperature  data[\u0026#39;t2m\u0026#39;] = data[\u0026#39;t2m\u0026#39;] - 273.15 df = data.t2m.to_dataframe() fig = plt.figure(figsize=(16,8)) df[\u0026#39;t2m\u0026#39;].plot() \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x20773fd0\u0026gt;  Let's plot montlhy temperature distribution for Alger:\ndf[\u0026#39;month\u0026#39;] = df.index.strftime(\u0026#34;%b\u0026#34;) df.head()    time latitude longitude t2m month     2018-01-01 00:00:00 36.75 3 16.7634 Jan   2018-01-01 01:00:00 36.75 3 16.4835 Jan   2018-01-01 02:00:00 36.75 3 16.1245 Jan   2018-01-01 03:00:00 36.75 3 16.197 Jan   2018-01-01 04:00:00 36.75 3 16.0389 Jan    ax = plt.axes() sns.boxplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;t2m\u0026#34;, data=df, palette=\u0026#34;Set1\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show() ","date":1580841589,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580841589,"objectID":"1992a6a058bf3b12af01d19be757ed10","permalink":"/post/shapefiles_country/","publishdate":"2020-02-04T10:39:49-08:00","relpermalink":"/post/shapefiles_country/","section":"post","summary":"From country shapefiles to Netcdf Mask In this tutorial, we will use shapefiles to create mask over specific countries.\nIn order to work with the whole globe, we will use gridded dataset ERA5 meteorological data.\nI will present a simple solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - regionmask: to mask a gridded file according to a shapefile - numpy: for simple array manipulations - geopandas: to open shapefiles - matplotlib: for plotting  1- Import librairies and create aliases.","tags":[],"title":"Shapefiles_Country","type":"post"},{"authors":[],"categories":[],"content":"﻿\nFrom shapefiles to Netcdf Mask Many times we need to create Netcdf mask files over continents or maybe countries. In this tutorial, we will use shapefiles to create those masks.\nIn order to work with the whole globe, we will use gridded dataset ERA5 meteorological data.\nI will present a simple solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - regionmask: to mask a gridded file according to a shapefile - numpy: for simple array manipulations - geopandas: to open shapefiles - matplotlib: for plotting  1- First we need to import librairies and create aliases. import xarray as xr import numpy as np import regionmask import geopandas as gpd import pandas as pd import matplotlib.pyplot as plt import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) %matplotlib inline Working over continents In this example we will use the classification developed by ARCGIS.\nShapefiles are available on this website: https://www.arcgis.com/home/item.html?id=5cf4f223c4a642eb9aa7ae1216a04372\nAfter downloaded the shapefile, we must load it using geopandas library:\nPATH_TO_SHAPEFILE = \u0026#39;./continent_shapefile/continent.shp\u0026#39; continents = gpd.read_file(PATH_TO_SHAPEFILE) continents 0 Asia 1 North America 2 Europe 3 Africa 4 South America 5 Oceania 6 Australia 7 Antarctica Name: CONTINENT, dtype: GeoDataFrame\nShapes are here a GeoDataFrame containing all polygons illustrating the continent boundaries.\nNow we can load the ERA5 gridded data. The parameter chunks is very important, it defines how big are the “pieces” of data moved from the disk to the memory. With this value the entire computation on a workstation with 32 GB takes a couple of minutes.\nWe will load all the temperature files for the year 2018 using Xarray library.\nmodel=\u0026#39;ERA5_T2m_1h\u0026#39; t_in = \u0026#39;J:/REANALYSES/ERA5/T2m_1h/\u0026#39; data = t_in + model + \u0026#39;_2018*_sfc.nc\u0026#39; ds = xr.open_mfdataset(data, chunks = {\u0026#39;time\u0026#39;: 10}) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 0.0 0.25 0.5 0.75 ... 359.25 359.5 359.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  The next function assign_coordswill convert the longitude from the 0-360 range to -180,180\nds = ds.assign_coords(longitude=(((ds.longitude + 180) % 360) - 180)).sortby(\u0026#39;longitude\u0026#39;) ds \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  Our xarray Dataset contains a single variable (t2m) which is stored as a dask.array. This is the result of loading files with open_mfdataset.\nNow we will use regionmask module to create a gridded mask with the function regions_cls documented here: https://regionmask.readthedocs.io/en/stable/generated/regionmask.Regions_cls.html#regionmask.Regions_cls\nWith this function we will create an object able to mask ERA5 gridded data.\ncontinents_mask_poly = regionmask.Regions_cls(name = \u0026#39;CONTINENT\u0026#39;, numbers = list(range(0,8)), names = list(continents.CONTINENT), abbrevs = list(continents.CONTINENT), outlines = list(continents.geometry.values[i] for i in range(0,8))) continents_mask_poly 8 'CONTINENT' Regions () Asia North America Europe Africa South America Oceania Australia Antarctica  Now we are ready to apply the mask on the gridded dataset xarray ERA5.\nWe select only the first timestep to speed up the process.\nThis step could take few minutes because of ERA5 resolution and grid : Dimensions: (latitude: 721, longitude: 1440)\nmask = continents_mask_poly.mask(ds.isel(time = 0), lat_name=\u0026#39;latitude\u0026#39;, lon_name=\u0026#39;longitude\u0026#39;) mask \u0026lt;xarray.DataArray 'region' (latitude: 721, longitude: 1440)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, 7., 7., ..., 7., 7., 7.], [nan, 7., 7., ..., 7., 7., 7.], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75  Mask can be saved (for example as a NetCDF) for a later use.\nmask.to_netcdf(\u0026#39;./mask_all_continents.nc\u0026#39;) A quick visualisation:\nplt.figure(figsize=(15,8)) ax = plt.axes() mask.plot(ax = ax) continents.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;, lw = 1) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x787df98\u0026gt;  2- Extract time-series We can now for each contient aggregate the grid cells.\nWe will first develop two methods to mask our area. Then, we will extract the time series.\n- Method 1: We will do this work over Africa.\nID_CONTINENT = 3 print(continents.CONTINENT[ID_CONTINENT]) Africa  As first step, I will save the latitude and longitude vectors because I will use it later. Then, I select the mask points where the value is equal to target value (the ID_CONTINENT code define before).\nIn the numpy array sel_mask all the values are nan except for the selected ones.\nlat = mask.latitude.values lon = mask.longitude.values sel_mask = mask.where(mask == ID_CONTINENT).values sel_mask array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]])  To speed-up the process I want to crop the xarray Dataset selecting the smallest box containing the entire mask. To do this I store in id_lon and id_lat the coordinate points where the mask has at least a non-nan value.\nid_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))] id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))] id_lat array([ 37.25, 37. , 36.75, 36.5 , 36.25, 36. , 35.75, 35.5 , 35.25, 35. , 34.75, 34.5 , 34.25, 34. , 33.75, 33.5 , 33.25, 33. , 32.75, 32.5 , 32.25, 32. , 31.75, 31.5 , 31.25, 31. , 30.75, 30.5 , 30.25, 30. , 29.75, 29.5 , 29.25, 29. , 28.75, 28.5 , 28.25, 28. , 27.75, 27.5 , 27.25, 27. , 26.75, 26.5 , 26.25, 26. , 25.75, 25.5 , 25.25, 25. , 24.75, 24.5 , 24.25, 24. , 23.75, 23.5 , 23.25, 23. , 22.75, 22.5 , 22.25, 22. , 21.75, 21.5 , 21.25, 21. , 20.75, 20.5 , 20.25, 20. , 19.75, 19.5 , 19.25, 19. , 18.75, 18.5 , 18.25, 18. , 17.75, 17.5 , 17.25, 17. , 16.75, 16.5 , 16.25, 16. , 15.75, 15.5 , 15.25, 15. , 14.75, 14.5 , 14.25, 14. , 13.75, 13.5 , 13.25, 13. , 12.75, 12.5 , 12.25, 12. , 11.75, 11.5 , 11.25, 11. , 10.75, 10.5 , 10.25, 10. , 9.75, 9.5 , 9.25, 9. , 8.75, 8.5 , 8.25, 8. , 7.75, 7.5 , 7.25, 7. , 6.75, 6.5 , 6.25, 6. , 5.75, 5.5 , 5.25, 5. , 4.75, 4.5 , 4.25, 4. , 3.75, 3.5 , 3.25, 3. , 2.75, 2.5 , 2.25, 2. , 1.75, 1.5 , 1.25, 1. , 0.75, 0.5 , 0.25, 0. , -0.25, -0.5 , -0.75, -1. , -1.25, -1.5 , -1.75, -2. , -2.25, -2.5 , -2.75, -3. , -3.25, -3.5 , -3.75, -4. , -4.25, -4.5 , -4.75, -5. , -5.25, -5.5 , -5.75, -6. , -6.25, -6.5 , -6.75, -7. , -7.25, -7.5 , -7.75, -8. , -8.25, -8.5 , -8.75, -9. , -9.25, -9.5 , -9.75, -10. , -10.25, -10.5 , -10.75, -11. , -11.25, -11.5 , -11.75, -12. , -12.25, -12.5 , -12.75, -13. , -13.25, -13.5 , -13.75, -14. , -14.25, -14.5 , -14.75, -15. , -15.25, -15.5 , -15.75, -16. , -16.25, -16.5 , -16.75, -17. , -17.25, -17.5 , -17.75, -18. , -18.25, -18.5 , -18.75, -19. , -19.25, -19.5 , -19.75, -20. , -20.25, -20.5 , -20.75, -21. , -21.25, -21.5 , -21.75, -22. , -22.25, -22.5 , -22.75, -23. , -23.25, -23.5 , -23.75, -24. , -24.25, -24.5 , -24.75, -25. , -25.25, -25.5 , -25.75, -26. , -26.25, -26.5 , -26.75, -27. , -27.25, -27.5 , -27.75, -28. , -28.25, -28.5 , -28.75, -29. , -29.25, -29.5 , -29.75, -30. , -30.25, -30.5 , -30.75, -31. , -31.25, -31.5 , -31.75, -32. , -32.25, -32.5 , -32.75, -33. , -33.25, -33.5 , -33.75, -34. , -34.25, -34.5 , -34.75], dtype=float32)  The Xarray dataset is reduced selecting only the target year and the coordinates containing the target region. Then the dataset is load from the dask array using compute and then filtered using the mask.\nout_sel1 = ds.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == ID_CONTINENT) out_sel1 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 289, longitude: 333, time: 8760) Coordinates: * latitude (latitude) float64 37.25 37.0 36.75 36.5 ... -34.25 -34.5 -34.75 * longitude (longitude) float64 -25.25 -25.0 -24.75 ... 57.25 57.5 57.75 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  - Method 2: We can directly use xarray library to apply netcdf mask with using .where() method and DataArray mask:\nmask \u0026lt;xarray.DataArray 'region' (latitude: 721, longitude: 1440)\u0026gt; array([[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, 7., 7., ..., 7., 7., 7.], [nan, 7., 7., ..., 7., 7., 7.], [nan, nan, nan, ..., nan, nan, nan]]) Coordinates: * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75  out_sel2 = ds.where(mask == 3) out_sel1 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 289, longitude: 333, time: 8760) Coordinates: * latitude (latitude) float64 37.25 37.0 36.75 36.5 ... -34.25 -34.5 -34.75 * longitude (longitude) float64 -25.25 -25.0 -24.75 ... 57.25 57.5 57.75 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  out_sel2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 8760) Coordinates: * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * time (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(8760, 721, 1440), chunksize=(10, 721, 1440)\u0026gt; Attributes: Conventions: CF-1.6 history: 2019-11-11 13:33:07 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...  Quick visualisation, we will display the first step of our DataArray masked.\nFor out_sel1 array :\nplt.figure(figsize=(12,8)) ax = plt.axes() out_sel1.t2m.isel(time = 0).plot(ax = ax) continents.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xba03908\u0026gt;  For out_sel2 array :\nplt.figure(figsize=(15,8)) ax = plt.axes() out_sel2.t2m.isel(time = 0).plot(ax = ax) continents.plot(ax = ax, alpha = 0.8, facecolor = \u0026#39;none\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x759f9e8\u0026gt;  ! Finally we can resample our dataset by day and then compute a daily mean.  With out_sel1:\ndaily_mean = out_sel1.resample(time = \u0026#39;1D\u0026#39;).mean()-273.15 daily_mean \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 289, longitude: 333, time: 365) Coordinates: * latitude (latitude) float64 37.25 37.0 36.75 36.5 ... -34.25 -34.5 -34.75 * longitude (longitude) float64 -25.25 -25.0 -24.75 ... 57.25 57.5 57.75 * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31 Data variables: t2m (time, latitude, longitude) float32 nan nan nan ... nan nan nan  To compute spatial mean over Africa:\ndaily_mean = daily_mean.t2m.mean(dim=(\u0026#39;longitude\u0026#39;,\u0026#39;latitude\u0026#39;)) daily_mean \u0026lt;xarray.DataArray 't2m' (time: 365)\u0026gt; array([20.868923, 20.559137, 20.618475, ..., 20.510223, 20.604036, 20.943094], dtype=float32) Coordinates: * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31  Then we plot the time-series…\ndaily_mean.plot() [\u0026lt;matplotlib.lines.Line2D at 0xc2d4cf8\u0026gt;]  And we save it as a csv.\ndaily_mean.to_pandas().to_csv(\u0026#39;average-daily-temperature.csv\u0026#39;, header = [\u0026#39;t2m\u0026#39;]) Just to see if we find same results with masking method 2. We will plot the same time serie with out_sel2 Xarray:\ndaily_mean2 = out_sel2.resample(time = \u0026#39;1D\u0026#39;).mean()-273.15 daily_mean2 \u0026lt;xarray.Dataset\u0026gt; Dimensions: (latitude: 721, longitude: 1440, time: 365) Coordinates: * latitude (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0 * longitude (longitude) float32 -180.0 -179.75 -179.5 ... 179.25 179.5 179.75 * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31 Data variables: t2m (time, latitude, longitude) float32 dask.array\u0026lt;shape=(365, 721, 1440), chunksize=(1, 721, 1440)\u0026gt;  daily_mean2 = daily_mean2.t2m.mean(dim=(\u0026#39;longitude\u0026#39;,\u0026#39;latitude\u0026#39;)) daily_mean2 \u0026lt;xarray.DataArray 't2m' (time: 365)\u0026gt; dask.array\u0026lt;shape=(365,), dtype=float32, chunksize=(1,)\u0026gt; Coordinates: * time (time) datetime64[ns] 2018-01-01 2018-01-02 ... 2018-12-31  daily_mean2.plot() [\u0026lt;matplotlib.lines.Line2D at 0xc2d1f28\u0026gt;]  ","date":1580484028,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580484028,"objectID":"9f2f3f0d0b9b48b7115ba3816a5d3628","permalink":"/post/shapefile_netcdf/","publishdate":"2020-01-31T07:20:28-08:00","relpermalink":"/post/shapefile_netcdf/","section":"post","summary":"﻿\nFrom shapefiles to Netcdf Mask Many times we need to create Netcdf mask files over continents or maybe countries. In this tutorial, we will use shapefiles to create those masks.\nIn order to work with the whole globe, we will use gridded dataset ERA5 meteorological data.\nI will present a simple solution based on open-source Python modules:\n - xarray: for manipulating \u0026amp; reading gridded data, and – very important – operate out-of-memory thanks to its dask capabilities - regionmask: to mask a gridded file according to a shapefile - numpy: for simple array manipulations - geopandas: to open shapefiles - matplotlib: for plotting  1- First we need to import librairies and create aliases.","tags":[],"title":"Shapefile_netcdf","type":"post"},{"authors":[],"categories":[],"content":"Plot of a radial temperature chart In this post, we will plot a radial temperature chart using daily temperature from Environment Canada.\nThe objective of this product is to obtain an analysis of the daily evolution of the temperature and its positioning in relation to the norm and the historical records of the Julian day.\nIn this work, we will extract data from Environment and Climate Change Canada for the RIGAUD station in Quebec. Here is the information about this station:\nName: RIGAUD Username: 5252 Latitude: 45.5 degN Longitude: -74.37 degW Period covered: 1963 - today  A list of stations is available on this site: ftp://ftp.tor.ec.gc.ca/Pub/Get_More_Data_More_data/Repository%20of%20stations%20FR.csv\nThe data will be uploaded directly to Environment and Climate Change Canada's website http://climate.weather.gc.ca/\nThe data is in XML format. XML or eXtensible Markup Language is a generic markup language. To read this format, we will call the \u0026lsquo;xml.etree.ElementTree\u0026rsquo; library: https://docs.python.org/2/library/xml.etree.elementtree.html#module-xml.etree.ElementTree\nFirst, we import the necessary libraries:\n- matplotlib: module to plot our graph - datetime: python module for manipulating dates - wget: module to extract data on a url - pandas: module for working with data structures - os: \u0026quot;system\u0026quot; module to create, delete ... files from our environment  import matplotlib.pyplot as plt import numpy as np import matplotlib.ticker as ticker from datetime import date import os import wget import pandas as pd import xml.etree.ElementTree as ET import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) For example, we will extract the year 2000 daily records of the station RIGAUD (ID: 5252).\nid_stat = 5252 year = 2000 tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=2\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() stationsdata = root.findall(\u0026#39;.//stationdata\u0026#39;) -1 / unknown  Notre fichier XML a la structure suivant:\nOur XML file got this structure:\nWe need to target the maxtemp and mintemp tags for our work, so the stationdata[0] and station[1] fields.\nresultmax = [] resultmin = [] for stationdata in stationsdata: champs1=stationdata.find(\u0026#39;maxtemp\u0026#39;) resultmax.append(champs1.text) # we append data for each day in a year  data_max=np.array(resultmax,\u0026#34;float\u0026#34;) for stationdata in stationsdata: champs2=stationdata.find(\u0026#39;mintemp\u0026#39;) resultmin.append(champs2.text) # we append data for each day in a year  data_min=np.array(resultmin,\u0026#34;float\u0026#34;) We then have two python lists with daily minimum and maximum temperatures.\nprint(resultmax[0:5]) ['-1.0', '5.5', '2.0', '4.0', '-3.0']  We can do the same over the whole recording period of the station (1963-2019) by applying a for loop over the years.\nFor each year and each variable to be extracted we will increment a list in the variables min_array = [] and max_array = [].\nyi = 1963 yf = 2019 id_stat = 5252 station = \u0026#39;RIGAUD\u0026#39; max_array = [] min_array = [] for year in range(yi,yf+1): ### loop over yars  tmp_file = wget.download(\u0026#39;http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=xml\u0026amp;stationID=\u0026#39;+str(int(id_stat))+\u0026#39;\u0026amp;Year=\u0026#39;+str(year)+\u0026#39;\u0026amp;Month=01\u0026amp;Day=14\u0026amp;timeframe=2\u0026#39;) tree=ET.parse(tmp_file) os.remove(tmp_file) root=tree.getroot() stationsdata = root.findall(\u0026#39;.//stationdata\u0026#39;) resultmax = [] resultmin = [] for stationdata in stationsdata: champs1=stationdata.find(\u0026#39;maxtemp\u0026#39;) resultmax.append(champs1.text) # loop over days for tmax  data_max=np.array(resultmax,\u0026#34;float\u0026#34;) for stationdata in stationsdata: champs2=stationdata.find(\u0026#39;mintemp\u0026#39;) resultmin.append(champs2.text) # loop over days for tmin  data_min=np.array(resultmin,\u0026#34;float\u0026#34;) max_array.append(data_max) min_array.append(data_min) len(max_array) -1 / unknown 57  For each min_array and max_array fields, we get a list of lists. We will flatten thoses lists:\ndef flatten(input): new_list = [] for i in input: for j in i: new_list.append(j) return new_list min_array=flatten(min_array) max_array=flatten(max_array) len(min_array) 20819  So we now have two lists of 20454 days for each min_array and max_array fields.\nKnowing that the period extends from January 1, 1963 to December 31, 2018, we can add a temporal dimension to our dataframe with the datetime module of python.\nstart = date(1963, 1, 1) end = date(2019, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) tmin_dataset = pd.Series(min_array, index=rng) tmax_dataset = pd.Series(max_array, index=rng) tmax_dataset.head() 1963-01-01 -11.1 1963-01-02 -6.7 1963-01-03 -3.9 1963-01-04 -2.2 1963-01-05 -3.9 Freq: D, dtype: float64  One of the big advantages of the pandas dataframe is that we can very easily work with time series. Here we will calculate the daily climatologies of the tmin_dataset and tmax_dataset variables over the 1981-2010 normal period.\nAlso, with the pandas groupby tools, we can calculate the daily minimum and maximum of tmin_dataset and tmax_dataset over the full period of the recording.\nmintmin = [] maxtmax = [] climtmin = [] climtmax = [] clim_min_dataset=tmin_dataset[\u0026#39;1981\u0026#39;:\u0026#39;2010\u0026#39;] clim_max_dataset=tmax_dataset[\u0026#39;1981\u0026#39;:\u0026#39;2010\u0026#39;] climtmin = clim_min_dataset.groupby([clim_min_dataset.index.month, clim_min_dataset.index.day]).mean() climtmax = clim_max_dataset.groupby([clim_max_dataset.index.month, clim_max_dataset.index.day]).mean() mintmin = tmin_dataset.groupby([tmin_dataset.index.month, tmin_dataset.index.day]).min() maxtmax = tmax_dataset.groupby([tmax_dataset.index.month, tmax_dataset.index.day]).max() data_min_tmin=np.array(mintmin,\u0026#34;float\u0026#34;) data_max_tmax=np.array(maxtmax,\u0026#34;float\u0026#34;) data_clim_tmax=np.array(climtmax,\u0026#34;float\u0026#34;) data_clim_tmin=np.array(climtmin,\u0026#34;float\u0026#34;) print(len(data_clim_tmax)) print(len(maxtmax)) 366 366  We can know visualize our timeries with a radial chart.\nx = [] ndays=366 Azs=np.arange(0,ndays) angle = Azs * 2.0 * np.pi / ndays fig=plt.figure(figsize=(12,12)) ax = fig.add_subplot(111, polar=True) ax.plot([angle[0],angle[0]], [data_min[0],data_max[0]],\u0026#39;red\u0026#39;, alpha=1.0, linewidth=3.0, label=str(yf)) ax.plot([angle[0],angle[0]], [data_clim_tmin[0],data_clim_tmax[0]],\u0026#39;blue\u0026#39;, alpha=0.3, linewidth=3.0, label=\u0026#39;Climatology (1981-2010)\u0026#39;) ax.plot([angle[0],angle[0]], [data_min_tmin[0],data_max_tmax[0]],\u0026#39;grey\u0026#39;, alpha=0.4, linewidth=3.0, label=\u0026#39;Extreme (\u0026#39;+str(yi)+\u0026#39;-\u0026#39;+str(yf)+\u0026#39;)\u0026#39;) leg=plt.legend(bbox_to_anchor=(0.15, 1.0),fontsize=10) leg.get_frame().set_linewidth(0.0) leg.set_title(str(station)+\u0026#39;\u0026#39;+str(yf), prop={\u0026#39;size\u0026#39;: 10, \u0026#39;weight\u0026#39;: \u0026#39;heavy\u0026#39;}) ax.plot([angle,angle], [data_min,data_max],\u0026#39;red\u0026#39;, alpha=1.0, linewidth=3.0) ax.plot([angle,angle], [data_clim_tmin,data_clim_tmax],\u0026#39;blue\u0026#39;, alpha=0.3, linewidth=3.0) ax.plot([angle,angle], [data_min_tmin,data_max_tmax],\u0026#39;grey\u0026#39;, alpha=0.4, linewidth=3.0) ax.set_rmin(-40) ax.set_rmax(+40) ax.grid(True) ax.set_theta_direction(-1) ax.set_theta_offset(np.pi / 2) ax.set_rticks([-30,-20, -10, 0, 10, 20, 30]) # less radial ticks ax.set_rlabel_position(-45.) # get radial labels away from plotted line ax.set_yticklabels([\u0026#39;$-30^{\\circ}$\u0026#39;, \u0026#39;$-20^{\\circ}$\u0026#39;, \u0026#39;$-10^{\\circ}$\u0026#39;, \u0026#39;$0^{\\circ}$\u0026#39;, \u0026#39;$10^{\\circ}$\u0026#39;, \u0026#39;$20^{\\circ}$\u0026#39;, \u0026#39;$30^{\\circ}$\u0026#39; ], fontsize=10) # Set the major and minor tick locations ax.xaxis.set_major_locator(ticker.MultipleLocator(np.pi/6)) ax.xaxis.set_minor_locator(ticker.MultipleLocator(np.pi/12)) ax.grid(linewidth=1,color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;) # Turn off major tick labels ax.xaxis.set_major_formatter(ticker.NullFormatter()) ax.set_frame_on(False) # Set the minor tick width to 0 so you don\u0026#39;t see them for tick in ax.xaxis.get_minor_ticks(): tick.tick1line.set_markersize(0) tick.tick2line.set_markersize(0) tick.label1.set_horizontalalignment(\u0026#39;center\u0026#39;) # Set the names of your ticks, with blank spaces for the major ticks ax.set_xticklabels([\u0026#39;\u0026#39;,\u0026#39;Jan\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Feb\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Mar\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Apr\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;May\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Jun\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Jul\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Aug\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Sep\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Oct\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Nov\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;Dec\u0026#39;],minor=True) plt.show() ","date":1575691203,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575691203,"objectID":"8fc9d803e13f0f530f5c49b99bb31c78","permalink":"/post/temperature_radial_chart/","publishdate":"2019-12-06T20:00:03-08:00","relpermalink":"/post/temperature_radial_chart/","section":"post","summary":"Plot of a radial temperature chart In this post, we will plot a radial temperature chart using daily temperature from Environment Canada.\nThe objective of this product is to obtain an analysis of the daily evolution of the temperature and its positioning in relation to the norm and the historical records of the Julian day.\nIn this work, we will extract data from Environment and Climate Change Canada for the RIGAUD station in Quebec.","tags":[],"title":"Temperature_Radial_Chart","type":"post"},{"authors":[],"categories":[],"content":"Extract daily temperature from Environment Canada using Python The objective of this product is to retrieve daily temperature data from the second generation homogenized dataset of Environment and Climate Change Canada developed by Vincent et al. 2012.\nAdjusted and homogenized Canadian climate dataset (DCCAH) were prepared to provide a better spatial and temporal representation of the climate trends in Canada.\nIn the Second Generation of Homogenized Temperature, new adjustments were applied to the daily minimum temperatures at synoptic stations (mainly airports) to address the bias due to the change in observing time in July 1961 (Vincent et al. 2009).\nDaily homogenized temperatures (minimum, maximum and mean) can be dowloaded on this link: ftp://ccrp.tor.ec.gc.ca/pub/EC_data/AHCCD_daily/Raw dataset can be downloaded here: http://climate.weather.gc.ca/historical_data/search_historic_data_f.html\nIn this post, we will work on a specific province in Canada (using filters). To do this, we will use Temperature_Stations.xls available on ftp site. This file provide us a list of all stations available.\nWe first need to import our librairies:\nimport pandas as pd import os from datetime import date import calendar import numpy as np import pathlib import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) from itertools import islice We will work with daily minimum temperature data only for the Northwest Territories of Canada.\nReferring to the document Temperature_Stations.xls, we see that the acronym for this province is: NWT.\ndataframe = pd.read_excel(\u0026#34;./Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.head()     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     0 BC AGASSIZ 1100120 1893 1 2018 9 49.25 -121.77 15 N   1 BC ATLIN 1200560 1905 8 2018 12 59.57 -133.7 674 N   2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   4 BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    Using this Dataframe we can define some input parameters to filter our data.\nvarin = \u0026#39;dn\u0026#39; # variable acronym  path = \u0026#39;Homog_daily_min_temp_v2018\u0026#39; # path to get data  varout = \u0026#39;Tasmin\u0026#39; province = \u0026#39;NWT\u0026#39; # Province to work with We can now filter our dataset.\nglobals()[\u0026#39;dataframe_\u0026#39;+province] = dataframe.loc[(dataframe[\u0026#34;Prov\u0026#34;] == province),:] globals()[\u0026#39;dataframe_\u0026#39;+province]     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     65 NWT CAPE PARRY 2200675 1957 5 2018 12 70.17 -124.72 87 N   66 NWT FORT GOOD HOPE 2201450 1944 8 2018 12 66.23 -128.65 82 Y   68 NWT FORT RELIANCE 2201903 1948 10 2018 12 62.72 -109.17 168 Y   69 NWT FORT SIMPSON 2202103 1895 11 2018 12 61.77 -121.23 169 Y   70 NWT FORT SMITH 2202201 1913 7 2018 12 60.02 -111.97 205 Y   71 NWT HAY RIVER 2202401 1893 9 2018 12 60.83 -115.78 166 Y   72 NWT INUVIK 2202578 1957 3 2018 12 68.3 -133.48 103 Y   73 NWT MOULD BAY 250M001 1948 5 2018 12 76.23 -119.35 2 Y   74 NWT NORMAN WELLS 2202801 1943 5 2018 12 65.28 -126.8 73 Y   75 NWT SACHS HARBOUR 2503648 1955 11 2018 8 72 -125.27 86 Y   76 NWT TUKTOYAKTUK 2203914 1957 6 2018 12 69.45 -133 18 Y   77 NWT YELLOWKNIFE 2204101 1942 7 2018 12 62.47 -114.43 206 Y    We found 13 stations for this province.\nWe want to work with YELLOWKNIFE station: stnid = 2204101.\nstnid = \u0026#39;2204101\u0026#39; f1 = open(\u0026#39;./\u0026#39;+path+\u0026#39;/\u0026#39;+str(varin)+str(stnid)+\u0026#39;.txt\u0026#39;, \u0026#39;r\u0026#39;) for line in islice(f1, 7): print(line) 2204101, YELLOWKNIFE , NWT, station joined , Homogenized daily minimum temperature , Deg Celcius, Updated to December 2018 2204101, YELLOWKNIFE , NWT, station jointe , Temperature quotidienne minimale homogeneisee, Deg Celcius, Mise a jour jusqu a decembre 2018 Year Mo Day 01 Day 02 Day 03 Day 04 Day 05 Day 06 Day 07 Day 08 Day 09 Day 10 Day 11 Day 12 Day 13 Day 14 Day 15 Day 16 Day 17 Day 18 Day 19 Day 20 Day 21 Day 22 Day 23 Day 24 Day 25 Day 26 Day 27 Day 28 Day 29 Day 30 Day 31 Annee Mo Jour 01 Jour 02 Jour 03 Jour 04 Jour 05 Jour 06 Jour 07 Jour 08 Jour 09 Jour 10 Jour 11 Jour 12 Jour 13 Jour 14 Jour 15 Jour 16 Jour 17 Jour 18 Jour 19 Jour 20 Jour 21 Jour 22 Jour 23 Jour 24 Jour 25 Jour 26 Jour 27 Jour 28 Jour 29 Jour 30 Jour 31 1942 7 12.2 13.3 11.7 10.0 9.4 11.7 14.4 14.4 13.3 12.2 10.6 11.7 11.1 12.8 14.4 15.0 13.9 14.4 13.9 14.4 13.9 13.3 11.7 12.2 11.1 12.2 13.3 10.0 10.6 11.7 8.3 1942 8 7.8 5.0 9.4 12.8 9.4 9.4 10.0 10.6 12.8 10.6 12.2 12.2 9.4 15.0 12.8 11.7 14.4 14.4 11.7 8.3 9.4 7.8 12.2 8.9 3.9 7.2 10.6 11.1 7.2 5.0 3.3 1942 9 3.9 6.1 6.1 8.3 9.4 11.1 11.1 6.7 6.1 9.4 5.0 9.4 7.2 4.4 5.0 4.4 3.3 1.1 2.2 -0.6 -2.2 0.0 -1.7 1.1 -4.4 -0.6 0.0 -0.6 1.7 1.1 -9999.9M  Cleaning data: We see that in our dataset we have for each line the daily data by year and by month according to the structure:\nThere is a 4 rows header. We will delete this header and also delete the alphanumeric characters, clean the missing values and create a dataframe.\nf1 = open(\u0026#39;./\u0026#39;+path+\u0026#39;/\u0026#39;+str(varin)+str(stnid)+\u0026#39;.txt\u0026#39;, \u0026#39;r\u0026#39;) f2 = open(\u0026#39;./tmp.txt\u0026#39;, \u0026#39;w\u0026#39;) for line in f1: for word in line: if word == \u0026#39;M\u0026#39;: f2.write(word.replace(\u0026#39;M\u0026#39;, \u0026#39;\u0026#39;)) elif word == \u0026#39;a\u0026#39;: f2.write(word.replace(\u0026#39;a\u0026#39;, \u0026#39;\u0026#39;)) else: f2.write(word) f1.close() f2.close() df_station = pd.read_csv(\u0026#39;./tmp.txt\u0026#39;, delim_whitespace=True, skiprows = range(0, 4)) df_station.head()     1942 7 12.2 13.3 11.7 10.0 9.4 11.7.1 14.4 14.4.1 13.3.1 12.2.1 10.6 11.7.2 11.1 12.8 14.4.2 15.0 13.9 14.4.3 13.9.1 14.4.4 13.9.2 13.3.2 11.7.3 12.2.2 11.1.1 12.2.3 13.3.3 10.0.1 10.6.1 11.7.4 8.3     0 1942 8 7.8 5 9.4 12.8 9.4 9.4 10 10.6 12.8 10.6 12.2 12.2 9.4 15 12.8 11.7 14.4 14.4 11.7 8.3 9.4 7.8 12.2 8.9 3.9 7.2 10.6 11.1 7.2 5 3.3   1 1942 9 3.9 6.1 6.1 8.3 9.4 11.1 11.1 6.7 6.1 9.4 5 9.4 7.2 4.4 5 4.4 3.3 1.1 2.2 -0.6 -2.2 0 -1.7 1.1 -4.4 -0.6 0 -0.6 1.7 1.1 -9999.9   2 1942 10 0 4.4 1.7 5 3.3 -1.1 4.4 2.2 3.9 1.7 5 0.6 -2.8 -1.1 2.8 -1.1 -1.1 2.8 0.6 -2.8 -2.8 -6.7 -10.6 -11.1 -7.2 -2.2 -1.7 -1.1 -3.9 -5 -8.3   3 1942 11 -11.7 -13.9 -13.3 -12.8 -12.8 -12.8 -12.2 -17.8 -12.2 -21.1 -16.1 -16.1 -17.8 -11.1 -22.2 -25 -25 -22.8 -17.8 -23.9 -26.7 -13.9 -18.3 -26.7 -32.2 -31.1 -34.4 -26.7 -30 -27.2 -9999.9   4 1942 12 -23.9 -20.6 -20.6 -25 -20.6 -20.6 -26.1 -27.8 -30 -31.1 -28.9 -23.9 -25 -31.1 -33.9 -37.2 -37.8 -38.9 -40.6 -39.4 -33.3 -35 -35.6 -33.9 -26.1 -31.7 -34.4 -28.9 -24.4 -33.9 -40.6    That's better but we still have some missing values. We will also change column names.\ndf_station.columns = [\u0026#39;Year\u0026#39;, \u0026#39;Month\u0026#39;, \u0026#39;D1\u0026#39;,\u0026#39;D2\u0026#39;,\u0026#39;D3\u0026#39;,\u0026#39;D4\u0026#39;,\u0026#39;D5\u0026#39;,\u0026#39;D6\u0026#39;,\u0026#39;D7\u0026#39;,\u0026#39;D8\u0026#39;,\u0026#39;D9\u0026#39;,\u0026#39;D10\u0026#39;, \u0026#39;D11\u0026#39;,\u0026#39;D12\u0026#39;,\u0026#39;D13\u0026#39;,\u0026#39;D14\u0026#39;,\u0026#39;D15\u0026#39;,\u0026#39;D16\u0026#39;,\u0026#39;D17\u0026#39;,\u0026#39;D18\u0026#39;,\u0026#39;D19\u0026#39;,\u0026#39;D20\u0026#39;, \u0026#39;D21\u0026#39;,\u0026#39;D22\u0026#39;,\u0026#39;D23\u0026#39;,\u0026#39;D24\u0026#39;,\u0026#39;D25\u0026#39;,\u0026#39;D26\u0026#39;,\u0026#39;D27\u0026#39;,\u0026#39;D28\u0026#39;,\u0026#39;D29\u0026#39;,\u0026#39;D30\u0026#39;,\u0026#39;D31\u0026#39;] os.remove(\u0026#34;./tmp.txt\u0026#34;) # nettoyage des valeurs manquantes  try: df_station = df_station.replace({\u0026#39;E\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: df_station = df_station.replace({\u0026#39;a\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: df_station = df_station.replace({\u0026#39;-9999.9\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: df_station = df_station.replace({-9999.9:\u0026#39;\u0026#39;}, regex=True) except: pass for col in df_station.columns[2:]: df_station[col] = pd.to_numeric(df_station[col], errors=\u0026#39;coerce\u0026#39;) df_station.head()     Year Month D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 D11 D12 D13 D14 D15 D16 D17 D18 D19 D20 D21 D22 D23 D24 D25 D26 D27 D28 D29 D30 D31     0 1942 8 7.8 5 9.4 12.8 9.4 9.4 10 10.6 12.8 10.6 12.2 12.2 9.4 15 12.8 11.7 14.4 14.4 11.7 8.3 9.4 7.8 12.2 8.9 3.9 7.2 10.6 11.1 7.2 5 3.3   1 1942 9 3.9 6.1 6.1 8.3 9.4 11.1 11.1 6.7 6.1 9.4 5 9.4 7.2 4.4 5 4.4 3.3 1.1 2.2 -0.6 -2.2 0 -1.7 1.1 -4.4 -0.6 0 -0.6 1.7 1.1 -9999.9   2 1942 10 0 4.4 1.7 5 3.3 -1.1 4.4 2.2 3.9 1.7 5 0.6 -2.8 -1.1 2.8 -1.1 -1.1 2.8 0.6 -2.8 -2.8 -6.7 -10.6 -11.1 -7.2 -2.2 -1.7 -1.1 -3.9 -5 -8.3   3 1942 11 -11.7 -13.9 -13.3 -12.8 -12.8 -12.8 -12.2 -17.8 -12.2 -21.1 -16.1 -16.1 -17.8 -11.1 -22.2 -25 -25 -22.8 -17.8 -23.9 -26.7 -13.9 -18.3 -26.7 -32.2 -31.1 -34.4 -26.7 -30 -27.2 -9999.9   4 1942 12 -23.9 -20.6 -20.6 -25 -20.6 -20.6 -26.1 -27.8 -30 -31.1 -28.9 -23.9 -25 -31.1 -33.9 -37.2 -37.8 -38.9 -40.6 -39.4 -33.3 -35 -35.6 -33.9 -26.1 -31.7 -34.4 -28.9 -24.4 -33.9 -40.6    We can now detect the minimum and maximum recording years and write the daily data on a single column.\nyearmin = df_station[\u0026#39;Year\u0026#39;].min() yearmax = df_station[\u0026#39;Year\u0026#39;].max() m_start = df_station[\u0026#39;Month\u0026#39;].loc[(df_station[\u0026#39;Year\u0026#39;] == yearmin)].min() m_end = df_station[\u0026#39;Month\u0026#39;].loc[(df_station[\u0026#39;Year\u0026#39;] == yearmax)].max() d_end = calendar.monthrange(yearmax, m_end)[1] tmp_tmin = [ ] for year in range(yearmin,yearmax+1): ### Loop over years for month in range(1,13): df = [] last_day = calendar.monthrange(year, month)[1] tmin = df_station.loc[(df_station[\u0026#34;Year\u0026#34;] == year) \u0026amp; (df_station[\u0026#34;Month\u0026#34;] == month)].iloc[:,2:last_day+2].values if len(tmin) == 0: a = np.empty((calendar.monthrange(year,month)[1])) a[:] = np.nan df=pd.DataFrame(a) else: df=pd.DataFrame(tmin.T) start = date(year, month, 1) end = date(year, month, last_day) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) df[\u0026#39;datetime\u0026#39;] = rng df.index = df[\u0026#39;datetime\u0026#39;] tmp_tmin.append(df) tmp_tmin = pd.concat(tmp_tmin) df = pd.DataFrame({\u0026#39;datetime\u0026#39;: tmp_tmin[\u0026#39;datetime\u0026#39;], \u0026#39;Var\u0026#39;: tmp_tmin.iloc[:,0]}, columns = [\u0026#39;datetime\u0026#39;,\u0026#39;Tmin\u0026#39;]) df.index = df[\u0026#39;datetime\u0026#39;] tmp_tmin = tmp_tmin.drop([\u0026#34;datetime\u0026#34;], axis=1) tmp_tmin.tail()    datetime 0     2018-12-27 00:00:00 -32.6   2018-12-28 00:00:00 -33.3   2018-12-29 00:00:00 -27.7   2018-12-30 00:00:00 -35.2   2018-12-31 00:00:00 -33.8    visualization: Quick visualization of the monthly average temperatures for the month of January. We will group the data by month and calculate the average.\nimport matplotlib.pylab as plt import datetime month_tmin = tmp_tmin.resample(\u0026#39;M\u0026#39;).mean() month_tmin.tail()    datetime 0     2018-08-31 00:00:00 8.9871   2018-09-30 00:00:00 -0.673333   2018-10-31 00:00:00 -4.99032   2018-11-30 00:00:00 -15.5233   2018-12-31 00:00:00 -21.8129    tmin_janvier = month_tmin[month_tmin.index.month==1] tmin_janvier.head()    datetime 0     1942-01-31 00:00:00 nan   1943-01-31 00:00:00 -32.6871   1944-01-31 00:00:00 -25.471   1945-01-31 00:00:00 -28.6355   1946-01-31 00:00:00 -33.4032    plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[10,6] plt.plot(tmin_janvier.index, tmin_janvier[:], label=\u0026#39;Tmin Station\u0026#39;, linewidth=2, c=\u0026#39;r\u0026#39;) plt.title(\u0026#39;Monthly mean of daily minimum temperature: January from \u0026#39; + datetime.date(yearmin, 1, 1).strftime(\u0026#39;%Y\u0026#39;)+ \u0026#39;et \u0026#39; + datetime.date(yearmax, 1, 1).strftime(\u0026#39;%Y\u0026#39;), fontsize=15, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize=15, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.ylabel(\u0026#39;°C\u0026#39;, fontsize=15, color=\u0026#39;black\u0026#39;, weight=\u0026#39;semibold\u0026#39;) plt.show() Final code The following code retrieves all stations for a specific province but for a common period.\nFor example, we wish to extract all daily temperature data for the province of the Northwest Territories but only for the common period 1989-2018.\nWe wish to have one file per station.\nimport pandas as pd import os from datetime import date import calendar import numpy as np import pathlib ################################################ # varin = \u0026#39;dn\u0026#39; path = \u0026#39;Homog_daily_min_temp_v2018\u0026#39; varout = \u0026#39;Tasmoy\u0026#39; province = \u0026#39;NWT\u0026#39; yearmin = 1989 yearmax = 2018 ############################################################################### dataframe = pd.read_excel(\u0026#34;./Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) globals()[\u0026#39;dataframe_\u0026#39;+province] = dataframe.loc[(dataframe[\u0026#34;Prov\u0026#34;] == province) \u0026amp; (dataframe[\u0026#34;année déb.\u0026#34;] \u0026lt;= yearmin) \u0026amp; (dataframe[\u0026#34;année fin.\u0026#34;] \u0026gt;= yearmax),:] names = [] for i, row in globals()[\u0026#39;dataframe_\u0026#39;+province].iterrows(): stnid = row[\u0026#39;stnid\u0026#39;] f1 = open(\u0026#39;./\u0026#39;+path+\u0026#39;/\u0026#39;+str(varin)+str(stnid)+\u0026#39;.txt\u0026#39;, \u0026#39;r\u0026#39;) f2 = open(\u0026#39;./tmp.txt\u0026#39;, \u0026#39;w\u0026#39;) for line in f1: for word in line: if word == \u0026#39;M\u0026#39;: f2.write(word.replace(\u0026#39;M\u0026#39;, \u0026#39;\u0026#39;)) elif word == \u0026#39;a\u0026#39;: f2.write(word.replace(\u0026#39;a\u0026#39;, \u0026#39;\u0026#39;)) else: f2.write(word) f1.close() f2.close() station = pd.read_csv(\u0026#39;./tmp.txt\u0026#39;, delim_whitespace=True, skiprows = range(0, 4)) station.columns = [\u0026#39;Annee\u0026#39;, \u0026#39;Mois\u0026#39;, \u0026#39;D1\u0026#39;,\u0026#39;D2\u0026#39;,\u0026#39;D3\u0026#39;,\u0026#39;D4\u0026#39;,\u0026#39;D5\u0026#39;,\u0026#39;D6\u0026#39;,\u0026#39;D7\u0026#39;,\u0026#39;D8\u0026#39;,\u0026#39;D9\u0026#39;,\u0026#39;D10\u0026#39;, \u0026#39;D11\u0026#39;,\u0026#39;D12\u0026#39;,\u0026#39;D13\u0026#39;,\u0026#39;D14\u0026#39;,\u0026#39;D15\u0026#39;,\u0026#39;D16\u0026#39;,\u0026#39;D17\u0026#39;,\u0026#39;D18\u0026#39;,\u0026#39;D19\u0026#39;,\u0026#39;D20\u0026#39;, \u0026#39;D21\u0026#39;,\u0026#39;D22\u0026#39;,\u0026#39;D23\u0026#39;,\u0026#39;D24\u0026#39;,\u0026#39;D25\u0026#39;,\u0026#39;D26\u0026#39;,\u0026#39;D27\u0026#39;,\u0026#39;D28\u0026#39;,\u0026#39;D29\u0026#39;,\u0026#39;D30\u0026#39;,\u0026#39;D31\u0026#39;] os.remove(\u0026#34;./tmp.txt\u0026#34;) try: station = station.replace({\u0026#39;E\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: station = station.replace({\u0026#39;a\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: station = station.replace({\u0026#39;-9999.9\u0026#39;:\u0026#39;\u0026#39;}, regex=True) except: pass try: station = station.replace({-9999.9:\u0026#39;\u0026#39;}, regex=True) except: pass for col in station.columns[2:]: station[col] = pd.to_numeric(station[col], errors=\u0026#39;coerce\u0026#39;) m_start = station[\u0026#39;Mois\u0026#39;].loc[(station[\u0026#39;Annee\u0026#39;] == yearmin)].min() m_end = station[\u0026#39;Mois\u0026#39;].loc[(station[\u0026#39;Annee\u0026#39;] == yearmax)].max() d_end = calendar.monthrange(yearmax, m_end)[1] tmp_tmin = [ ] for year in range(yearmin,yearmax+1): ### Boucle sur les annees for month in range(1,13): df = [] last_day = calendar.monthrange(year, month)[1] tmin = station.loc[(station[\u0026#34;Annee\u0026#34;] == year) \u0026amp; (station[\u0026#34;Mois\u0026#34;] == month)].iloc[:,2:last_day+2].values if len(tmin) == 0: a = np.empty((calendar.monthrange(year,month)[1])) a[:] = np.nan df=pd.DataFrame(a) else: df=pd.DataFrame(tmin.T) start = date(year, month, 1) end = date(year, month, last_day) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) df[\u0026#39;datetime\u0026#39;] = rng df.index = df[\u0026#39;datetime\u0026#39;] tmp_tmin.append(df) tmp_tmin = pd.concat(tmp_tmin) df = pd.DataFrame({\u0026#39;datetime\u0026#39;: tmp_tmin[\u0026#39;datetime\u0026#39;], \u0026#39;Var\u0026#39;: tmp_tmin.iloc[:,0]}, columns = [\u0026#39;datetime\u0026#39;,\u0026#39;Tmin\u0026#39;]) df.index = df[\u0026#39;datetime\u0026#39;] tmp_tmin = tmp_tmin.drop([\u0026#34;datetime\u0026#34;], axis=1) name = row[\u0026#39;Nom de station\u0026#39;].replace(\u0026#39;\u0026#39;,\u0026#39;_\u0026#39;) name = name.replace(\u0026#34;\u0026#39;\u0026#34;,\u0026#39;\u0026#39;) names.append(name) mypath=\u0026#39;./Daily_data_by_Province/\u0026#39;+varout+\u0026#39;/\u0026#39; pathlib.Path(mypath).mkdir(parents=True, exist_ok=True) tmp_tmin.to_csv(mypath+name+\u0026#39;_daily_\u0026#39;+varout+\u0026#39;_\u0026#39;+str(yearmin)+\u0026#39;-\u0026#39;+str(yearmax)+\u0026#39;.csv\u0026#39;) latlon = pd.DataFrame({\u0026#39;Latitude\u0026#39;: globals()[\u0026#39;dataframe_\u0026#39;+province][\u0026#34;lat (deg)\u0026#34;], \u0026#39;Longitude\u0026#39;: globals()[\u0026#39;dataframe_\u0026#39;+province][\u0026#34;long (deg)\u0026#34;] }, columns = [\u0026#39;Latitude\u0026#39;,\u0026#39;Longitude\u0026#39;]) latlon.to_csv(\u0026#39;./Daily_data_by_Province/stations_latlon_\u0026#39;+province+\u0026#39;.csv\u0026#39;) names = pd.DataFrame(names) names.to_csv(\u0026#39;./Daily_data_by_Province/stations_noms_\u0026#39;+province+\u0026#39;.csv\u0026#39;) base_filename = \u0026#39;./Daily_data_by_Province/stations_noms_\u0026#39;+province+\u0026#39;.txt\u0026#39; names[0].to_csv(base_filename, sep=\u0026#39;\\t\u0026#39;, index = False) ","date":1575690639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575690639,"objectID":"c7175df8435a025284248b2e831c63ce","permalink":"/post/eccc_temp/","publishdate":"2019-12-06T19:50:39-08:00","relpermalink":"/post/eccc_temp/","section":"post","summary":"Extract daily temperature from Environment Canada using Python The objective of this product is to retrieve daily temperature data from the second generation homogenized dataset of Environment and Climate Change Canada developed by Vincent et al. 2012.\nAdjusted and homogenized Canadian climate dataset (DCCAH) were prepared to provide a better spatial and temporal representation of the climate trends in Canada.\nIn the Second Generation of Homogenized Temperature, new adjustments were applied to the daily minimum temperatures at synoptic stations (mainly airports) to address the bias due to the change in observing time in July 1961 (Vincent et al.","tags":[],"title":"ECCC_Temp","type":"post"},{"authors":[],"categories":[],"content":"ANUSPLIN climatology using Cartopy Agriculture and Agri-Food Canada have produced daily precipitation, minimum and maximum temperature across Canada (south of 60°N) for climate related application purpose using thin-plate smoothing splines, as implemented in the ANUSPLIN climate modeling software (Hutchinson et al., 2009; McKenney et al., 2011).\nThe so-called ANUSPLIN data uses ground-based observations and generates daily gridded data from 1951 to 2017 on a Lambert conformal conic projection with 5’ arc minutes spacing (equivalent to a resolution of about 10 km). The key strength of this spatial interpolation method is its global dependence on all data, permitting robust and stable determination of spatially varying dependences on elevation. Hutchinson et al. (2009) have shown that while ANUSPLIN fall month’s absolute errors were remarkably small, those of winter months were quite large due to rather difficult observation and measurement conditions.\nData are available on:\nftp://ftp.nrcan.gc.ca/pub/outgoing/canada_daily_grids\nIn this post, we will see how to use Cartopy with Netcdf in order to display a nice climatology.\nWe will display a climatoly of ANUSPLIN from 1981 to 2010.\nThis dataset is only availabe with ascii grid format, I'll here show how to convert it into Netcdf using Python in an other post.\nThe Cartopy python library allows you to analyze, process and plot georeferenced data with the help of Matplotlib.\nhttps://scitools.org.uk/cartopy/docs/latest/#\nWe first import our librairies:\nfrom netCDF4 import Dataset, num2date import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import datetime import xarray as xr import pandas as pd filename=\u0026#39;K:/DATA/ANUSPLIN_10km/Netcdf/all_domaine/YEAR/Mean_tasmoy/ANUSPLIN_10km_MEAN_YEAR_Mean_tasmoy_1950-2017.nc\u0026#39; nc_fid=Dataset(filename,\u0026#39;r\u0026#39;) nc_fid.variables OrderedDict([('lon', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 lon(y, x) units: degrees_east long_name: Longitude CoordinateAxisType: Lon unlimited dimensions: current shape = (1068, 510) filling on, default _FillValue of 9.969209968386869e+36 used), ('lat', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 lat(y, x) units: degrees_north long_name: Latitude CoordinateAxisType: Lat unlimited dimensions: current shape = (1068, 510) filling on, default _FillValue of 9.969209968386869e+36 used), ('time', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float64 time(time) long_name: Time delta_t: unlimited dimensions: current shape = (68,) filling on, default _FillValue of 9.969209968386869e+36 used), ('Mean_tasmoy', \u0026lt;class 'netCDF4._netCDF4.Variable'\u0026gt; float32 Mean_tasmoy(time, y, x) long_name: Mean_tasmoy units: Celcius missing_value: -999.0 coordinates: lon lat unlimited dimensions: current shape = (68, 1068, 510) filling on, default _FillValue of 9.969209968386869e+36 used)])  data=nc_fid.variables[\u0026#39;Mean_tasmoy\u0026#39;][:].squeeze() lons=nc_fid.variables[\u0026#39;lon\u0026#39;][:].squeeze() lats=nc_fid.variables[\u0026#39;lat\u0026#39;][:].squeeze() time = nc_fid.variables[\u0026#39;time\u0026#39;] data.shape (68, 1068, 510)  data_m = data[32:62,:,:] clim_81_2010=data_m.mean(axis=0) clim_81_2010.shape (1068, 510)  We will add some cities to display on our map. We first need to create a dataframe with names of stations and localisations.\nnames=[\u0026#39;Whitehorse\u0026#39;,\u0026#39;Yellowknife\u0026#39;,\u0026#39;Iqaluit\u0026#39;,\u0026#39;Victoria\u0026#39;,\u0026#39;Edmonton\u0026#39;,\u0026#39;Regina\u0026#39;,\u0026#39;Winnipeg\u0026#39;,\u0026#39;Toronto\u0026#39;,\u0026#39;Ottawa\u0026#39;,\u0026#39;Quebec\u0026#39;,\u0026#39;Halifax\u0026#39;,\u0026#39;Charlottetown\u0026#39;,\u0026#39;St-John s\u0026#39;] latitudes=[60.721188,62.453972,63.748611,48.407326,53.631611,50.445210, 49.895077,43.651070,45.424721,46.829853,44.651070,46.238888,47.560539] longitudes=[-135.056839,-114.371788,-68.519722,-123.329773,-113.323975,-104.618896, -97.138451,-79.347015,-75.695000,-71.254028,-63.582687,-63.129166,-52.712830] df = pd.DataFrame(list(zip(names, latitudes, longitudes)), columns =[\u0026#39;Names\u0026#39;, \u0026#39;latitudes\u0026#39;, \u0026#39;longitudes\u0026#39;]) df     Names latitudes longitudes     0 Whitehorse 60.7212 -135.057   1 Yellowknife 62.454 -114.372   2 Iqaluit 63.7486 -68.5197   3 Victoria 48.4073 -123.33   4 Edmonton 53.6316 -113.324   5 Regina 50.4452 -104.619   6 Winnipeg 49.8951 -97.1385   7 Toronto 43.6511 -79.347   8 Ottawa 45.4247 -75.695   9 Quebec 46.8299 -71.254   10 Halifax 44.6511 -63.5827   11 Charlottetown 46.2389 -63.1292   12 St-John s 47.5605 -52.7128    import matplotlib.pylab as plt import cartopy.crs as ccrs import cartopy.feature as cfeature import numpy as np import matplotlib as mpl from carto import scale_bar Y=np.array([[50,136,189],[102,194,165],[171,221,164],[230,245,152],\\ [255,255,191],[254,224,139],[253,174,97],[244,109,67],[213,62,79]])/255. colbar=mpl.colors.ListedColormap(Y) fig=plt.figure(figsize=(28,16), frameon=True) ax = plt.subplot(111, projection=ccrs.LambertConformal()) ax.set_extent([-130,-55,35,80]) #ax.coastlines(resolution=\u0026#39;110m\u0026#39;); ax.add_feature(cfeature.OCEAN.with_scale(\u0026#39;50m\u0026#39;)) # couche ocean ax.add_feature(cfeature.LAND.with_scale(\u0026#39;50m\u0026#39;)) # couche land ax.add_feature(cfeature.LAKES.with_scale(\u0026#39;50m\u0026#39;)) # couche lac ax.add_feature(cfeature.BORDERS.with_scale(\u0026#39;50m\u0026#39;)) # couche frontieres ax.add_feature(cfeature.RIVERS.with_scale(\u0026#39;50m\u0026#39;)) # couche rivières  coast = cfeature.NaturalEarthFeature(category=\u0026#39;physical\u0026#39;, scale=\u0026#39;10m\u0026#39;, # ajout de la couche cotière  facecolor=\u0026#39;none\u0026#39;, name=\u0026#39;coastline\u0026#39;) ax.add_feature(coast, edgecolor=\u0026#39;black\u0026#39;) states_provinces = cfeature.NaturalEarthFeature( category=\u0026#39;cultural\u0026#39;, name=\u0026#39;admin_1_states_provinces_lines\u0026#39;, scale=\u0026#39;10m\u0026#39;, facecolor=\u0026#39;none\u0026#39;) ax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;) ax.plot(df[\u0026#39;longitudes\u0026#39;], df[\u0026#39;latitudes\u0026#39;], \u0026#39;ko\u0026#39;, ms=5, transform=ccrs.Geodetic()) for lg, lt, name in zip(df[\u0026#39;longitudes\u0026#39;], df[\u0026#39;latitudes\u0026#39;], df[\u0026#39;Names\u0026#39;]): if name in [u\u0026#39;Nazaré\u0026#39;, \u0026#39;Marinha Grande\u0026#39;]: ax.text(lg - .05, lt + .05, name, va=\u0026#39;center\u0026#39;, ha=\u0026#39;right\u0026#39;, transform=ccrs.Geodetic(), fontweight=\u0026#39;bold\u0026#39;) else: ax.text(lg + .5, lt + .5, name, va=\u0026#39;center\u0026#39;, ha=\u0026#39;left\u0026#39;, transform=ccrs.Geodetic(), fontweight=\u0026#39;bold\u0026#39;) mm = ax.pcolormesh(lons,\\ lats,\\ clim_81_2010,\\ vmin=-30,\\ vmax=15, \\ transform=ccrs.PlateCarree(),\\ cmap=colbar ) ax.gridlines() # Define gridline locations and draw the lines using cartopy\u0026#39;s built-in gridliner: xticks = np.arange(-150.0,-40.0,20) yticks =np.arange(10,80,10) # Standard 6,000 km scale bar. scale_bar(ax, (0.85, 0.05), 500 ,plot_kwargs = dict(linestyle=\u0026#39;dashed\u0026#39;, color=\u0026#39;black\u0026#39;)) cbar = plt.colorbar(mm, orientation=\u0026#39;horizontal\u0026#39;, shrink=0.5, drawedges=\u0026#39;True\u0026#39;, ticks=np.arange(-30, 15.1, 5),extend=\u0026#39;both\u0026#39;) cbar.set_label(u\u0026#39;\\nProjection = Lambert Conformal Conic \\nResolution: 5 Arcs-Minutes (10 km)\\nData provided by Natural Resources Canada / Created by Guillaume Dueymes\u0026#39;, size=\u0026#39;medium\u0026#39;) # Affichage de la légende de la barre de couleur cbar.ax.tick_params(labelsize=17) plt.xlabel(u\u0026#39;\\n\\n\\nTemperature / Température (°C)\u0026#39;,size=\u0026#39;x-large\u0026#39;) string_title=u\u0026#39;Climate normals of mean annual temperature (°C)\\nreference period 1981-2010\\n\u0026#39; plt.title(string_title, size=\u0026#39;xx-large\u0026#39;) plt.savefig(\u0026#39;./ANUSPLIN_NLDAS_10km_YEAR_CLIM_1981-2010.png\u0026#39;, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1) plt.show() ","date":1575338174,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575338174,"objectID":"d5630635c7a6159d33fcc5d736b661de","permalink":"/post/cartopy_climatology/","publishdate":"2019-12-02T17:56:14-08:00","relpermalink":"/post/cartopy_climatology/","section":"post","summary":"ANUSPLIN climatology using Cartopy Agriculture and Agri-Food Canada have produced daily precipitation, minimum and maximum temperature across Canada (south of 60°N) for climate related application purpose using thin-plate smoothing splines, as implemented in the ANUSPLIN climate modeling software (Hutchinson et al., 2009; McKenney et al., 2011).\nThe so-called ANUSPLIN data uses ground-based observations and generates daily gridded data from 1951 to 2017 on a Lambert conformal conic projection with 5’ arc minutes spacing (equivalent to a resolution of about 10 km).","tags":[],"title":"Cartopy_climatology","type":"post"},{"authors":null,"categories":null,"content":"","date":1575244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575244800,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2019-12-02T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"Réseau Inondations InterSectoriel du Québec.","tags":["Inondations"],"title":"","type":"project"},{"authors":["Irina Sagurova","Antoinette Ludwig","Nicholas H. Ogden","Yann Pelcat","Guillaume Dueymes","Philippe Gachon"],"categories":["2"],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"f93e4e42d5544ce86272d249f286781a","permalink":"/publication/article9/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/article9/","section":"publication","summary":"Background: The geographic range of the tick Amblyomma americanum, a vector of diseases of public health significance such as ehrlichiosis, has expanded from the southeast of the United States northward during the 20th century. Recently, populations of this tick have been reported to be present close to the Canadian border in Michigan and New York states, but established populations are not known in Canada. Previous research suggests that changing temperature patterns with climate change may influence tick life cycles and permit northward range expansion of ticks in the northern hemisphere. Objectives: We aimed to estimate minimal temperature conditions for survival of A. americanum populations at the northern edge of the tick's range and to investigate the possibility of range expansion of A. americanum into northern U.S. states and southern Canada in the coming decades. Methods: A simulation model of the tick A. americanum was used, via simulations using climate data from meteorological stations in the United States and Canada, to estimate minimal temperature conditions for survival of A. americanum populations at the northern edge of the tick's range. Results: The predicted geographic scope of temperature suitability [ ≥ 3,285 annual cumulative degree days (DD)  0 ° C ] included most of the central and eastern U.S. states east of longitude 110°W, which is consistent with current surveillance data for the presence of the tick in this region, as well as parts of southern Quebec and Ontario in Canada. Regional climate model output raises the possibility of northward range expansion into all provinces of Canada from Alberta to Newfoundland and Labrador during the coming decades, with the greatest northward range expansion (up to 1,000 km by the year 2100) occurring under the greenhouse gas (GHG) emissions of Representative Concentration Pathway (RCP) 8.5. Predicted northward range expansion was reduced by approximately half under the reduced GHG emissions of RCP4.5. Discussion: Our results raise the possibility of range expansion of A. americanum into northern U.S. states and southern Canada in the coming decades, and conclude that surveillance for this tick, and the diseases it transmits, would be prudent.","tags":[],"title":"Predicted Northward Expansion of the Geographic Range of the Tick Vector Amblyomma americanum in North America under Future Climate Conditions","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\"data.csv\") data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file and adding markup: mmark to your page front matter.\nTo render inline or block math, wrap your LaTeX math with $$...$$.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ renders as\n\\[\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}\\]\nExample inline math $$\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2$$ renders as \\(\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2\\) .\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$ renders as\n\\[f(k;p_0^*) = \\begin{cases} p_0^* \u0026 \\text{if }k=1, \\\\ 1-p_0^* \u0026 \\text {if }k=0.\\end{cases}\\]\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD; A--B; A--C; B--D; C--D; ``` renders as\ngraph TD; A--B; A--C; B--D; C--D; An example sequence diagram:\n```mermaid sequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good! ``` renders as\nsequenceDiagram participant Alice participant Bob Alice-John: Hello John, how are you? loop Healthcheck John-John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--Alice: Great! John-Bob: How about you? Bob--John: Jolly good! An example Gantt diagram:\n```mermaid gantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d ``` renders as\ngantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a Markdown extension for asides, also referred to as notices or hints. By prefixing a paragraph with A\u0026gt;, it will render as an aside. You can enable this feature by adding markup: mmark to your page front matter, or alternatively using the Alert shortcode.\nA\u0026gt; A Markdown aside is useful for displaying notices, hints, or definitions to your readers. renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.\n Did you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":null,"categories":null,"content":"Python includes several built-in container types, but the most common ones are lists and tuples, which we would see in this tutorial.\nThere are certain things you can do with all container types. These operations include indexing, slicing, adding, multiplying. In addition, Python has built-in functions like finding the length of a sequence, finding its largest and smallest elements\u0026hellip;\nEach element of a sequence is assigned by a number - its position or index. The first index is zero, the second index is one\u0026hellip;\nCreating a list is as simple as putting different comma-separated values between square brackets :\n list1 = [ a, b, c, d, e] list2 = [ 1, 2, 3, 4, 5]  Python is an object-oriented language, lists are associated with methods: object.method() Functions can be applied to lists.\n2.1 Create a list To create a list, we use comma-separated values between square brackets.\nmy_list = [1,2,3,4,5,6,7,8,\u0026#34;hello\u0026#34;,10.5] print(my_list) # fonction to print elements in list  [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5]  We can use range()function to generate a sequence of numbers over time.\nmy_list = list(range(10)) my_list [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  my_list = list(range(1981,2011,2)) my_list [1981, 1983, 1985, 1987, 1989, 1991, 1993, 1995, 1997, 1999, 2001, 2003, 2005, 2007, 2009]  When we work with Python object, dir() command always shows us the tasks we can do with this object: dir(my_list)\ndir(my_list) ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']  2.2 Accessing Values in Lists : To access values in a list, use the square brackets for slicing along with the index or indices to obtain value available at that index.\nThe first index is zero, the second index is one\u0026hellip; to read first element, we use index 0. Then to read last element, we use -1 index.\nIt is also possible to modify a value with its index.\nmy_list = [1,2,3,4,5,6,7,8,\u0026#34;hello\u0026#34;,10.5] my_list[:] # To use all elements [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5]  my_list[1] # to access an item from the list: here we access the second element.  2  my_list[2:4] # slicing: to access the elements between the 3rd position and the 4th [3, 4]  my_list[:4] # slicing: to access all elements up to index 4 or 4th position [1, 2, 3, 4]  my_list[3:] # slicing: to access all elements from index 3 [4, 5, 6, 7, 8, 'hello', 10.5]  my_list[-2] # to get the 2nd value from the end, with use negative indexes, we do not start from 0 anymore. 'hello'  my_list[1:-4] # we start from the index 1, with slicing, we stop at the 4th index from the end [2, 3, 4, 5, 6]  my_list[::2] # to extract the elements with an increment [1, 3, 5, 7, 'hello']  my_list[1::2] # to extract the elements with an increment [2, 4, 6, 8, 10.5]  2.3 Updating Lists https://docs.python.org/2/tutorial/datastructures.html#more-on-lists\n2.3.1 Add elements To addan element: we use .append()method\nmy_list = [1,2,3,4,5,6,7,8,\u0026#34;hello\u0026#34;,10.5] my_list [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5]  my_list.append(2) print(my_list) [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5, 2]  my_list [1, 2, 3, 4, 5, 6, 7, 8, 'hello', 10.5, 2]  2.3.2 Insert elements To insertan element to our list using index: insert()method\nmy_list.insert(5,\u0026#34;new\u0026#34;) print(my_list) [1, 2, 3, 4, 5, 'new', 6, 7, 8, 'hello', 10.5, 2]  2.3.3 Update elements To changean element in our list using index\nmy_list[0]=\u0026#34;a\u0026#34; print(my_list) ['a', 2, 3, 4, 5, 'new', 6, 7, 8, 'hello', 10.5, 2]  To changeseveral elements in our list: slicing\nmy_list[3:5]=[8,10,11,22] print(my_list) ['a', 2, 3, 8, 10, 11, 22, 'new', 6, 7, 8, 'hello', 10.5, 2]  2.3.4 Remove elements To removein our list: 2 methods\nmy_list[0:2]=[] # remove using slicing  print(my_list) [3, 8, 10, 11, 22, 'new', 6, 7, 8, 'hello', 10.5, 2]  my_list [3, 8, 10, 11, 22, 'new', 6, 7, 8, 'hello', 10.5, 2]  del my_list[3] # to remove using keywords print(my_list) [3, 8, 10, 22, 'new', 6, 7, 8, 'hello', 10.5, 2]  https://www.programiz.com/python-programming/keyword-list\n2.4 Some useful operations on lists  Concatenate two lists  my_list1=[1,2,3,4,5] my_list2=[6,7,8,9,10] my_list3=my_list1+my_list2 my_list3 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]   Duplicate a list  my_list1=[1,2,3,4,5] my_list1*3 [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]   Reverse elements in list  .reverse()method is used to reverse elements in list\nmy_list1=[1,2,3,4,5] print(my_list1) my_list1.reverse() print(my_list1) [1, 2, 3, 4, 5] [5, 4, 3, 2, 1]   Count elements in list  .count()method is used to return count of how many times obj occurs in list.\nmy_list2= [\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;c\u0026#34;] my_list2.count(\u0026#34;a\u0026#34;) 3   .index()method returns the lowest index in list that obj appears  my_list2=[\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;c\u0026#34;] my_list2.index(\u0026#34;c\u0026#34;) 4   function sum()  my_list1=[1,2,3,4,5] sum(my_list1) 15   function .len()gives the total length of the list.  len(my_list1) 5   .sort()function to sort objects of list  my_list = [90,3,8,4,1,10,25,99] my_list.sort() my_list [1, 3, 4, 8, 10, 25, 90, 99]  my_list = [90,3,8,4,1,10,25,99] my_list [90, 3, 8, 4, 1, 10, 25, 99]  my_list.sort() my_list.reverse() my_list [99, 90, 25, 10, 8, 4, 3, 1]   min()and max()functions to return the minimumand the maximumfrom list  my_list [99, 90, 25, 10, 8, 4, 3, 1]  min(my_list) 1  max(my_list) 99   example applying a loop forand print()function to return values from a list  list_month = [\u0026#34;jan\u0026#34;,\u0026#34;feb\u0026#34;,\u0026#34;mar\u0026#34;] for month in list_month: print(month) jan feb mar   enumerate()function to return values and index  list_month = [\u0026#34;jan\u0026#34;,\u0026#34;feb\u0026#34;,\u0026#34;mar\u0026#34;] for month in enumerate(list_month): print(month) (0, 'jan') (1, 'feb') (2, 'mar')   .split()method to transform string into list  my_string = \u0026#34;January-February-March\u0026#34; my_string.split(\u0026#34;-\u0026#34;) ['January', 'February', 'March']   .join()method to transform list into string  list1 = [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;] \u0026#34;:\u0026#34;.join(list1) 'January:February:March'  2.5 Loop over lists You can loop over the elements of a list like this:\nlist1 = [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;] for month in list1: print(month) January February March  If you want access to the index of each element within the body of a loop, use the built-in enumerate function:\nlist1 = [\u0026#39;January\u0026#39;, \u0026#39;February\u0026#39;, \u0026#39;March\u0026#39;] for idx, month in enumerate(list1): print(\u0026#39;#%d: %s\u0026#39; % (idx + 1, month)) #1: January #2: February #3: March  2.6 List comprehensions When programming, frequently we want to transform one type of data into another. As a simple example, consider the following code that computes square numbers:\nnums = [0, 1, 2, 3, 4] squares = [] for x in nums: squares.append(x ** 2) print(squares) [0, 1, 4, 9, 16]  But, with You can make this code simpler using a list comprehension:\nnums = [0, 1, 2, 3, 4] squares = [x ** 2 for x in nums] print(squares) [0, 1, 4, 9, 16]  List comprehensions can also contain conditions:\nnums = [0, 1, 2, 3, 4] even_squares = [x ** 2 for x in nums if x % 2 == 0] print(even_squares) [0, 4, 16]  2.7 Tuples   1 tuple is a list, a set of stored values, but with the difference that a tuple can not be modified as in a list.\n  the interest of a tuple is what is stored in a tuple will never be modifiable\n  We write all the elements of a tuple by separating them with commas and all surrounded by parentheses: - my_tuple = (,,,,)\n  We will use a tuple to define some kinds of constants that are not intended to change\ntuple1=(1,2,3,4,5) tuple1 (1, 2, 3, 4, 5)  tuple1[0]=0 # we can\u0026#39;t change a tuple --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-60-c1b0272b0229\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 tuple1[0]=0 # we can't change a tuple TypeError: 'tuple' object does not support item assignment ----------------------------------------------------------------------------  tuple1[1] 2  - Exercice on lists:  1- Create a list named \u0026quot;my_notes\u0026quot; which contains the following numbers: 19,7,15,9,10,6,18,10,16,14,13,10,2,20,17,8,12,10,11,4 2- Calculate the overall average of this class of 20 students and put the result in a variable called \u0026quot;general_note\u0026quot; 3- Find the lowest score and the highest score 4- Sort the list of notes from largest to smallest 5- Replace note 2 by 6 6- Count the number of notes equal to 10 in the class 7- Count the number of notes equal to 10 in the class 8- Find the number of students with a grade\u0026gt; 10  1- Create a list named \u0026ldquo;my_notes\u0026rdquo; which contains the following numbers\nmes_notes=[19,7,15,9,10,6,18,10,16,14,13,10,2,20,17,8,12,10,11,4] mes_notes [19, 7, 15, 9, 10, 6, 18, 10, 16, 14, 13, 10, 2, 20, 17, 8, 12, 10, 11, 4]  2 - Calculate the overall average of this class of 20 students and put the result in a variable called \u0026ldquo;general_note\u0026rdquo;\nimport numpy moyenne_generale=numpy.mean(mes_notes) moyenne_generale 11.55  3- Find the lowest score and the highest score\nmin(mes_notes) 2  max(mes_notes) 20  4- Sort the list of notes from largest to smallest - method 1 :\nmes_notes.sort() mes_notes [2, 4, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]  mes_notes.reverse() mes_notes [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 10, 10, 10, 9, 8, 7, 6, 4, 2]   method 2: using .sort() method  help(mes_notes.sort) Help on built-in function sort: sort(*, key=None, reverse=False) method of builtins.list instance Stable sort *IN PLACE*.  mes_notes.sort(reverse=True) mes_notes [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 10, 10, 10, 9, 8, 7, 6, 4, 2]  5- Replace note 2 by 6\nmes_notes[19]=6 mes_notes [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 10, 10, 10, 9, 8, 7, 6, 4, 6]  6- Count the number of notes equal to 10 in the class? : method count()\nmes_notes.count(10) 4  7- Count the number of notes equal to 10 in the class\nmes_notes_array=numpy.asarray(mes_notes) mes_notes_array array([20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 10, 10, 10, 9, 8, 7, 6, 4, 6])  mes_notes_array[mes_notes_array\u0026gt;10] # affichage des notes supérieures à 10  array([20, 19, 18, 17, 16, 15, 14, 13, 12, 11])  mes_notes_array\u0026gt;10 # cette opération est un masque avec des booléens array([ True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False])  len(mes_notes_array[mes_notes_array\u0026gt;10]) 10  mes_notes_array[mes_notes_array\u0026gt;10] array([20, 19, 18, 17, 16, 15, 14, 13, 12, 11])  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ac42c0e67ba065a88d6a128f0baedba8","permalink":"/courses/tutorial_python/2-lists_in_python/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/2-lists_in_python/","section":"courses","summary":"Python includes several built-in container types, but the most common ones are lists and tuples, which we would see in this tutorial.\nThere are certain things you can do with all container types. These operations include indexing, slicing, adding, multiplying. In addition, Python has built-in functions like finding the length of a sequence, finding its largest and smallest elements\u0026hellip;\nEach element of a sequence is assigned by a number - its position or index.","tags":null,"title":"2 List in Python","type":"docs"},{"authors":null,"categories":null,"content":"3.1 Flow control with the conditional structure: If, else The flow control statements can be classified into Conditional Statements and Iteration Statements.\n  The Conditional Statements selects a particular set of statements for execution depending upon a specified condition. The most popular conditional control statement is ‘if else’, so let’s see how it works in python.:\n Syntax IF: if condition: indentation ---\u0026gt; action 1 elif: indentation ---\u0026gt; action 2 else: indentation ---\u0026gt; action 3    WARNING: python works in indentation   Multiple tasks can be assigned in conditional structures.\n  my_variable = 5 if (my_variable \u0026gt; 5 ): my_variable = my_variable + 1 my_variable 5  if (my_variable \u0026gt; 5 ): my_variable = my_variable + 1 else: my_variable = my_variable - 1 my_variable 4  if (my_variable \u0026gt; 5 ): my_variable = my_variable + 1 elif (my_variable == 5) : my_variable = my_variable * 10 else: my_variable = my_variable - 1 my_variable 3  3.2 Flow control: Loop for: A loop allows to repeat instructions according to your needs.\n Syntax FOR: items = [1,2,3] for i in items: indentation ---\u0026gt; print (i) # 1,2,3  Be careful once again to respect the indentation.\nmy_list=[1,2,3,4,5,6,7,8,9,10] for value in my_list: print(value*2) print(\u0026#34;---\u0026#34;) 2 --- 4 --- 6 --- 8 --- 10 --- 12 --- 14 --- 16 --- 18 --- 20 ---  for value in my_list: my_new_value=value*2 print(\u0026#34;The multiplication of 2 * %d= %d\u0026#34; % (value,my_new_value)) # to concatenate text and a variable #% d: python knows we\u0026#39;ll display an integer #% s: python knows we\u0026#39;ll display a string #% f: python knows we will display a float # we give a tuple to display The multiplication of 2 * 1 = 2 The multiplication of 2 * 2 = 4 The multiplication of 2 * 3 = 6 The multiplication of 2 * 4 = 8 The multiplication of 2 * 5 = 10 The multiplication of 2 * 6 = 12 The multiplication of 2 * 7 = 14 The multiplication of 2 * 8 = 16 The multiplication of 2 * 9 = 18 The multiplication of 2 * 10 = 20  3.3 Flow control: while() loop , same as if structure but repeated  Syntaxe WHILE: run = True while run: indentation ---\u0026gt; print('running') if \u0026lt;condition\u0026gt;: run = False  As long as the condition is met, the iteration in the loop continues. Be careful to respect the indentation.\nmy_list=[1,2,3,4,5,6,7,8,9,10] counter=0 while (counter \u0026lt; 10): print(\u0026#34;My counter = %d\u0026#34; % (counter)) print(\u0026#34;My value = %d\u0026#34; % (my_list[counter])) # in a tuple, we want the value from our list counter=counter+1 My counter = 0 My value = 1 My counter = 1 My value = 2 My counter = 2 My value = 3 My counter = 3 My value = 4 My counter = 4 My value = 5 My counter = 5 My value = 6 My counter = 6 My value = 7 My counter = 7 My value = 8 My counter = 8 My value = 9 My counter = 9 My value = 10  # we can combine several conditions in a while loop # we will for example extract only the even values # If modulo value = 0, even number counter=0 counter_true_result=0 while (counter \u0026lt; 10): if my_list[counter] % 2 == 0: print(\u0026#34;My counter = %d\u0026#34; % (counter)) print(\u0026#34;My value = %d\u0026#34; % (my_list[counter])) counter_true_result=counter_true_result+1 counter=counter+1 print(counter_true_result) My counter = 1 My value = 2 My counter = 3 My value = 4 My counter = 5 My value = 6 My counter = 7 My value = 8 My counter = 9 My value = 10 5  3.4 Loop range It's possible to create a loop with range() function:\nfor my_value in range(0,10): print(my_value*2) print(\u0026#34;---\u0026#34;) 0 --- 2 --- 4 --- 6 --- 8 --- 10 --- 12 --- 14 --- 16 --- 18 ---  3.5 To stop a loop: It's possible to stop a loop with break command.\nmy_list=[1,2,3,4,5,6,7,8,9,10] for i in my_list: if i \u0026gt; 5: print(\u0026#34;stop: the following values are greater than 5.\u0026#34;) break print(i) 1 2 3 4 5 stop: the following values are greater than 5.  # to import a library import numpy as np dir(np) The help()function provides help on a function of the library.\n - Example: help(numpy.mean)  help (np.median) # for help on a library function # we can distinguish between optional and mandatory arguments Help on function median in module numpy: median(a, axis=None, out=None, overwrite_input=False, keepdims=False) Compute the median along the specified axis. Returns the median of the array elements. Parameters ---------- a : array_like Input array or object that can be converted to an array. axis : {int, sequence of int, None}, optional Axis or axes along which the medians are computed. The default is to compute the median along a flattened version of the array. A sequence of axes is supported since version 1.9.0. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary. overwrite_input : bool, optional If True, then allow use of memory of input array `a` for calculations. The input array will be modified by the call to `median`. This will save memory when you do not need to preserve the contents of the input array. Treat the input as undefined, but it will probably be fully or partially sorted. Default is False. If `overwrite_input` is ``True`` and `a` is not already an `ndarray`, an error will be raised. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `arr`. .. versionadded:: 1.9.0 Returns ------- median : ndarray A new array holding the result. If the input contains integers or floats smaller than ``float64``, then the output data-type is ``np.float64``. Otherwise, the data-type of the output is the same as that of the input. If `out` is specified, that array is returned instead. See Also -------- mean, percentile Notes ----- Given a vector ``V`` of length ``N``, the median of ``V`` is the middle value of a sorted copy of ``V``, ``V_sorted`` - i e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the two middle values of ``V_sorted`` when ``N`` is even. Examples -------- \u0026gt;\u0026gt;\u0026gt; a = np.array([[10, 7, 4], [3, 2, 1]]) \u0026gt;\u0026gt;\u0026gt; a array([[10, 7, 4], [ 3, 2, 1]]) \u0026gt;\u0026gt;\u0026gt; np.median(a) 3.5 \u0026gt;\u0026gt;\u0026gt; np.median(a, axis=0) array([ 6.5, 4.5, 2.5]) \u0026gt;\u0026gt;\u0026gt; np.median(a, axis=1) array([ 7., 2.]) \u0026gt;\u0026gt;\u0026gt; m = np.median(a, axis=0) \u0026gt;\u0026gt;\u0026gt; out = np.zeros_like(m) \u0026gt;\u0026gt;\u0026gt; np.median(a, axis=0, out=m) array([ 6.5, 4.5, 2.5]) \u0026gt;\u0026gt;\u0026gt; m array([ 6.5, 4.5, 2.5]) \u0026gt;\u0026gt;\u0026gt; b = a.copy() \u0026gt;\u0026gt;\u0026gt; np.median(b, axis=1, overwrite_input=True) array([ 7., 2.]) \u0026gt;\u0026gt;\u0026gt; assert not np.all(a==b) \u0026gt;\u0026gt;\u0026gt; b = a.copy() \u0026gt;\u0026gt;\u0026gt; np.median(b, axis=None, overwrite_input=True) 3.5 \u0026gt;\u0026gt;\u0026gt; assert not np.all(a==b)  - Exercise Objective: To manipulate a list containing the prices of 58 houses Creation of the list \u0026ldquo;price_of_58_houses\u0026rdquo;\nprice_of_58_houses=list(range(125000,700000,10000)) price_of_58_houses[0:20] [125000, 135000, 145000, 155000, 165000, 175000, 185000, 195000, 205000, 215000, 225000, 235000, 245000, 255000, 265000, 275000, 285000, 295000, 305000, 315000]  len(price_of_58_houses) 58 1- How many houses have a price greater than or equal to 300000 euros? 2- How many houses have a price between 250000 and 400000 euros? 3- How many houses have a price that is not higher than 600000 euros? 4- How many houses have a price lower than 150000 euros or more than 650000 euros?  Hint: Scroll through the list with a loop, use if condition statements, and a counter to count the true results.\n# 1.  nombre_maisons=0 for prix in price_of_58_houses: if prix \u0026gt;= 300000: nombre_maisons=nombre_maisons+1 print(\u0026#34;Number of house with price greater than or equal to 300000 euros : %d\u0026#34; % (nombre_maisons)) Number of house with price greater than or equal to 300000 euros : 40  # 2.  nombre_maisons=0 for prix in price_of_58_houses: if (prix \u0026gt;= 250000) and (prix \u0026lt;= 400000): compteur nombre_maisons=nombre_maisons+1 print(\u0026#34;The result is : %d\u0026#34; % (nombre_maisons)) The result is : 15  # 3. nombre_maisons=0 for prix in price_of_58_houses: if not(prix \u0026gt; 600000): nombre_maisons=nombre_maisons+1 print(\u0026#34;The result is : %d\u0026#34; % (nombre_maisons)) The result is : 48  # 4.  nombre_maisons=0 for prix in price_of_58_houses: if (prix \u0026lt; 150000) or (prix \u0026gt; 650000): nombre_maisons=nombre_maisons+1 print(\u0026#34;The result is : : %d\u0026#34; % (nombre_maisons)) The result is : 8\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"9d9c207528bfb3e6aaad7eb82af12d1a","permalink":"/courses/tutorial_python/3-flow_control/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/3-flow_control/","section":"courses","summary":"3.1 Flow control with the conditional structure: If, else The flow control statements can be classified into Conditional Statements and Iteration Statements.\n  The Conditional Statements selects a particular set of statements for execution depending upon a specified condition. The most popular conditional control statement is ‘if else’, so let’s see how it works in python.:\n Syntax IF: if condition: indentation ---\u0026gt; action 1 elif: indentation ---\u0026gt; action 2 else: indentation ---\u0026gt; action 3    WARNING: python works in indentation   Multiple tasks can be assigned in conditional structures.","tags":null,"title":"3 Flow control","type":"docs"},{"authors":null,"categories":null,"content":"4 Libraries and functions in Python 4.1 Introduction to librairies To facilitate the processing, manipulation and visualization of data, Python has many libraries.\nPython libraries allow you to import codes and functions that will make our analysis easier.\nHere are the most popular libraries:\n numpy: mathematics or scientific calculations: http://www.numpy.org/ pandas: data manipulation: http://pandas.pydata.org/ matplotlib: visualization of data: https://matplotlib.org/ scikit-learn: machine learning: https://scikit-learn.org Datetime: formatting dates: https: //docs.python.org/2/library/datetime.html  To import a library under Python, use the function: import + library_name\n# To import a library import numpy Every library imported under python has help.\nTo know what a library contains, we use the function: dir ()\n - Example: dir(numpy)  dir(numpy) The help () function provides help with a function in the library\n - Example: help(numpy.mean)  help(numpy.mean) Help on function mean in module numpy: mean(a, axis=None, dtype=None, out=None, keepdims=\u0026lt;no value\u0026gt;) Compute the arithmetic mean along the specified axis. Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. `float64` intermediate and return values are used for integer inputs. Parameters ---------- a : array_like Array containing numbers whose mean is desired. If `a` is not an array, a conversion is attempted. axis : None or int or tuple of ints, optional Axis or axes along which the means are computed. The default is to compute the mean of the flattened array. .. versionadded:: 1.7.0 If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before. dtype : data-type, optional Type to use in computing the mean. For integer inputs, the default is `float64`; for floating point inputs, it is the same as the input dtype. out : ndarray, optional Alternate output array in which to place the result. The default is ``None``; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See `doc.ufuncs` for details. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. If the default value is passed, then `keepdims` will not be passed through to the `mean` method of sub-classes of `ndarray`, however any non-default value will be. If the sub-class' method does not implement `keepdims` any exceptions will be raised. Returns ------- m : ndarray, see dtype parameter above If `out=None`, returns a new array containing the mean values, otherwise a reference to the output array is returned. See Also -------- average : Weighted average std, var, nanmean, nanstd, nanvar Notes ----- The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below). Specifying a higher-precision accumulator using the `dtype` keyword can alleviate this issue. By default, `float16` results are computed using `float32` intermediates for extra precision. Examples -------- \u0026gt;\u0026gt;\u0026gt; a = np.array([[1, 2], [3, 4]]) \u0026gt;\u0026gt;\u0026gt; np.mean(a) 2.5 \u0026gt;\u0026gt;\u0026gt; np.mean(a, axis=0) array([ 2., 3.]) \u0026gt;\u0026gt;\u0026gt; np.mean(a, axis=1) array([ 1.5, 3.5]) In single precision, `mean` can be inaccurate: \u0026gt;\u0026gt;\u0026gt; a = np.zeros((2, 512*512), dtype=np.float32) \u0026gt;\u0026gt;\u0026gt; a[0, :] = 1.0 \u0026gt;\u0026gt;\u0026gt; a[1, :] = 0.1 \u0026gt;\u0026gt;\u0026gt; np.mean(a) 0.54999924 Computing the mean in float64 is more accurate: \u0026gt;\u0026gt;\u0026gt; np.mean(a, dtype=np.float64) 0.55000000074505806  list_1=[1,2,3,4,5,6,7,8,9,10] list_1 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  numpy.mean(list_1) # name of library + name of function  5.5  numpy.max(list_1) 10  4.2 Built-in function Python gives you many built-in functions like print(), etc\nHere's a list: https://docs.python.org/3/library/functions.html\nSome examples: abs(-1) # Return the absolute value of a number  1  len([1,2,3]) # Return the length (the number of items) of an object.  3  max([1,3,2,6,99,1]) #Return max value from a list 99  round(1.2) # Return number rounded to ndigits precision  1  4.3 Create own functions. These functions are called user-defined functions.\n  a function makes our code more fluid and readable:\n  Syntax: to write a function, we start with the def() keyword\n def sum (a, b): # we define the parameters indentation ---\u0026gt; return a + b # return keyword to return result variable    It does not specify the type of return, it is dynamically resolved at the time of execution of the program.\n  Call of the function: result = sum (12,4) # result = 16\n  Python also supports keyword arguments.\n def sum (a, b): # we define the parameters return a + b # return keyword to return result variable result = sum (12, b = 4) ----------------------------- def sum (a, b = 4): # we define the parameters return a + b # return keyword to return result variable result = sum (12)  def sign(x): if x \u0026gt; 0: return \u0026#39;positive\u0026#39; elif x \u0026lt; 0: return \u0026#39;negative\u0026#39; else: return \u0026#39;zero\u0026#39; my_list_to_test = [0,2,-8,10,1,-6] for element in my_list_to_test: print(sign(element)) zero positive negative positive positive negative  We will often define functions to take optional keyword arguments, like this:\ndef hello(name, loud=False): if loud: print(\u0026#39;HELLO, %s\u0026#39; % name.upper()) else: print(\u0026#39;Hello, %s!\u0026#39; % name) hello(\u0026#39;Bob\u0026#39;) hello(\u0026#39;Fred\u0026#39;, loud=True) Hello, Bob! HELLO, FRED  4.4 Functions with *args  Special syntax in python that allows to manage a variable number of parameters when calling a function It's the * that counts args is a convention   args is a list of parameters containing the parameters of a function    Syntax * args:\n def print_ingredients (* args): for ingredients in args: print (ingredient) print_ingredients ( 'Tomatoes') print_ingredients ( 'Tomatoes' Banana) print_ingredients ( 'Tomatoes' Banana, apple)  def sum(*args): total = 0 for number in args: total += number print(total) sum(2,3) sum(2,3,10,90,23) 5 128  4.5 Functions with **kwargs  Like * args, but for keyword arguments It's the * that counts ** kwargs is a python dictionary containing the keys / values ​​of the parameters of a function it must always be present last in the list of parameters of a function  Syntax ** kwargs:\n def print_languages ​​(** args): for language, definition in kwargs.items (): print ('{} is {}'. format (language, definition)) print_languages ​​(Python = 'awesome') print_languages ​​(Python = 'awesome', Java = 'verbose')  def capitals(**kwargs): for country, capital in kwargs.items(): print(\u0026#34;The capital of {} in {}\u0026#34;.format(country, capital)) capitals(France = \u0026#39;Paris\u0026#39;, Germany=\u0026#39;Berlin\u0026#39;) The capital of France in Paris The capital of Germany in Berlin  def capitals(title, ending=\u0026#39;\u0026#39;, **kwargs): print(title) for country, capital in kwargs.items(): print(\u0026#34;The capital of {} in {}\u0026#34;.format(country, capital)) if ending: print(ending) capitals(\u0026#34;List of countries\u0026#34;, France = \u0026#39;Paris\u0026#39;, Germany=\u0026#39;Berlin\u0026#39;) List of countries The capital of France in Paris The capital of Germany in Berlin  keywords = {\u0026#39;france\u0026#39;: \u0026#39;Paris\u0026#39;, \u0026#39;Germany\u0026#39; : \u0026#39;Allemage\u0026#39;} capitals(\u0026#34;List of countries 2 \u0026#34;, **keywords) List of countries 2 The capital of france in Paris The capital of Germany in Allemage  4.6 Import own functions Modules refer to a file containing Python statements and definitions.\nA file containing Python code, for e.g.: example.py, is called a module and its module name would be example.\nWe use modules to break down large programs into small manageable and organized files. Furthermore, modules provide reusability of code.\nWe can define our most used functions in a module and import it, instead of copying their definitions into different programs.\nHere, we have defined different funcitons inside a module named my_functions.\nWe use the import keyword to import our module and then our functions. To import our previously defined module example we type the following in the Python prompt:\nimport my_functions Using the module name we can access the function using the dot . operator. For example:\nmy_functions.substraction(4,6) -2  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"30ee55255960cecb37c5fffee543c4b3","permalink":"/courses/tutorial_python/4-functions/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/4-functions/","section":"courses","summary":"4 Libraries and functions in Python 4.1 Introduction to librairies To facilitate the processing, manipulation and visualization of data, Python has many libraries.\nPython libraries allow you to import codes and functions that will make our analysis easier.\nHere are the most popular libraries:\n numpy: mathematics or scientific calculations: http://www.numpy.org/ pandas: data manipulation: http://pandas.pydata.org/ matplotlib: visualization of data: https://matplotlib.org/ scikit-learn: machine learning: https://scikit-learn.org Datetime: formatting dates: https: //docs.python.org/2/library/datetime.html  To import a library under Python, use the function: import + library_name","tags":null,"title":"4 Functions","type":"docs"},{"authors":null,"categories":null,"content":" The dictionary stores (key, value) pairs They are unordered data structures Principle: we can link a key to a value The values of a dictionary can be of any type, but the keys must be of an immutable data type such as strings, numbers, or tuples. Keys are unique within a dictionary while values may not be. Each key is separated from its value by a colon (:), the items are separated by commas, and the whole thing is enclosed in curly braces. An empty dictionary without any items is written with just two curly braces, like this: {}. Dictionary values have no restrictions. They can be any arbitrary Python object, either standard objects or user-defined objects. However, same is not true for the keys. Keys must be immutable. Which means you can use strings, numbers or tuples as dictionary keys but something like [\u0026lsquo;key\u0026rsquo;] is not allowed.  You can find all you need to know about dictionaries in the documentation: https://docs.python.org/2/library/stdtypes.html#dict\n5.1 Create a dictionary: In a dictionary, each key is separated from its value by a colon (:), the items are separated by commas, and the whole thing is enclosed in curly braces like this: {}.\n# exemple : students\u0026#39; note dictionary my_dictionary = { \u0026#34;Marie\u0026#34; : 15, \u0026#34;Thomas\u0026#34; : 12, \u0026#34;Julien\u0026#34; : \u0026#34;absent\u0026#34;, \u0026#34;Elise\u0026#34; : 9, \u0026#34;Samuel\u0026#34; : 17 } my_dictionary # We have the key : the value  {'Marie': 15, 'Thomas': 12, 'Julien': 'absent', 'Elise': 9, 'Samuel': 17}  5.2 Update a dictionary: You can update a dictionary by adding a new entry or a key-value pair, modifying an existing entry, or deleting an existing entry as shown below.\n To addkey-value pair  # Adding a key-value pair in a dictionary my_dictionary[\u0026#34;Julie\u0026#34;]=9 print(my_dictionary) {'Marie': 15, 'Thomas': 12, 'Julien': 'absent', 'Elise': 9, 'Samuel': 17, 'Julie': 9}   To changea value using existing key  my_dictionary[\u0026#34;Julien\u0026#34;]=13 print(my_dictionary) {'Marie': 15, 'Thomas': 12, 'Julien': 13, 'Elise': 9, 'Samuel': 17, 'Julie': 9}  More than one entry per key not allowed. Which means no duplicate key is allowed. When duplicate keys encountered during assignment, the last assignment wins.\ndict = {\u0026#39;Name\u0026#39;: \u0026#39;Zara\u0026#39;, \u0026#39;Age\u0026#39;: 7, \u0026#39;Name\u0026#39;: \u0026#39;Manni\u0026#39;} dict {'Name': 'Manni', 'Age': 7}   To deletea value using a key  You can either remove individual dictionary elements or clear the entire contents of a dictionary. You can also delete entire dictionary in a single operation.\nTo explicitly remove an entire dictionary, just use the del statement. Following is a simple example −\ndel my_dictionary[\u0026#34;Julie\u0026#34;] # remove entry with key \u0026#39;Name\u0026#39; my_dictionary.clear() # remove all entries in my_dictionary del my_dictionary # delete entire dictionary print(my_dictionary) --------------------------------------------------------------------------- NameError Traceback (most recent call last) \u0026lt;ipython-input-7-04bcd56b6daf\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 print(my_dictionary) NameError: name 'my_dictionary' is not defined ---------------------------------------------------------------------------  5.3 Access a dictionary: To access dictionary elements, you can use the familiar square brackets along with the key to obtain its value.\nmy_dictionary = { \u0026#34;Marie\u0026#34; : 15, \u0026#34;Thomas\u0026#34; : 12, \u0026#34;Julien\u0026#34; : \u0026#34;absent\u0026#34;, \u0026#34;Elise\u0026#34; : 9, \u0026#34;Samuel\u0026#34; : 17 } my_dictionary[\u0026#34;Samuel\u0026#34;] 17   We can access ditionary elements using FORloop:  by value valeur by key by key-value pair    # .keys() method returns list of dictionary dict\u0026#39;s keys: my_dictionary.keys() dict_keys(['Marie', 'Thomas', 'Julien', 'Elise', 'Samuel'])  # .values() method returns list of dictionary dict\u0026#39;s values my_dictionary.values() dict_values([15, 12, 'absent', 9, 17])  # Example using for loop and keys dictionary for key in my_dictionary.keys(): print(key) Marie Thomas Julien Elise Samuel  for value in my_dictionary.values(): print(value) 15 12 absent 9 17  # .items() method returns a list of dict\u0026#39;s (key, value) tuple pairs my_dictionary.items() dict_items([('Marie', 15), ('Thomas', 12), ('Julien', 'absent'), ('Elise', 9), ('Samuel', 17)])  # Example using for loop and key-value pairs in dictionary for key,value in my_dictionary.items(): # la boucle for obtient un tuple à chaque itération if value == \u0026#39;absent\u0026#39;: print(\u0026#39;Absent\u0026#39;) else: print(\u0026#39;The average of %sis %s/20\u0026#39; % (key, value)) The average of Marie is 15 /20 The average of Thomas is 12 /20 Absent The average of Elise is 9 /20 The average of Samuel is 17 /20  Exercise on dictionaries Goal: To manipulate a dictionary containing the notes of 15 students. 1- Create a dictionary named notes_levels containing the following notes: Mary: 15; Samuel: 17; Gaston: 12; Fred: 10; Mae: 5; Julie: 15; Zoe: 7; Claire: 20; Chloe: 8; Julian: 14, Gael: 9, Samia: 15, Omar: 11, Gabriel: 16, Manon: 2 2- What is the average of the class? 3- Display the total number of students in the class. 4- How many students have a grade strictly above average? 5- What is the name of the best student in the class? 6- How many students have a first name with strictly less than 4 letters? 7- Show the first name of the pupils who have an even note (multiple of 2).!  Correction  1:  notes_levels ={ \u0026#34;Marie\u0026#34; : 15, \u0026#34;Samuel\u0026#34; : 17, \u0026#34;Gaston\u0026#34; : 12, \u0026#34;Fred\u0026#34; : 10, \u0026#34;Mae\u0026#34; : 5, \u0026#34;Julie\u0026#34; : 15, \u0026#34;Zoe\u0026#34; : 7, \u0026#34;Claire\u0026#34; : 20, \u0026#34;Chloe\u0026#34; : 8, \u0026#34;Julien\u0026#34; : 14, \u0026#34;Gaël\u0026#34; : 9, \u0026#34;Samia\u0026#34; : 15, \u0026#34;Omar\u0026#34; : 11, \u0026#34;Gabriel\u0026#34; : 16, \u0026#34;Manon\u0026#34; : 2 }  2:  notes_eleves.values() dict_values([15, 17, 12, 10, 5, 15, 7, 20, 8, 14, 9, 15, 11, 16, 2])  import numpy average_note =numpy.mean(list(notes_eleves.values())) `` ```python average_note 11.733333333333333   3:  nombre_eleves=len(notes_eleves) print(\u0026#34;Le nombre d\u0026#39;élèves dans la classe est de %d\u0026#34; % (nombre_eleves)) Le nombre d'élèves dans la classe est de 15  len(notes_eleves.keys()) 15   4:  nombre_eleves_avec_note_sup_moyenne=0 for valeur in notes_eleves.values(): if valeur \u0026gt; moyenne_generale: nombre_eleves_avec_note_sup_moyenne=nombre_eleves_avec_note_sup_moyenne+1 print(\u0026#34;Le nombre d\u0026#39;élèves avec une note supérieure à %.2fest de %délèves\u0026#34; % (moyenne_generale, nombre_eleves_avec_note_sup_moyenne)) Le nombre d'élèves avec une note supérieure à 11.73 est de 8 élèves   5:  # On va d\u0026#39;abord déterminer la meilleure note: utilisation de la fonction max() meilleure_note=max(notes_eleves.values()) ```python meilleure_note 20  # On va parcourir notre dictionnaire et trouver la clef associée à notre valeur # Pour cela on va travailler sur les tuples avec la méthode item() for prenom,note in notes_eleves.items(): if note == meilleure_note: print(prenom) Claire   6:  # On va parcourir les clefs de notre dictionnaire et mettre une condition sur la longueur de chaque clef nombre_eleves=0 for prenom in notes_eleves.keys(): if len(prenom) \u0026lt; 4: nombre_eleves=nombre_eleves+1 print(nombre_eleves) 2   7:  # On va parcourir les tuples du dictionnaire et mettre une condition sur les valeurs  for prenom,note in notes_eleves.items(): if note % 2 == 0: print(prenom, note) Gaston 12 Fred 10 Claire 20 Chloe 8 Julien 14 Gabriel 16 Manon 2  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ceaee89e50e6b119c01402939535006b","permalink":"/courses/tutorial_python/5-dictionaries/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/5-dictionaries/","section":"courses","summary":"The dictionary stores (key, value) pairs They are unordered data structures Principle: we can link a key to a value The values of a dictionary can be of any type, but the keys must be of an immutable data type such as strings, numbers, or tuples. Keys are unique within a dictionary while values may not be. Each key is separated from its value by a colon (:), the items are separated by commas, and the whole thing is enclosed in curly braces.","tags":null,"title":"5 Dictionaries","type":"docs"},{"authors":null,"categories":null,"content":" Numpy is the core library for scientific computing in Python A new data container will be used: the ndarray (N-dimensional array). There are vectors (one-dimensional arrays), multidimensional arrays It provides a high-performance multidimensional array object, and tools for working with these arrays We'll see how to initialize Numpy arrays in several ways, access values in arrays, perform math and matrix operations, and use arrays for both masking and comparisons.  https://docs.scipy.org/doc/numpy/reference/\n6.1- Create a Numpy array or ndarray We first need to import the numpy package:\n# We will import Numpy library and create alias.  # Codes with aliases are easier to write and read.  import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.\nUnlike a list, you can not create empty Numpy tables. You will find below several ways to initialize a Numpy table according to your needs:\n Using array()function to create Numpy array:  table1 = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]) table1 array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])  Creation of a two-dimensional array with rows and columns; we create a list of lists. Each list is a row of the table.\n# 2 rows and 3 columns table2 = np.array([[1,2,3], [4,5,6]]) table2 array([[1, 2, 3], [4, 5, 6]])  # 3 rows and 3 columns table3 = np.array([[1,2,3], [4,5,6], [7,8,9]]) table3 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])   Using range()function to create Numpy array:  table4 = np.array(range(10)) # table with values from 0 to 9 table4 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])   Using zeros()function to create Numpy array with \u0026lsquo;0\u0026rsquo; value :  table5 = np.zeros((4,3)) # table with 4 rows and 3 columns table5 array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])   Using ones()function to create Numpy array with ones:  table6 = np.ones((4,3)) # table with 4 rows and 3 columns table6 array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])   Using identity()function to create Numpy array as matrix identity:  table7 = np.identity(4) # 4 dimensions table7 array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]])   Converting list to ndarray with array()function  my_list = [0,1,2,3,4,5,6] my_list [0, 1, 2, 3, 4, 5, 6]  table8 = np.array(my_list) table8 array([0, 1, 2, 3, 4, 5, 6])   Using random()function to create ndarray with random values:  table9 = np.random.randint(100,size=(4,3)) # 4*3 ndarray with random values between 0 and 100  table9 array([[78, 47, 23], [79, 17, 5], [61, 41, 71], [12, 41, 27]])   Using full()to create a constant array.  table10 = np.full((2,2), 7) # Create a constant array table10 array([[7, 7], [7, 7]])  6.2 Access data in a Numpy or ndarray array We can access an individual element or a slice of values. Similar to lists, the first element is indexed to 0. For example, array1 [0,0] indicates that we are accessing the first row and the first column. The first number of the tuple [0,0] indicates the index of the line and the second number indicates the index of the column:\nmy_table = np.array([[1,2,3], [4,5,6], [7,8,9]]) my_table array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])   Here are some examples to access values in a numpy array:  my_table[1,2] # We want the element located at row with index 1 and column with index 2 # ndarray(row,column)  6  my_table[1,-1] # We want the element at row index 1 and last column 6  my_table[0,1] # We want the element located at row with index 0 and column with index 1 2  my_table[1,0] # We want the element located at row with index 1 and column with index 0  4   Here are some examples to access data using Slicing  my_table[:,0] # We want all rows with column index 0 array([1, 4, 7])  my_table[0,:] # We want all columns with row index 0 array([1, 2, 3])  my_table[:,0:3:2] # We want all rows and index columns from 0 to 3 with steps of 2. # So all rows and columns 1 and 2. array([[1, 3], [4, 6], [7, 9]])  my_table[:,-1] #We want all rows at last column array([3, 6, 9])  my_table[:,1:-1] # We want all rows with columns between index 1 and last index array([[2], [5], [8]])  my_table[2,1:-1] # We want values at row with index 2 and rows between index 1 and last index array([8])  6.3 Mathematical and matrix calculations on a Numpy array: Numpy tables are very easy to manipulate: concatenate, add, multiply, transpose with a single line of code. Below you will find some examples of various arithmetic and multiplicative operations with Numpy tables.\narray1 = np.arange(9).reshape(3,3) array1 array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])  array2 = np.random.randint(50, size=(3,3)) array2 array([[47, 20, 49], [40, 15, 7], [ 4, 35, 14]])   Basic arithmetic operations: addition, subtraction, multiplication, division Here a list of mathematical functions provided by numpy in: https://docs.scipy.org/doc/numpy/reference/routines.math.html  array1 + 10 # add a value to each element array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])  array1 - 10 # substract value to each element  array([[-10, -9, -8], [ -7, -6, -5], [ -4, -3, -2]])  array1 * 100 # multiply value to each element  array([[ 0, 100, 200], [300, 400, 500], [600, 700, 800]])  array1[:,0] * 10 # we multiply by 10 all elements at column with index 0 array([ 0, 30, 60])  array1 / 2 # we divide a value to each element array([[0. , 0.5, 1. ], [1.5, 2. , 2.5], [3. , 3.5, 4. ]])   some Numpy functions and methods applicable on Numpy tables:  all,any,apply_along_axis,argmax,argmin,argsort,average,bincount,ceil,clip,conj,corrcoef,cov,crosscumprod,cumsum,diff,dot,floor,inner,lexsort,max,maximum,mean,median,min,minimum,nonzero,outer,re,round,sort,std,sum,trace,transpose,var,vdotvectorize,wherenp.add(array1,10) # add a value to each element array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])  np.subtract(array1,10) # substract a value to each element  array([[-10, -9, -8], [ -7, -6, -5], [ -4, -3, -2]])  np.multiply(array1,100) # multiply a value to each element array([[ 0, 100, 200], [300, 400, 500], [600, 700, 800]])  np.divide(array1, 2) # divide a value to each element array([[0. , 0.5, 1. ], [1.5, 2. , 2.5], [3. , 3.5, 4. ]])  np.mean(array1) # computing the Numpy table average using the mean() function 4.0  array1.mean() # computing the Numpy table average using the mean() method 4.0  array1.min() # computing the Numpy table mimimum using the min() method 0  array1.max() # computing the Numpy table maximum using the max() method 8  np.mean(array1, axis=0) # we apply mean() function only over columns  array([3., 4., 5.])  np.mean(array1, axis=1) # we apply mean() function only over rows  array([1., 4., 7.])   Operations between several tables  (array1 +1) * array2 # multiplication of 2 tables array([[ 47, 40, 147], [160, 75, 42], [ 28, 280, 126]])  array1 + array2 # sum of 2 tables array([[47, 21, 51], [43, 19, 12], [10, 42, 22]])  np.dot(array1, array2) # dot product of 2 tables array([[ 48, 85, 35], [321, 295, 245], [594, 505, 455]])  6.4 Update Numpy array: Other interesting features include concatenation, splitting, transposition (changing elements from one row to another and vice versa) and obtaining elements diagonally.\na) - To manipulate / modify the dimensions of a Numpy array: The dimension of a table is given by the number of elements following each axis. We have specific methods and attributes specific to ndarray ():\nnp.floor(10*np.random.random((3,4))) array([[1., 5., 5., 0.], [5., 5., 4., 4.], [6., 2., 5., 2.]])  a = np.floor(10*np.random.random((3,4))) print(a, a.shape, a.ndim) [[3. 5. 5. 6.] [0. 7. 1. 6.] [4. 8. 2. 5.]] (3, 4) 2  Depending on our programming needs, we can change the size of a table.\na.ravel() # ravel() function to write over table on 1 dimension (flattened) array([3., 5., 5., 6., 0., 7., 1., 6., 4., 8., 2., 5.])  a.reshape(6,2) # reshape() function to change the dimension of our array array([[3., 5.], [5., 6.], [0., 7.], [1., 6.], [4., 8.], [2., 5.]])  a.T #T method to calculate the transpose of our array. array([[3., 0., 4.], [5., 7., 8.], [5., 1., 2.], [6., 6., 5.]])  print(a.T.shape, a.shape) (4, 3) (3, 4)  The reshape function returns its argument with a modified form, while the ndarray.resize method modifies the array itself:\na.resize((2,6)) a array([[3., 5., 5., 6., 0., 7.], [1., 6., 4., 8., 2., 5.]])  If a dimension is set to -1 in a resizing operation, the other dimensions are automatically calculated:\na.reshape(4,-1) array([[3., 5., 5.], [6., 0., 7.], [1., 6., 4.], [8., 2., 5.]])  b) - Working with a subset of a Numpy table: my_table = np.array([[1,2,3], [4,5,6], [7,8,9]]) my_table array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  We will select a subset of our ndarray with selecting the first row.\nsubset = my_table[0] subset array([1, 2, 3])  We want to change the first element of our subset.\nsubset[0] = 100 subset array([100, 2, 3])  But by modifying our sub-table, we realize that we have modified our initial table.\nmy_table array([[100, 2, 3], [ 4, 5, 6], [ 7, 8, 9]])  A modification of the subset causes a modification of the initial table. Our subset array is a view of our initial table. Reason: saving memory when working with large volumes of data.\nIf you really want to work with a subset without modifying the original array, you must make a copy with the copy () \u0026lt;/ b\u0026gt; function.\nmy_table = np.array([[1,2,3], [4,5,6], [7,8,9]]) subset = my_table[0].copy() subset[0]=101 subset array([101, 2, 3])  my_table array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  c)- To concatenate Numpy array: vstack, column_stack, concatenate concatenate()function to join a sequence of arrays along an existing axis.  Concatenate function can take two or more arrays of the same shape and by default it concatenates row-wise i.e. axis=0.\narray1=np.array([[1,2,3],[4,5,6],[7,8,9]]) array1 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  array2=np.array([[2,5,6],[9,10,11],[5,6,9]]) array2 array([[ 2, 5, 6], [ 9, 10, 11], [ 5, 6, 9]])  np.concatenate([array1,array2], axis=0) # to join a sequence of arrays along rows (axis = 0)  array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [ 2, 5, 6], [ 9, 10, 11], [ 5, 6, 9]])  np.concatenate([array1,array2], axis=1) # to join a sequence of arrays along columns (axis = 1)  array([[ 1, 2, 3, 2, 5, 6], [ 4, 5, 6, 9, 10, 11], [ 7, 8, 9, 5, 6, 9]])  In addition to the concatenate function, NumPy also offers two convenient functions hstack and vstack to stack/combine arrays horizontally or vertically.\n vstack()function stacks arrays in sequence vertically i.e. row wise. And the result is the same as using concatenate with axis=0. hstack()function stacks arrays horizontally i.e. column wise. And the result is the same as using concatenate with axis=1.  array3=np.array([10,20,30]) # 1D Numpy array array3 array([10, 20, 30])  array4=np.array([[1,2,3],[4,5,6],[7,8,9]]) # bi-dimentionnal Numpy array array4 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  np.vstack([array4,array3]) array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 20, 30]])  array5=np.array([[10],[20],[30]]) array5.shape (3, 1)  np.hstack([array4,array5]) array([[ 1, 2, 3, 10], [ 4, 5, 6, 20], [ 7, 8, 9, 30]])  d)- To split Numpy arrays: This is the opposite of concatenation. We have the split (), hsplit () and vsplit () functions.\narray=np.array([15,16,17,12,49,52,12,14,36]) len(array) # array size 9  np.split(array,3) # we split the array into 3 arrays [array([15, 16, 17]), array([12, 49, 52]), array([12, 14, 36])]  We can split our numpy array using breaking point with index.\nnp.split(array,[2,6]) # we want to cut our table into 3 tables, the numbers between [] are the breakpoints. # corresponds to the indexes where we cut [array([15, 16]), array([17, 12, 49, 52]), array([12, 14, 36])]  array1,array2,array3=np.split(array,[2,6]) print(array1,array2,array3) [15 16] [17 12 49 52] [12 14 36]  array1,array2,array3,array4=np.split(array,[2,4,6]) print(array1,array2,array3,array4) [15 16] [17 12] [49 52] [12 14 36]  To break 2-dimensional tables. we use hsplit or vsplit.\narray2=np.array([[1,2,3],[4,5,6],[7,8,9]]) array2 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  array1,array1bis=np.vsplit(array2, [2]) print(array1,array1bis) [[1 2 3] [4 5 6]] [[7 8 9]]  array2 array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  array1,array1bis=np.hsplit(array2,[2]) print(array1,array1bis) [[1 2] [4 5] [7 8]] [[3] [6] [9]]  e)- To delete rows and columns in a Numpy array: delete()function array=np.array([[1,2,3],[4,5,6],[7,8,9]]) array array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  np.delete(array,2,axis=0) # to delete row with index 2  array([[1, 2, 3], [4, 5, 6]])  np.delete(array2,2,axis=1) # to delete column with index 2  array([[1, 2], [4, 5], [7, 8]])  f)- To calculate the transpose of a Numpy table: transpose()function np.transpose(array) array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])  g)- To get the values of the diagonal of a Numpy table: diagonal()function array.diagonal() array([1, 5, 9])  6.5 Comparisons and masks : With Numpy arrays, you can use a boolean matrix to filter and compare Numpy arrays.\ntable = np.random.randint(100,size=(6,6)) # 4*3 Numpy array with random values between 0 and 100  table array([[62, 72, 26, 37, 76, 39], [38, 66, 42, 78, 15, 97], [81, 19, 83, 89, 87, 66], [11, 49, 53, 71, 63, 25], [75, 87, 68, 88, 30, 12], [95, 61, 14, 40, 61, 31]])  mask = table\u0026gt;50 # we generate here a mask with same dimension than our original table but with Boolean values mask array([[ True, True, False, False, True, False], [False, True, False, True, False, True], [ True, False, True, True, True, True], [False, False, True, True, True, False], [ True, True, True, True, False, False], [ True, True, False, False, True, False]])  Being both of the same size, we can use this Boolean matrix to our advantage. In other words, we can do Boolean masking. With this Boolean matrix as a mask, we can use it to select the particular subset of data that interests us.\ntable[mask] #table[table\u0026gt;50] # same job array([62, 72, 76, 66, 78, 97, 81, 83, 89, 87, 66, 53, 71, 63, 75, 87, 68, 88, 95, 61, 61])  table[table\u0026gt;50] array([62, 72, 76, 66, 78, 97, 81, 83, 89, 87, 66, 53, 71, 63, 75, 87, 68, 88, 95, 61, 61])  We have many other comparison operators to compare two arrays such as == (equality),! = (No equality), \u0026lt;= (less than or equal to). We can even combine two Boolean statements \u0026amp; (for \u0026ldquo;AND\u0026rdquo; conditions) or | (for the \u0026ldquo;OR\u0026rdquo; conditions).\n#table[table\u0026gt;=50]  #table[table\u0026lt;50]  #table[table!=50]  #table[table==50]  #table[(table \u0026gt;=50) \u0026amp; (table \u0026lt;=70)]  table[(table\u0026gt;=50) | (table\u0026lt;=40)] array([62, 72, 26, 37, 76, 39, 38, 66, 78, 15, 97, 81, 19, 83, 89, 87, 66, 11, 53, 71, 63, 25, 75, 87, 68, 88, 30, 12, 95, 61, 14, 40, 61, 31])  Exercise using Numpy library: we will work on data station using Numpy library.  read the file containing the daily precipitation and temperature data for the Ottawa station for the year 2017  1- Create a Numpy table with a column for temperature and a column for precipitation. (With two Numpy 1D tables, create a 2D array (365 rows and 2 columns).\n2- Convert the temperature data into Celcius (T [Celcius] = T [Kelvin] - 273.15).\n3- How many days have an accumulation greater than 25mm?\n4- What temperature was recorded for the day with the greatest accumulation?\n5- Calculate the number of degree days (\u0026gt; 0degC) for the year 2017.\n6- Calculate the daily precipitation totals for the year 2017 and assign this variable to the cumul_recipitation table. Add the cumul_recipitation table to the table.\n7- Just for the exercise, split the array into 2 arrays, then concatenate them again to get the initial array.\nimport numpy as np # to read a csv file with numpy, we can use genfromtxt() function.  temperature = np.genfromtxt(\u0026#34;./DATA/OTTAWA_tasmoy_2017.csv\u0026#34;, dtype=float) precipitation = np.genfromtxt(\u0026#34;./DATA/OTTAWA_PrecTOT_2017.csv\u0026#34;, dtype=float) print(temperature.shape, precipitation.shape) (365,) (365,)  Correction #1-  tableau = np.column_stack([temperature,precipitation]) #tableau = np.hstack([temperature,precipitation]) tableau = np.hstack([temperature.reshape(len(temperature),-1),precipitation.reshape(len(precipitation),-1)]) `\nprint(tableau.shape) (365, 2)  #2-  tableau[:,0]=tableau[:,0]-273.15 `` ```python len(tableau[tableau[:,1]\u0026gt;=25]) 10  # 4-  precipitation_max = np.nanmax(tableau[:,1]) print(precipitation_max) 45.01  tableau[tableau[:,1] == precipitation_max] array([[15.8 , 45.01]])  # 5-  sum(tableau[:,1][tableau[:,1]\u0026gt;0]) 1328.5300000000002  # 6-  cumul_precipitation = np.nancumsum(tableau[:,1]) #  tableau = np.column_stack([tableau, cumul_precipitation]) array([[-6.50000e+00, 1.10000e-01, 1.10000e-01], [-6.80000e+00, 3.03000e+00, 3.14000e+00], [-1.80000e+00, 2.65700e+01, 2.97100e+01], ..., [ nan, 0.00000e+00, 1.32726e+03], [-2.15000e+01, 1.27000e+00, 1.32853e+03], [-2.20000e+01, 0.00000e+00, 1.32853e+03]])  # 7.  temperature2,precipitation2,cumul2 = np.hsplit(tableau, 3) tableau_original=np.concatenate([temperature2+273.15, precipitation2], axis = 1) print(tableau_original) ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"d72fbdd2d25ce0013ee0035ecdfc22c5","permalink":"/courses/tutorial_python/6-numpy_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/6-numpy_library/","section":"courses","summary":"Numpy is the core library for scientific computing in Python A new data container will be used: the ndarray (N-dimensional array). There are vectors (one-dimensional arrays), multidimensional arrays It provides a high-performance multidimensional array object, and tools for working with these arrays We'll see how to initialize Numpy arrays in several ways, access values in arrays, perform math and matrix operations, and use arrays for both masking and comparisons.  https://docs.","tags":null,"title":"6 Numpy library","type":"docs"},{"authors":null,"categories":null,"content":"Pandas is a library specialized in data manipulation. This library contains a set of optimized functions for handling large datasets. It allows to create and export tables of data from text files (separators, .csv, fixed format, compressed), binary (HDF5 with Pytable), HTML, XML, JSON, MongoDB, SQL \u0026hellip;\nA new data structure is used with this library: the DataFrame. There are two types of data with pandas: seriesand dataframes.\n  a dataframe is an array that is created with dictionaries or lists\n  they are based on Numpy or ndarray tables\n  they can have column and line names\n  they have the particularity of being able to mix the types of data: str, float, Nan, Int \u0026hellip;\n  they can be viewed as an excel sheet but with a larger number of data volumes and a larger number of functions and attributes.\n  7.1 Series introduction - Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.). The axis labels are collectively called index.\n# first, we must import Pandas library import pandas as pd # aliasing as pd import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) A pandas Series can be created using the following constructor:\nserie = pd.Series([11,15,12,13,14]) print(serie) 0 11 1 15 2 12 3 13 4 14 dtype: int64  We have in series, indexes and values. These indexes can be replaced by text with the index option. Be careful, the number of indexes must correspond to the number of values.\nserie = pd.Series([11,15,12,13,14], index=[\u0026#34;Montreal\u0026#34;, \u0026#34;Ottawa\u0026#34;, \u0026#34;Toronto\u0026#34;, \u0026#34;Gatineau\u0026#34;, \u0026#34;Quebec\u0026#34;]) print(serie) Montreal 11 Ottawa 15 Toronto 12 Gatineau 13 Quebec 14 dtype: int64  The describe() method computes a summary of statistic\nserie.describe() count 5.000000 mean 13.000000 std 1.581139 min 11.000000 25% 12.000000 50% 13.000000 75% 14.000000 max 15.000000 dtype: float64  We can use index to access to an element.\nserie[\u0026#34;Montreal\u0026#34;] 11  We can use index number.\nserie[3] 13  We can use several indexes.\nserie[[\u0026#34;Montreal\u0026#34;, \u0026#34;Quebec\u0026#34;, \u0026#34;Toronto\u0026#34;]] Montreal 11 Quebec 14 Toronto 12 dtype: int64  There are large number of methods collectively compute descriptive statistics such as min(), max(), sum() \u0026hellip;\nserie.min() 11  serie.max() 15  We can apply comparison operators.\nserie[serie\u0026gt;12] Ottawa 15 Gatineau 13 Québec 14 dtype: int64  serie\u0026gt;12 Montréal False Ottawa True Toronto False Gatineau True Québec True dtype: bool  7.2 Pandas Dataframes Pandas DataFrame is two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). A Data frame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows and columns. Pandas DataFrame consists of three principal components, the data, rows, and columns.\n7.2.1 Create a Dataframe In the real world, a Pandas DataFrame will be created by loading the datasets from existing storage, storage can be SQL Database, CSV file, and Excel file. Pandas DataFrame can be created from the lists, dictionary, and from a list of dictionary etc. Dataframe can be created in different ways here are some ways by which we create a dataframe:\n- a) Using a Numpy array import numpy as np stations = np.genfromtxt(\u0026#34;./DATA/DATA_Barrage_1963_2017_5.csv\u0026#34;, delimiter=\u0026#34;,\u0026#34;, dtype=\u0026#39;float\u0026#39;) stations array([[ 39.7 , 39.09 , 39.55645161, 23.23 , 22.5 , 22.85903226, 2390.52612903, 3164.97 , 1673.8 ], [ 41.16 , 40.96 , 41.08806452, 23.22 , 22.43 , 22.7583871 , 2227.28290323, 3008.18 , 1697.87 ], [ 41.15 , 41.05 , 41.11322581, 23.35 , 22.87 , 23.15548387, 2851.27419355, 3231.8 , 2367.85 ], [ 41.09 , 40.61 , 40.9683871 , 23.78 , 22.7 , 23.03258065, 2635.03774194, 3967.33 , 2069.65 ], [ 41.09 , 39.6 , 40.26967742, 24.24 , 22.87 , 23.64580645, 3924.23451613, 5407.32 , 2417.89 ]])  To create the dataframe, we use the Pandas DataFrame () function. It is at this stage that we define the names of our columns. In input we put the table Numpy.\ndataframe = pd.DataFrame(stations, columns=[\u0026#34;Amont Max\u0026#34;, \u0026#34;Amont Min\u0026#34;, \u0026#34;Amont Mean\u0026#34;, \u0026#34;Aval Max\u0026#34;, \u0026#34;Aval Max\u0026#34;, \u0026#34;Aval Mean\u0026#34;, \u0026#34;Debit Mean\u0026#34;, \u0026#34;Debit Max\u0026#34;,\u0026#34;Debit Min\u0026#34;]) dataframe     Amont Max Amont Min Amont Mean Aval Max Aval Max Aval Mean Debit Mean Debit Max Debit Min     0 39.7 39.09 39.5565 23.23 22.5 22.859 2390.53 3164.97 1673.8   1 41.16 40.96 41.0881 23.22 22.43 22.7584 2227.28 3008.18 1697.87   2 41.15 41.05 41.1132 23.35 22.87 23.1555 2851.27 3231.8 2367.85   3 41.09 40.61 40.9684 23.78 22.7 23.0326 2635.04 3967.33 2069.65   4 41.09 39.6 40.2697 24.24 22.87 23.6458 3924.23 5407.32 2417.89    - b) Create Dataframe loading csv file: read_table()or read_csv()function  read_table () and read_csv () are the most useful functions under Pandas for reading text files and generating a DataFrame.  We will work with a dataset from a hydraulic dam. Our csv file has 9 variables, the first line gives us the names of the variables (or labels).\nA csv document can be read with the read_table () function, with the separator attribute \u0026ldquo;,\u0026quot;.\nbarrage = pd.read_table(\u0026#34;./DATA/DATA_EXTREME_Carillon_1963_2017_5.csv\u0026#34;, sep=\u0026#34;,\u0026#34;) barrage.head()     Amont_max Amont_min Amont_moyen Aval_max Aval_min Aval_moyen Debit_Moyen Debit_max Debit_min     0 39.7 39.09 39.5565 23.23 22.5 22.859 2390.53 3164.97 1673.8   1 41.16 40.96 41.0881 23.22 22.43 22.7584 2227.28 3008.18 1697.87   2 41.15 41.05 41.1132 23.35 22.87 23.1555 2851.27 3231.8 2367.85   3 41.09 40.61 40.9684 23.78 22.7 23.0326 2635.04 3967.33 2069.65   4 41.09 39.6 40.2697 24.24 22.87 23.6458 3924.23 5407.32 2417.89    However, if we know that our file to read is a csv, we can use a simpler function of Pandas which is read_csv () .\nNo need to use sep=\u0026rsquo;\u0026rsquo; option. He will find the separator by default.\nhelp(pd.read_csv) Several options are available to the read_csv () function. It is important to know the list of possibilities and options offered by this simple command.\n         path Path to our file   sep Delimiter like , ; \\t or \\s+ for a variable number of spaces   header default 0, the first line contains the name of the variables; if None the names are generated or defined later   index_col Names or numbers of columns defining the indexes of lines, indexes which can be hierarchized   names If header = None, list of variable names   nrows Useful for testing and limiting the number of lines to read   skiprow List of lines to jump in reading   skip_footer Number of lines to jump at the end of file   na_values Definition of the code or codes signaling missing values. They can be defined in a dictionary to associate variables and codes specific missing values   usecols Selects a list of variables to read to avoid reading large or unnecessary fields or variables   skip_blank_lines If True , we skip the white lines   thousand Separator: \u0026ldquo;.\u0026rdquo; or \u0026ldquo;,\u0026rdquo;    # Using example file1 = \u0026#34;./DATA/DATA_EXTREME_Carillon_1963_2017_5.csv\u0026#34; col_names = [\u0026#39;Variable1\u0026#39;, \u0026#39;Variable2\u0026#39;, \u0026#39;Variable3\u0026#39;] df2 = pd.read_csv(file1, skiprows=1, usecols=[0, 1, 3], names=col_names) df2.head()     Variable1 Variable2 Variable3     0 39.7 39.09 23.23   1 41.16 40.96 23.22   2 41.15 41.05 23.35   3 41.09 40.61 23.78   4 41.09 39.6 24.24    - c)Create Dataframe loading ascii file: with open(\u0026#39;./DATA/Daily_Precipitation_1963-2017.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() with open(\u0026#39;./DATA/Daily_Precipitation_1963-2017.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() dataset = [float(row) for row in rows.split()] df3 = pd.DataFrame({\u0026#34;Precipitation\u0026#34; : dataset}) df3.head()     Precipitation     0 0   1 0   2 0   3 0   4 0   5 0   6 0   7 0   8 1.3    - d)Create Dataframe loading excell (.xls) file: read_excel()function We will open here an excel file (.xls extension). This file is a database containing information on all homogenized Environmental and Climate Change Canada temperature stations.\nThis database has 11 columns with data starting at the 4 th line.\nWe will define the \u0026ldquo;Province\u0026rdquo; column as index of our DataFrame.\ndf4 = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, index_col=0,skiprows = range(0, 3)) df4.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y    7.2.2 Access data from DataFrames The first thing to do when opening a new dataset is print out a few rows. We accomplish this with .head()method:\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.head()     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   4 BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    To see the last five rows use .tail()method. tail() also accepts a number, and in this case we printing the bottom two rows.:\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.tail()     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     332 NL PLUM POINT 8402958 1972 7 2016 6 51.07 -56.88 6 N   333 NL PORT AUXBASQUES 8402975 1909 2 2017 9 47.58 -58.97 40 N   334 NL ST ANTHONY 8403389 1946 6 2017 12 51.37 -55.6 33 Y   335 NL ST JOHN'S 8403505 1874 1 2017 12 47.62 -52.75 141 Y   336 NL STEPHENVILLE 8403801 1895 6 2017 12 48.53 -58.55 26 Y   337 NL WABUSH LAKE 8504177 1960 11 2017 12 52.93 -66.87 551 Y    Before exploring a Dataframe, you can modify the index to make it easier to analyze the dataset. For this, we use the .set_index () function. We must create a new object.\ndataframe_Prov_index = dataframe.set_index(\u0026#34;Prov\u0026#34;) dataframe_Prov_index.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    We can directly select a column from a Dataframe:\ndataframe_Prov_index[\u0026#39;Nom de station\u0026#39;].head() Prov BC AGASSIZ BC ATLIN BC BARKERVILLE BC BEAVERDELL BC BELLA COOLA Name: Nom de station, dtype: object  Pandas supports Multi-axes indexing to get the subset of pandas object. Then, to access an element in a dataframe, there are two methods:\n  the iloc () method to access data from index numbers\n  the loc () method to access data from labels\n  a- iloc () method: We can access data from Dataframe using index integer. Like numpy, this method is 0-based indexing.\n# Example1: select specific row and specific column dataframe_Prov_index.iloc[0,0] 'AGASSIZ'  # Example2: iloc: # select first 4 rows f and all columns dataframe_Prov_index.iloc[0:4,:]    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y    # Example3: iloc: # select all rows and 4 specific columns  dataframe_Prov_index.iloc[:,0:4].head()    Prov Nom de station stnid année déb. mois déb.     BC AGASSIZ 1100120 1893 1   BC ATLIN 1200560 1905 8   BC BARKERVILLE 1090660 1888 2   BC BEAVERDELL 1130771 1939 1   BC BELLA COOLA 1060841 1895 5    # Example4: iloc:# Slicing through list of values print(dataframe_Prov_index.iloc[[1, 3, 5], [1, 3]]) print(dataframe_Prov_index.iloc[1:3, :]) dataframe_Prov_index.iloc[:,1:3].head()  stnid mois déb. Prov BC 1200560 8 BC 1130771 1 BC 1021480 7 Nom de station stnid année déb. mois déb. année fin. mois fin. \\ Prov BC ATLIN 1200560 1905 8 2017 12 BC BARKERVILLE 1090660 1888 2 2015 3 lat (deg) long (deg) élév (m) stns jointes Prov BC 59.57 -133.70 674 N BC 53.07 -121.52 1265 N     Prov stnid année déb.     BC 1100120 1893   BC 1200560 1905   BC 1090660 1888   BC 1130771 1939   BC 1060841 1895    b- La méthode loc(): This method has purely label based indexing.\n.loc() has multiple access methods like −\n -single scalar label -list of labels -slice object -Boolean array  .loc takes two single/list/range operator separated by \u0026lsquo;,'. The first one indicates the row and the second one indicates columns.\n# Example1: loc: select all rows for a specific column dataframe_Prov_index.loc[:,\u0026#34;Nom de station\u0026#34;].head() Prov BC AGASSIZ BC ATLIN BC BARKERVILLE BC BEAVERDELL BC BELLA COOLA Name: Nom de station, dtype: object  # Example2: loc: select all rows for a specific index name dataframe_Prov_index.loc[\u0026#34;QC\u0026#34;,:].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     QC AMOS 709CEE9 1913 6 2017 8 48.57 -78.13 305 Y   QC BAGOTVILLE 7060400 1880 11 2017 12 48.33 -71 159 Y   QC BAIE COMEAU 704S001 1965 1 2017 12 49.13 -68.2 130 Y   QC BEAUCEVILLE 7027283 1913 8 2017 8 46.15 -70.7 168 Y   QC BELLETERRE 7080600 1951 9 2004 4 47.38 -78.7 322 N    # Example3: Select all rows for multiple columns, say list[] dataframe_Prov_index.loc[:,[\u0026#34;Nom de station\u0026#34;, \u0026#34;année déb.\u0026#34;, \u0026#34;année fin.\u0026#34;]].head()    Prov Nom de station année déb. année fin.     BC AGASSIZ 1893 2017   BC ATLIN 1905 2017   BC BARKERVILLE 1888 2015   BC BEAVERDELL 1939 2006   BC BELLA COOLA 1895 2017    # Example4: Select few rows for multiple columns, say list[] dataframe_Prov_index.loc[[\u0026#39;BC\u0026#39;,\u0026#39;QC\u0026#39;],[\u0026#34;Nom de station\u0026#34;, \u0026#34;année déb.\u0026#34;, \u0026#34;année fin.\u0026#34;]].head()    Prov Nom de station année déb. année fin.     BC AGASSIZ 1893 2017   BC ATLIN 1905 2017   BC BARKERVILLE 1888 2015   BC BEAVERDELL 1939 2006   BC BELLA COOLA 1895 2017    # Example 5: # for getting values with a boolean array (dataframe_Prov_index.loc[\u0026#39;BC\u0026#39;,[\u0026#34;année déb.\u0026#34;]]\u0026gt;1900).head()    Prov année déb.     BC False   BC True   BC False   BC True   BC False    # Example 6: # for getting values with a boolean array dataframe_Prov_index.loc[dataframe_Prov_index[\u0026#34;année fin.\u0026#34;]\u0026gt;2015,:].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y   BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N   BC BLUE RIVER 1160899 1946 9 2017 12 52.13 -119.28 683 Y    # Example 7: # for getting values with a boolean array df2 = dataframe_Prov_index.loc[\u0026#34;QC\u0026#34;,:] df2.loc[df2[\u0026#34;année fin.\u0026#34;]==2017,:].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     QC AMOS 709CEE9 1913 6 2017 8 48.57 -78.13 305 Y   QC BAGOTVILLE 7060400 1880 11 2017 12 48.33 -71 159 Y   QC BAIE COMEAU 704S001 1965 1 2017 12 49.13 -68.2 130 Y   QC BEAUCEVILLE 7027283 1913 8 2017 8 46.15 -70.7 168 Y   QC CAUSAPSCAL 7051200 1913 11 2017 8 48.37 -67.23 168 N    7.2.3 Change a Dataframe 7.2.3.1 Column Selection/Addition/Deletion- We will use here our previous Dataframe.\ndataframe_Prov_index.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    a- Create a new variable We can select columns from our Dataframe to create a new one. In this example, we will calculate the number of recording years for each station.\ndelta_year = (dataframe_Prov_index[\u0026#34;année fin.\u0026#34;] - dataframe_Prov_index[\u0026#34;année déb.\u0026#34;]) + 1 delta_year.head() Prov BC 125 BC 113 BC 128 BC 68 BC 123 dtype: int64  b- Column Addition in a DataFrame dataframe_Prov_index[\u0026#34;total année\u0026#34;] = delta_year dataframe_Prov_index.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes total année     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N 125   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 113   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N 128   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 68    c- Column Deletion in a DataFrame Columns from a Dataframe can be deleted or popped; let us take an example to understand how.\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) # using del function print (\u0026#34;Deleting \u0026#39;stns jointes\u0026#39;column using DEL function:\u0026#34;) del dataframe[\u0026#39;stns jointes\u0026#39;] dataframe.head() Deleting 'stns jointes' column using DEL function:      Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m)     0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15   1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674   2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265   3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838   4 BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18    # using pop function print (\u0026#34;Deleting \u0026#39;stnid\u0026#39;column using POP function:\u0026#34;) dataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.pop(\u0026#39;stnid\u0026#39;) dataframe.head() Deleting 'stnid' column using POP function:      Prov Nom de station année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     0 BC AGASSIZ 1893 1 2017 12 49.25 -121.77 15 N   1 BC ATLIN 1905 8 2017 12 59.57 -133.7 674 N   2 BC BARKERVILLE 1888 2 2015 3 53.07 -121.52 1265 N   3 BC BEAVERDELL 1939 1 2006 9 49.48 -119.05 838 Y   4 BC BELLA COOLA 1895 5 2017 11 52.37 -126.68 18 Y    # using drop method dataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)) dataframe.drop([\u0026#34;stns jointes\u0026#34;], axis=1).head()     Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m)     0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15   1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674   2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265   3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838   4 BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18    7.2.3.2 Row Selection/Addition/Deletion- We will now understand row selection, addition and deletion through examples. Let us begin with the concept of selection.\na- Row Selection Selection by Label Rows can be selected by passing row label to a loc function.\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)).set_index(\u0026#34;Prov\u0026#34;) dataframe.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    dataframe.loc[\u0026#39;BC\u0026#39;].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    Rows can be selected by passing integer location to an iloc function.\ndataframe.iloc[0] Nom de station AGASSIZ stnid 1100120 année déb. 1893 mois déb. 1 année fin. 2017 mois fin. 12 lat (deg) 49.25 long (deg) -121.77 élév (m) 15 stns jointes N Name: BC, dtype: object  Multiple rows can be selected using ‘ : ’ operator.\ndataframe[2:4]    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y    b- Row Addition Add new rows to a DataFrame using function append()function. This function will append the rows at the end.\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)).set_index(\u0026#34;Prov\u0026#34;) df_new = pd.DataFrame({\u0026#39;Nom de station\u0026#39;: [\u0026#39;station1\u0026#39;, \u0026#39;station2\u0026#39;], \u0026#39;stnid\u0026#39;: [8888, 9999], \u0026#39;Prov\u0026#39;: [\u0026#39;BC\u0026#39;, \u0026#39;QC\u0026#39;]}).set_index(\u0026#34;Prov\u0026#34;) df_new    Prov Nom de station stnid     BC station1 8888   QC station2 9999    dataframe = dataframe.append(df_new) dataframe.tail()    Prov Nom de station année déb. année fin. lat (deg) long (deg) mois déb. mois fin. stnid stns jointes élév (m)     NL ST JOHN'S 1874 2017 47.62 -52.75 1 12 8403505 Y 141   NL STEPHENVILLE 1895 2017 48.53 -58.55 6 12 8403801 Y 26   NL WABUSH LAKE 1960 2017 52.93 -66.87 11 12 8504177 Y 551   BC station1 nan nan nan nan nan nan 8888 nan nan   QC station2 nan nan nan nan nan nan 9999 nan nan    c- Row Deletion Use index label to delete or drop rows from a DataFrame. If label is duplicated, then multiple rows will be dropped.\ndataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)).set_index(\u0026#34;Prov\u0026#34;) dataframe.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N   BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N   BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N   BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y   BC BELLA COOLA 1060841 1895 5 2017 11 52.37 -126.68 18 Y    # Drop rows with label \u0026#39;BC\u0026#39; dataframe = dataframe.drop(\u0026#39;BC\u0026#39;) dataframe.head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     YT BURWASH 2100181 1966 10 2017 12 61.37 -139.05 807 Y   YT DAWSON 2100LRP 1901 1 2017 12 64.05 -139.13 370 Y   N YT HAINES JUNCTIO 2100630 1944 10 2017 12 60.75 -137.5 596 N   YT KOMAKUK BEACH 2100682 1958 7 2017 12 69.62 -140.2 13 Y   YT MAYO 2100701 1924 10 2017 12 63.62 -135.87 504 Y    dataframe = pd.read_excel(\u0026#34;./DATA/Homog_Temperature_Stations.xls\u0026#34;, skiprows = range(0, 3)).set_index(\u0026#34;Prov\u0026#34;) dataframe.loc[\u0026#39;ON\u0026#39;].head()    Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes     ON ATIKOKAN 6020LPQ 1917 1 2017 12 48.8 -91.58 442 Y   ON BEATRICE 6110607 1878 1 2017 12 45.13 -79.4 297 Y   ON BELLEVILLE 6150689 1921 1 2017 12 44.15 -77.4 76 N   ON BIG TROUT LAKE 6010735 1939 2 2017 12 53.83 -89.87 224 Y   ON BROCKVILLE 6100971 1915 7 2017 12 44.6 -75.67 96 Y    7.2.3.3 Merging/Joining Dataframe- Pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL.\npd.merge(left, right, how=\u0026#39;inner\u0026#39;, on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True)          left DataFrame object   right Another DataFrame object   on Columns (names) to join on. Must be found in both the left and right DataFrame objects   left_on Columns from the left DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame   right_on Columns from the right DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame   left_index If True, use the index (row labels) from the left DataFrame as its join key(s). In case of a DataFrame with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame   right_index Same usage as left_index for the right DataFrame   how One of \u0026lsquo;left\u0026rsquo;, \u0026lsquo;right\u0026rsquo;, \u0026lsquo;outer\u0026rsquo;, \u0026lsquo;inner\u0026rsquo;. Defaults to inner. Each method has been described below   sort Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve the performance substantially in many cases    Let us now create two different DataFrames and perform the merging operations on it.\nleft_dataframe = pd.DataFrame({ \u0026#39;id\u0026#39;:[1,2,3,4], \u0026#39;Nom de station\u0026#39;: [\u0026#39;MONTREAL TAVISH\u0026#39;, \u0026#39;QUEBEC\u0026#39;, \u0026#39;TADOUSSAC\u0026#39;,\u0026#39;OKA\u0026#39;], \u0026#39;variable\u0026#39;:[\u0026#39;var1\u0026#39;,\u0026#39;var2\u0026#39;,\u0026#39;var6\u0026#39;,\u0026#39;var5\u0026#39;]}) right_dataframe = pd.DataFrame( {\u0026#39;id\u0026#39;:[1,2,3,4], \u0026#39;Nom de station\u0026#39;: [\u0026#39;TORONTO\u0026#39;, \u0026#39;OTTAWA\u0026#39;, \u0026#39;KINGSTON\u0026#39;,\u0026#39;CHAPLEAU\u0026#39;], \u0026#39;variable\u0026#39;:[\u0026#39;var3\u0026#39;,\u0026#39;var1\u0026#39;,\u0026#39;var6\u0026#39;,\u0026#39;var5\u0026#39;]}) left_dataframe     id Nom de station variable     0 1 MONTREAL TAVISH var1   1 2 QUEBEC var2   2 3 TADOUSSAC var6   3 4 OKA var5    right_dataframe     id Nom de station variable     0 1 TORONTO var3   1 2 OTTAWA var1   2 3 KINGSTON var6   3 4 CHAPLEAU var5    a- Merge Two DataFrames on a Key pd.merge(left_dataframe,right_dataframe,on=\u0026#39;id\u0026#39;)     id Nom de station_x variable_x Nom de station_y variable_y     0 1 MONTREAL TAVISH var1 TORONTO var3   1 2 QUEBEC var2 OTTAWA var1   2 3 TADOUSSAC var6 KINGSTON var6   3 4 OKA var5 CHAPLEAU var5    b- Merge Two DataFrames on a Key pd.merge(left_dataframe,right_dataframe,on=[\u0026#39;id\u0026#39;,\u0026#39;variable\u0026#39;])     id Nom de station_x variable Nom de station_y     0 3 TADOUSSAC var6 KINGSTON   1 4 OKA var5 CHAPLEAU    c- Merge Two DataFrames Using \u0026lsquo;How\u0026rsquo; argument The how argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or the right tables, the values in the joined table will be NA.\n# Left Join pd.merge(left_dataframe, right_dataframe, on=\u0026#39;variable\u0026#39;, how=\u0026#39;left\u0026#39;)     id_x Nom de station_x variable id_y Nom de station_y     0 1 MONTREAL TAVISH var1 2 OTTAWA   1 2 QUEBEC var2 nan nan   2 3 TADOUSSAC var6 3 KINGSTON   3 4 OKA var5 4 CHAPLEAU    # right Join pd.merge(left_dataframe, right_dataframe, on=\u0026#39;variable\u0026#39;, how=\u0026#39;right\u0026#39;)     id_x Nom de station_x variable id_y Nom de station_y     0 1 MONTREAL TAVISH var1 2 OTTAWA   1 3 TADOUSSAC var6 3 KINGSTON   2 4 OKA var5 4 CHAPLEAU   3 nan nan var3 1 TORONTO    # outer Join pd.merge(left_dataframe, right_dataframe, on=\u0026#39;variable\u0026#39;, how=\u0026#39;outer\u0026#39;)     id_x Nom de station_x variable id_y Nom de station_y     0 1 MONTREAL TAVISH var1 2 OTTAWA   1 2 QUEBEC var2 nan nan   2 3 TADOUSSAC var6 3 KINGSTON   3 4 OKA var5 4 CHAPLEAU   4 nan nan var3 1 TORONTO    # inner Join pd.merge(left_dataframe, right_dataframe, on=\u0026#39;variable\u0026#39;, how=\u0026#39;inner\u0026#39;)     id_x Nom de station_x variable id_y Nom de station_y     0 1 MONTREAL TAVISH var1 2 OTTAWA   1 3 TADOUSSAC var6 3 KINGSTON   2 4 OKA var5 4 CHAPLEAU    7.2.3.4 Dataframe Concatenation- Pandas provides various facilities for easily combining together DataFrame objects.\npd.concat(objs,axis=0,join='outer',join_axes=None,ignore_index=False)           objs This is a sequence or mapping of Series, DataFrame objects   axis {0, 1, \u0026hellip;}, default 0. This is the axis to concatenate along   join {‘inner’, ‘outer’}, default ‘outer’. How to handle indexes on other axis(es). Outer for union and inner for intersection   ignore_index boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, \u0026hellip;, n - 1   join_axes This is the list of Index objects. Specific indexes to use for the other (n-1) axes instead of performing inner/outer set logic    dataframe1 = pd.DataFrame({ \u0026#39;id\u0026#39;:[1,2,3,4], \u0026#39;Nom de station\u0026#39;: [\u0026#39;MONTREAL TAVISH\u0026#39;, \u0026#39;QUEBEC\u0026#39;, \u0026#39;TADOUSSAC\u0026#39;,\u0026#39;OKA\u0026#39;], \u0026#39;variable\u0026#39;:[\u0026#39;var1\u0026#39;,\u0026#39;var2\u0026#39;,\u0026#39;var6\u0026#39;,\u0026#39;var5\u0026#39;]}) dataframe2 = pd.DataFrame( {\u0026#39;id\u0026#39;:[1,2,3,4], \u0026#39;Nom de station\u0026#39;: [\u0026#39;TORONTO\u0026#39;, \u0026#39;OTTAWA\u0026#39;, \u0026#39;KINGSTON\u0026#39;,\u0026#39;CHAPLEAU\u0026#39;], \u0026#39;variable\u0026#39;:[\u0026#39;var3\u0026#39;,\u0026#39;var1\u0026#39;,\u0026#39;var6\u0026#39;,\u0026#39;var5\u0026#39;]}) pd.concat([dataframe1,dataframe2])     id Nom de station variable     0 1 MONTREAL TAVISH var1   1 2 QUEBEC var2   2 3 TADOUSSAC var6   3 4 OKA var5   0 1 TORONTO var3   1 2 OTTAWA var1   2 3 KINGSTON var6   3 4 CHAPLEAU var5    Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can do this by using the keys argument −\npd.concat([dataframe1,dataframe2],keys=[\u0026#39;QC\u0026#39;,\u0026#39;ON\u0026#39;])     id Nom de station variable     (\u0026lsquo;QC\u0026rsquo;, 0) 1 MONTREAL TAVISH var1   (\u0026lsquo;QC\u0026rsquo;, 1) 2 QUEBEC var2   (\u0026lsquo;QC\u0026rsquo;, 2) 3 TADOUSSAC var6   (\u0026lsquo;QC\u0026rsquo;, 3) 4 OKA var5   (\u0026lsquo;ON\u0026rsquo;, 0) 1 TORONTO var3   (\u0026lsquo;ON\u0026rsquo;, 1) 2 OTTAWA var1   (\u0026lsquo;ON\u0026rsquo;, 2) 3 KINGSTON var6   (\u0026lsquo;ON\u0026rsquo;, 3) 4 CHAPLEAU var5    If we don't want the index being duplicated, set ignore_index to True.\npd.concat([dataframe1,dataframe2],keys=[\u0026#39;QC\u0026#39;,\u0026#39;ON\u0026#39;],ignore_index=True)     id Nom de station variable     0 1 MONTREAL TAVISH var1   1 2 QUEBEC var2   2 3 TADOUSSAC var6   3 4 OKA var5   4 1 TORONTO var3   5 2 OTTAWA var1   6 3 KINGSTON var6   7 4 CHAPLEAU var5    If the two Dataframes need to be added along axis=1, then the new columns will be appended.\npd.concat([dataframe1,dataframe2],axis=1)     id Nom de station variable id Nom de station variable     0 1 MONTREAL TAVISH var1 1 TORONTO var3   1 2 QUEBEC var2 2 OTTAWA var1   2 3 TADOUSSAC var6 3 KINGSTON var6   3 4 OKA var5 4 CHAPLEAU var5    Concatenating Using append A useful shortcut to concat are the append instance methods on DataFrame. They concatenate along axis=0, namely the index\ndataframe1.append(dataframe2)     id Nom de station variable     0 1 MONTREAL TAVISH var1   1 2 QUEBEC var2   2 3 TADOUSSAC var6   3 4 OKA var5   0 1 TORONTO var3   1 2 OTTAWA var1   2 3 KINGSTON var6   3 4 CHAPLEAU var5    7.2.4 Basic Functionality on DataFrame There are many built-in functions and methods:\nhttps://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html\nWe will present some useful functions with exploring a dataset.\ndataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) dataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     0 0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N nan nan nan nan nan   1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 5.63043 18.9033 -3.46052 -19.235 860.083   2 2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N nan nan nan nan nan   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N 12.1556 20.2007 6.77689 1.09467 2518.68     .shapemethod:  Returns a tuple representing the dimensionality of the DataFrame. Tuple (a,b), where a represents the number of rows and b represents the number of columns.\ndataframe.shape (289, 17)   .columnsmethod:  Returns names of the columns in our Dataframe.\ndataframe.columns Index(['Unnamed: 0', 'Prov', 'Nom de station', 'stnid', 'année déb.', 'mois déb.', 'année fin.', 'mois fin.', 'lat (deg)', 'long (deg)', 'élév (m)', 'stns jointes', 'Tmax', 'Tmax90p', 'Tmin', 'Tmin10p', 'DG0'], dtype='object')   .emptymethod:  Returns the Boolean value saying whether the Object is empty or not; True indicates that the object is empty.\ndataframe.empty False   .isnullmethod:  To detect missing values easier (and across different array dtypes), Pandas provides the isnull() and notnull() functions.\ndataframe[\u0026#39;Tmax\u0026#39;].isnull().head() 0 True 1 False 2 True 3 False 4 False Name: Tmax, dtype: bool  We combine this method with .sum() to know the number of missing values.\ndataframe[\u0026#39;Tmax\u0026#39;].isnull().sum() 117   .dropnamethod:  If you want to exclude the missing values, then use the dropna function along with the axis argument. By default, axis=0, i.e., along row, which means that if any value within a row is NA then the whole row is excluded.\ndataframe_sans_NaN = dataframe.dropna() dataframe_sans_NaN[\u0026#39;Tmax\u0026#39;].isnull().sum() 0  dataframe_sans_NaN.shape # nouvelle dimension de notre tableau  (169, 17)   sort_valuesmethod:  sort_values() is the method for sorting by values. It accepts a \u0026lsquo;by\u0026rsquo; argument which will use the column name of the DataFrame with which the values are to be sorted.\ndataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     0 0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N nan nan nan nan nan   1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 5.63043 18.9033 -3.46052 -19.235 860.083   2 2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N nan nan nan nan nan   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N 12.1556 20.2007 6.77689 1.09467 2518.68    df_label_sorted = dataframe.sort_values(by=\u0026#34;Prov\u0026#34;) df_label_sorted.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     127 127 AB SLAVE LAKE 3065995 1922 8 2017 12 55.3 -114.78 583 Y 7.48137 22.4643 -3.42227 -20.3803 1131.03   106 106 AB EDMONTON 3012216 1880 7 2017 12 53.57 -113.52 723 Y 8.98212 24.328 -3.11709 -19.1907 1095.93   104 104 AB COLD LAKE 3081680 1925 7 2017 12 54.42 -110.28 541 Y 7.72448 24.2737 -3.03759 -21.2037 1303.95   103 103 AB CARWAY 3031402 1914 8 2017 12 49 -113.37 1354 Y 11.1397 24.925 -1.61886 -13.98 956.837   102 102 AB CAMROSE 3011240 1946 3 2017 12 53.03 -112.82 739 N 8.96497 24.1757 -3.71442 -20.588 1048.13    The argument could takes a list of column values.\ndf_label_sorted = dataframe.sort_values(by=[\u0026#39;Prov\u0026#39;,\u0026#39;année déb.\u0026#39;]) df_label_sorted.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     106 106 AB EDMONTON 3012216 1880 7 2017 12 53.57 -113.52 723 Y 8.98212 24.328 -3.11709 -19.1907 1095.93   110 110 AB FORT CHIPEWYAN 3072655 1883 10 2017 12 58.77 -111.12 238 Y 4.28219 23.566 -6.54421 -28.6297 1135.11   121 121 AB MEDICINE HAT 3034485 1883 8 2017 12 50.02 -110.72 717 Y 12.6489 28.4973 -0.1559 -15.7833 1563.87   99 99 AB CALGARY 3031092 1885 1 2017 12 51.12 -114.02 1084 Y 10.835 24.4673 -1.44461 -15.235 1172.05   97 97 AB BANFF 3050519 1887 11 2017 12 51.2 -115.55 1397 Y 8.90652 23.132 -3.08488 -15.8583 750.37     sort_index()method:  Using the sort_index() method, by passing the axis arguments and the order of sorting, DataFrame can be sorted. By default, sorting is done on row labels in ascending order.\ndf_label_sorted.sort_index().head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     0 0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N nan nan nan nan nan   1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 5.63043 18.9033 -3.46052 -19.235 860.083   2 2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N nan nan nan nan nan   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N 12.1556 20.2007 6.77689 1.09467 2518.68    df_label_sorted.sort_index(ascending=False).head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     288 288 NL WABUSH LAKE 8504177 1960 11 2017 12 52.93 -66.87 551 Y 2.33974 19.6937 -8.02478 -29.4537 792.913   287 287 NL STEPHENVILLE 8403801 1895 6 2017 12 48.53 -58.55 26 Y 8.80653 20.614 1.81275 -10.582 1710.17   286 286 NL ST JOHN'S 8403505 1874 1 2017 12 47.62 -52.75 141 Y 9.07367 21.6237 1.54977 -8.919 1474.82   285 285 NL ST ANTHONY 8403389 1946 6 2017 12 51.37 -55.6 33 Y nan nan nan nan nan   284 284 NL PORT AUXBASQUES 8402975 1909 2 2017 9 47.58 -58.97 40 N nan nan nan nan nan    df_label_sorted.sort_index(axis=1).head()     DG0 Nom de station Prov Tmax Tmax90p Tmin Tmin10p Unnamed: 0 année déb. année fin. lat (deg) long (deg) mois déb. mois fin. stnid stns jointes élév (m)     106 1095.93 EDMONTON AB 8.98212 24.328 -3.11709 -19.1907 106 1880 2017 53.57 -113.52 7 12 3012216 Y 723   110 1135.11 FORT CHIPEWYAN AB 4.28219 23.566 -6.54421 -28.6297 110 1883 2017 58.77 -111.12 10 12 3072655 Y 238   121 1563.87 MEDICINE HAT AB 12.6489 28.4973 -0.1559 -15.7833 121 1883 2017 50.02 -110.72 8 12 3034485 Y 717   99 1172.05 CALGARY AB 10.835 24.4673 -1.44461 -15.235 99 1885 2017 51.12 -114.02 1 12 3031092 Y 1084   97 750.37 BANFF AB 8.90652 23.132 -3.08488 -15.8583 97 1887 2017 51.2 -115.55 11 12 3050519 Y 1397     .describe()method:  Generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.\ndataframe[\u0026#39;Tmax\u0026#39;].describe() count 172.000000 mean 7.916314 std 5.524630 min -15.366463 25% 7.364646 50% 8.988052 75% 11.018241 max 16.105511 Name: Tmax, dtype: float64   .dtypes()method:  Returns the dtypes in this object.\ndataframe.dtypes Unnamed: 0 int64 Prov object Nom de station object stnid object année déb. int64 mois déb. int64 année fin. int64 mois fin. int64 lat (deg) float64 long (deg) float64 élév (m) int64 stns jointes object Tmax float64 Tmax90p float64 Tmin float64 Tmin10p float64 DG0 float64 dtype: object  7.2.5 DataFrame Function Application To apply your own or another library’s functions to Pandas objects, you should be aware of the three important methods. The methods have been discussed below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame, row- or column-wise, or element wise.\n  Table wise Function Application: .pipe()  Row or Column Wise Function Application: .apply()  Element wise Function Application: .applymap()  .apply()method:\n  Arbitrary functions can be applied along the axes of a DataFrame or Panel using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument. By default, the operation performs column wise, taking each column as an array-like.\ndataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) dataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) dataframe[\u0026#34;stns jointes\u0026#34;]=dataframe[\u0026#34;stns jointes\u0026#34;].apply(lambda x: x.replace(\u0026#34;N\u0026#34;, \u0026#34;NaN\u0026#34;)) dataframe[\u0026#34;stns jointes\u0026#34;]=dataframe[\u0026#34;stns jointes\u0026#34;].apply(lambda x: x.replace(\u0026#34;Y\u0026#34;, \u0026#34;1\u0026#34;)) dataframe = dataframe.dropna() dataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 nan 5.63043 18.9033 -3.46052 -19.235 860.083   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 1 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 nan 12.1556 20.2007 6.77689 1.09467 2518.68   5 5 BC BLUE RIVER 1160899 1946 9 2017 12 52.13 -119.28 683 1 10.4408 25.9917 -1.12501 -12.5823 987.363   9 9 BC COMOX 1021830 1935 11 2017 12 49.72 -124.9 26 1 13.7376 23.1023 6.42485 -0.526 2444.39    dataframe[\u0026#34;Tmin\u0026#34;]=dataframe[\u0026#34;Tmin\u0026#34;].apply(lambda x: round(x,2)) dataframe[\u0026#34;Tmax\u0026#34;]=dataframe[\u0026#34;Tmax\u0026#34;].apply(lambda x: int(x)) dataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 nan 5 18.9033 -3.46 -19.235 860.083   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 1 12 23.6417 4.02 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 nan 12 20.2007 6.78 1.09467 2518.68   5 5 BC BLUE RIVER 1160899 1946 9 2017 12 52.13 -119.28 683 1 10 25.9917 -1.13 -12.5823 987.363   9 9 BC COMOX 1021830 1935 11 2017 12 49.72 -124.9 26 1 13 23.1023 6.42 -0.526 2444.39    7.2.6 DataFrame GroupBY method Any groupby operation involves one of the following operations on the original object. They are −\n Splitting the Object Applying a function Combining the results  In many situations, we split the data into sets and we apply some functionality on each subset. In the apply functionality, we can perform the following operations −\n Aggregation − computing a summary statistic Transformation − perform some group-specific operation Filtration − discarding the data with some condition  Let us now create a DataFrame object and perform all the operations on it −\ndataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) dataframe.head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     0 0 BC AGASSIZ 1100120 1893 1 2017 12 49.25 -121.77 15 N nan nan nan nan nan   1 1 BC ATLIN 1200560 1905 8 2017 12 59.57 -133.7 674 N 5.63043 18.9033 -3.46052 -19.235 860.083   2 2 BC BARKERVILLE 1090660 1888 2 2015 3 53.07 -121.52 1265 N nan nan nan nan nan   3 3 BC BEAVERDELL 1130771 1939 1 2006 9 49.48 -119.05 838 Y 12.628 23.6417 4.01748 -3.646 1798.09   4 4 BC BLIND CHANNEL 1021480 1958 7 2016 2 50.42 -125.5 23 N 12.1556 20.2007 6.77689 1.09467 2518.68    Looking at the DataFrame above, we see that there are at least 3 variables that we can use to group our dataset. For example, we can group our data by province (Prov), by year of beginning of recording or year of end of recording.\nWe will use the Pandas groupby module to group our data.\n .unique()method :  Returns the unique values of a column.\ndataframe[\u0026#34;Prov\u0026#34;].unique() array(['BC', 'YT', 'N YT', 'NT', 'NU', 'AB', 'SK', 'MB', 'ON', 'QC', 'NB', 'NS', 'PE', 'NL'], dtype=object)  Split Data into Groups: dataframe.groupby(\u0026#39;Prov\u0026#39;) \u0026lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000000008D35860\u0026gt;   To view groups:  dataframe.groupby(\u0026#39;Prov\u0026#39;).groups {'AB': Int64Index([ 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130], dtype='int64'), 'BC': Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50], dtype='int64'), 'MB': Int64Index([156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175], dtype='int64'), 'N YT': Int64Index([52], dtype='int64'), 'NB': Int64Index([254, 255, 256, 257, 258, 259, 260, 261], dtype='int64'), 'NL': Int64Index([275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288], dtype='int64'), 'NS': Int64Index([262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], dtype='int64'), 'NT': Int64Index([61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73], dtype='int64'), 'NU': Int64Index([74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], dtype='int64'), 'ON': Int64Index([176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215], dtype='int64'), 'PE': Int64Index([274], dtype='int64'), 'QC': Int64Index([216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253], dtype='int64'), 'SK': Int64Index([131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155], dtype='int64'), 'YT': Int64Index([51, 53, 54, 55, 56, 57, 58, 59, 60], dtype='int64')}   Group by with multiple columns:  dataframe.groupby([\u0026#39;Prov\u0026#39;,\u0026#39;année fin.\u0026#39;]).groups {('AB', 2011): Int64Index([116], dtype='int64'), ('AB', 2013): Int64Index([101], dtype='int64'), ('AB', 2016): Int64Index([100, 130], dtype='int64'), ('AB', 2017): Int64Index([ 96, 97, 98, 99, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129], dtype='int64'), ('BC', 2006): Int64Index([3], dtype='int64'), ('BC', 2013): Int64Index([20], dtype='int64'), ('BC', 2014): Int64Index([21, 25], dtype='int64'), ('BC', 2015): Int64Index([2, 8, 11, 16], dtype='int64'), ('BC', 2016): Int64Index([4, 6, 41], dtype='int64'), ('BC', 2017): Int64Index([ 0, 1, 5, 7, 9, 10, 12, 13, 14, 15, 17, 18, 19, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50], dtype='int64'), ('MB', 2016): Int64Index([156], dtype='int64'), ('MB', 2017): Int64Index([157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175], dtype='int64'), ('N YT', 2017): Int64Index([52], dtype='int64'), ('NB', 2017): Int64Index([254, 255, 256, 257, 258, 259, 260, 261], dtype='int64'), ('NL', 2011): Int64Index([282], dtype='int64'), ('NL', 2015): Int64Index([276], dtype='int64')}  Iterating through Groups: grouped = dataframe.groupby(\u0026#39;Prov\u0026#39;) for name,group in grouped: print(name) print(group) AB Unnamed: 0 Prov Nom de station stnid année déb. mois déb. \\ 96 96 AB ATHABASCA 3060L20 1918 6 97 97 AB BANFF 3050519 1887 11 98 98 AB BEAVERLODGE 3070600 1913 4 99 99 AB CALGARY 3031092 1885 1 année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes \\ 96 2017 12 54.82 -113.53 626 Y 97 2017 12 51.20 -115.55 1397 Y 98 2017 12 55.20 -119.40 745 Y 99 2017 12 51.12 -114.02 1084 Y  Select a group:  .get_group()method:  We can select a single group.\ngrouped.get_group(\u0026#39;QC\u0026#39;).head()     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     216 216 QC AMOS 709CEE9 1913 6 2017 8 48.57 -78.13 305 Y nan nan nan nan nan   217 217 QC BAGOTVILLE 7060400 1880 11 2017 12 48.33 -71 159 Y 8.28139 25.2973 -1.83359 -20.8117 1528.41   218 218 QC BEAUCEVILLE 7027283 1913 8 2017 8 46.15 -70.7 168 Y 10.5515 26.0783 -1.37634 -19.6283 1509.36   219 219 QC BROME 7020840 1890 9 2014 7 45.18 -72.57 206 N 11.1409 26.1767 -0.294151 -17.56 1676.23   220 220 QC CAUSAPSCAL 7051200 1913 11 2017 8 48.37 -67.23 168 N nan nan nan nan nan    Aggregations An aggregated function returns a single aggregated value for each group. Once the group by object is created, several aggregation operations can be performed on the grouped data.\nAn obvious one is aggregation via the aggregate or equivalent agg method −\ndataframe = pd.read_csv(\u0026#34;./DATA/Climato_Stations_ECCC_1981_2010_YEAR.csv\u0026#34;, encoding=\u0026#39;latin-1\u0026#39;) grouped = dataframe.groupby(\u0026#39;Prov\u0026#39;) grouped[\u0026#39;Tmin\u0026#39;].agg(np.mean) Prov AB -3.050928 BC 1.578024 MB -4.493900 N YT NaN NB 0.341750 NL -1.324334 NS 2.542629 NT -8.518655 NU -16.522658 ON 0.031970 PE 1.986827 QC -1.992695 SK -3.208984 YT -8.995421 Name: Tmin, dtype: float64  Applying Multiple Aggregation Functions at Once With grouped Series, you can also pass a list or dict of functions to do aggregation with, and generate DataFrame as output −\ngrouped[\u0026#39;Tmin\u0026#39;].agg([np.min, np.mean, np.max, np.std])    Prov amin mean amax std     AB -6.54421 -3.05093 -0.1559 1.40133   BC -6.06858 1.57802 7.01629 3.89638   MB -10.0943 -4.4939 -1.98027 2.61835   N YT nan nan nan nan   NB -0.491043 0.34175 0.840796 0.598429   NL -8.02478 -1.32433 1.81275 3.42585   NS 1.21413 2.54263 3.75946 0.982653   NT -12.4518 -8.51865 -6.68323 2.16937   NU -21.9351 -16.5227 -12.2332 3.0491   ON -7.48484 0.0319696 5.91872 3.64788   PE 1.98683 1.98683 1.98683 nan   QC -8.76805 -1.99269 1.62212 2.70668   SK -5.01656 -3.20898 -1.42885 1.03055   YT -10.2177 -8.99542 -7.77313 1.72858    Transformations Transformation on a group or a column returns an object that is indexed the same size of that is being grouped. Thus, the transform should return a result that is the same size as that of a group chunk.\ngrouped = dataframe.groupby(\u0026#39;Prov\u0026#39;) and_stand = lambda x: (x - x.mean()) / x.std() grouped[\u0026#39;Tmin\u0026#39;].transform(and_stand).head() 0 NaN 1 -1.293133 2 NaN 3 0.626082 4 1.334280 Name: Tmin, dtype: float64  Filtration Filtration filters the data on a defined criteria and returns the subset of data. The filter() function is used to filter the data.\ndataframe.groupby(\u0026#39;Prov\u0026#39;).filter(lambda x: len(x) == 1)     Unnamed: 0 Prov Nom de station stnid année déb. mois déb. année fin. mois fin. lat (deg) long (deg) élév (m) stns jointes Tmax Tmax90p Tmin Tmin10p DG0     52 52 N YT HAINES JUNCTIO 2100630 1944 10 2017 12 60.75 -137.5 596 N nan nan nan nan nan   274 274 PE CHARLOTTETOWN 8300301 1872 11 2017 12 46.28 -63.13 49 Y 10.0063 23.7683 1.98683 -12.0367 1912.16    In the above filter condition, we are asking to return the Provinces which have only one station.\n7.2.7 Save a DataFrame: For writing a DataFrame, use the .to_csv or _table functions with similar options as read_csv () seen previously.\ndataframe.to_csv(\u0026#34;./DATA/My_new_DataFrame.csv\u0026#34;, index = False, header = True, sep = \u0026#39;,\u0026#39;) 7.3 Date Functionality: Using the date.range()function by specifying the periods and the frequency, we can create the date series. By default, the frequency of range is Days.\npd.date_range(\u0026#39;1/1/2011\u0026#39;, periods=5) DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03', '2011-01-04', '2011-01-05'], dtype='datetime64[ns]', freq='D')  We can change the date frequency:\npd.date_range(\u0026#39;1/1/2011\u0026#39;, periods=5,freq=\u0026#39;M\u0026#39;) DatetimeIndex(['2011-01-31', '2011-02-28', '2011-03-31', '2011-04-30', '2011-05-31'], dtype='datetime64[ns]', freq='M')  bdate_range()stands for business date ranges. Unlike date_range(), it excludes Saturday and Sunday.\npd.bdate_range(\u0026#39;1/1/2011\u0026#39;, periods=10) DatetimeIndex(['2011-01-03', '2011-01-04', '2011-01-05', '2011-01-06', '2011-01-07', '2011-01-10', '2011-01-11', '2011-01-12', '2011-01-13', '2011-01-14'], dtype='datetime64[ns]', freq='B')  Convenience functions like date_range and bdate_range utilize a variety of frequency aliases. The default frequency for date_range is a calendar day while the default for bdate_range is a business day.\nstart = pd.datetime(2011, 1, 1) end = pd.datetime(2011, 1, 5) pd.date_range(start, end) DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03', '2011-01-04', '2011-01-05'], dtype='datetime64[ns]', freq='D')  7.4 Format dates with the Datetime module: Python provides many features to work with dates and time.\nDatetime is a module that allows you to manipulate dates and times as objects. The idea is simple: you manipulate the object to do all your calculations, and when you need to display it, you format the object into a string.\nhttps://docs.python.org/2/library/datetime.html\nYou can artificially create a datetime object with the following parameters:\n datetime (year, month, day, hour, minute, second, microsecond, timezone)  The parameters \u0026ldquo;year\u0026rdquo;, \u0026ldquo;month\u0026rdquo; and \u0026ldquo;day\u0026rdquo; are mandatory.\nThe datetime module provides the following classes:\n   Class Description     datetime.date A date instance represents a date   datetime.datetime An instance of datetime represents a date and time according to the Gregorian calendar   datetime.time An instance of time represents the time, except for the date   datetime.timedelta The timedelta class is used to keep the differences between two temporal or dated objects   datetime.tzinfo The tzinfo class is used to implement time zone support for time and datetime objects    We will see some examples of using DateTime and its classe.\n1- The datetime class of the datetime module -a Creating a datetime objectfrom datetime import datetime datetime(2019, 3, 1) # instance of datetime datetime.datetime(2019, 3, 1, 0, 0)  now = datetime.now() now datetime.datetime(2019, 10, 24, 14, 8, 0, 512783)  now = now.today() now datetime.datetime(2019, 10, 24, 14, 8, 0, 531785)  now = datetime.utcnow() now datetime.datetime(2019, 10, 24, 18, 8, 0, 546785)  When opening a csv or text file, we have information about the date and time of the measurements but in the form of strings: \u0026ldquo;2018-11-01 15:20\u0026rdquo; or \u0026ldquo;2017/12/1 16:35:22 \u0026ldquo;\u0026hellip;\nIt is possible during the reading to convert these strings into a datetime object.\ndt = datetime.strptime(\u0026#34;2018/11/01 15:20\u0026#34;, \u0026#34;%Y/%m/%d%H:%M\u0026#34;) dt datetime.datetime(2018, 11, 1, 15, 20)  dt = datetime.strptime(\u0026#34;2017/12/1 16:35:22\u0026#34;, \u0026#34;%Y/%m/%d%H:%M:%S\u0026#34;) dt datetime.datetime(2017, 12, 1, 16, 35, 22)  dt = datetime.strptime(\u0026#34;01/11/19 10-35:22\u0026#34;, \u0026#34;%d/%m/%y %H-%M:%S\u0026#34;) dt datetime.datetime(2019, 11, 1, 10, 35, 22)  dt = datetime.strptime(\u0026#34;1Mar 2019 à 09h35\u0026#34;, \u0026#34;%d%b %Y à %Hh%M\u0026#34;) dt datetime.datetime(2019, 3, 1, 9, 35)  b- Manipulate datetime objectFrom an object or instance of datetime, you can retrieve the time and date.\n#now.year #now.month #now.day #maintenant.hour now.minute #now.second #now.microsecond #now 8  We can change datetime instance:\nnow.replace(year=1995) datetime.datetime(1995, 10, 24, 18, 8, 0, 546785)  now.replace(month=1) datetime.datetime(2019, 1, 24, 18, 8, 0, 546785)  We can then convert datetime instance to string:\nd = datetime.now(); print(d) d 2019-10-24 14:08:00.720795 datetime.datetime(2019, 10, 24, 14, 8, 0, 720795)  d.strftime(\u0026#34;%H:%M\u0026#34;), d.strftime(\u0026#34;%Hh%Mmin\u0026#34;) ('14:08', '14h08min')  d.strftime(\u0026#34;%Y-%m %H:%M\u0026#34;) '2019-10 14:08'  \u0026#39;The day today is {0:%d} {0:%B} and it s {0:%Hh%Mmin} \u0026#39;.format(d, \u0026#34;day\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;time\u0026#34;) 'The day today is 24 October and it s 14h08min '   dateet timeclass  These two classes can be used to create a datetime instance.\nfrom datetime import datetime, date, time d = date(2005, 7, 14) t = time(12, 30) t datetime.time(12, 30)  datetime.combine(d, t) datetime.datetime(2005, 7, 14, 12, 30)  now = datetime.utcnow() now.date() now.time() datetime.time(18, 8, 0, 855803)   The timedelta class of the datetime module  from datetime import timedelta delta = timedelta(days=3, seconds=100) # we create our own timedelta datetime.now() datetime.datetime(2019, 10, 24, 14, 8, 0, 899806)  datetime.now() + delta datetime.datetime(2019, 10, 27, 14, 9, 40, 935808)  datetime.now() + timedelta(days=2, hours=4, minutes=3, seconds=12) datetime.datetime(2019, 10, 26, 18, 11, 12, 966809)  time_range = datetime(2010, 12, 31) - datetime(1981, 12, 31) time_range datetime.timedelta(days=10592)   Example1: Calculate the year of birth from a given age  from datetime import datetime old = 25 month = 10 actual_year = datetime.today().year actual_month = datetime.today().month result = actual_year - old - (1 if month \u0026gt; actual_month else 0) print(result) 1994   Example2: Calculate the year of birth from a given age  We can generate dates for time series with an arbitrary time step:\nfrom datetime import timedelta dt = timedelta(days = 5, hours = 6, minutes = 25) d0 = datetime(2000, 2, 21) [str(d0 + i * dt) for i in range(10)] ['2000-02-21 00:00:00', '2000-02-26 06:25:00', '2000-03-02 12:50:00', '2000-03-07 19:15:00', '2000-03-13 01:40:00', '2000-03-18 08:05:00', '2000-03-23 14:30:00', '2000-03-28 20:55:00', '2000-04-03 03:20:00', '2000-04-08 09:45:00'] ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"e257612f18141e3399f89ed09ec181c4","permalink":"/courses/tutorial_python/7-pandas_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/7-pandas_library/","section":"courses","summary":"Pandas is a library specialized in data manipulation. This library contains a set of optimized functions for handling large datasets. It allows to create and export tables of data from text files (separators, .csv, fixed format, compressed), binary (HDF5 with Pytable), HTML, XML, JSON, MongoDB, SQL \u0026hellip;\nA new data structure is used with this library: the DataFrame. There are two types of data with pandas: seriesand dataframes.\n  a dataframe is an array that is created with dictionaries or lists","tags":null,"title":"7 Pandas library","type":"docs"},{"authors":null,"categories":null,"content":"The Matplotlib library is one of the most used libraries for plotting data in Python.\nMany types of graphics can be developed with this library: https://matplotlib.org/gallery/index.html\nTo illustrate some features of the Python matplotlib.pyplot module (which provides a plotting system similar to that of MATLAB), we will use a database from the UQAM station.\nimport pandas as pd import numpy as np import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) dataframe_UQAM = pd.read_csv(\u0026#39;./DATA/UQAM_DATA_STATION_2018.csv\u0026#39;) dataframe_UQAM[\u0026#39;Date\u0026#39;]=pd.to_datetime(dataframe_UQAM[\u0026#39;Date\u0026#39;]) dataframe_UQAM = dataframe_UQAM.set_index(\u0026#34;Date\u0026#34;, drop=True) dataframe_UQAM.head()    Date Temperature minimale Temperature maximale Temperature moyenne Precipitation totale Dir_wind Mod_wind     2014-02-01 00:00:00 -4.6 0.9 -1.5 0 244 4   2014-02-02 00:00:00 -6.2 0.2 -3.5 0 163 2   2014-02-03 00:00:00 -4.8 0.5 -1.2 0 255 3   2014-02-04 00:00:00 -9.9 -4.8 -8 0 148 2   2014-02-05 00:00:00 -9.9 -6.3 -7.6 0 261 2    Introduction to Matplotlib conda install matplotlib By running this special iPython command, we will be displaying plots inline:\n%matplotlib inline import matplotlib.pyplot as plt plt.plot() # you create an empty graph or instance and then add layers. plt.show() Add data to our charts dataframe_UQAM[\u0026#39;2015\u0026#39;].head()    Date Temperature minimale Temperature maximale Temperature moyenne Precipitation totale Dir_wind Mod_wind     2015-01-01 00:00:00 -13.3 -6.4 -9.5 0 254 3   2015-01-02 00:00:00 -7.3 -3 -4.9 0 231 3   2015-01-03 00:00:00 -12 -3.2 -7.8 0 282 4   2015-01-04 00:00:00 -15.3 -8.7 -12.5 0 113 2   2015-01-05 00:00:00 -8.8 3.9 -2.6 0 178 3    year_to_plot = dataframe_UQAM[\u0026#39;2015\u0026#39;] plt.plot(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;]) plt.show() year_to_plot = dataframe_UQAM[\u0026#39;2015\u0026#39;] plt.plot(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;], marker=\u0026#39;x\u0026#39;) plt.show()  markeroption :  https://matplotlib.org/api/markers_api.html\n linestyleoption: to delete or not the lines  year_to_plot = dataframe_UQAM[\u0026#39;2015\u0026#39;] plt.plot(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;], marker=\u0026#39;x\u0026#39;, linestyle=\u0026#34;--\u0026#34;) plt.show()  scatter fonction: function allows you to create scatter plot .rcParamsmethod is used to enlarge the graphic window  plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] plt.scatter(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;]) plt.show()  to use the scatter color option, python wants an input list and not a dictionary color option : c=list()  plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] plt.scatter(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;], c=list(year_to_plot[\u0026#39;Temperature moyenne\u0026#39;])) plt.xlabel(\u0026#34;Temps\u0026#34;) plt.ylabel(\u0026#34;Température\u0026#34;) plt.title(\u0026#34;Temperature\u0026#34;, y=1.05) plt.show()  We used default scatter color () with the cmap option, we can choose our color panel: https://matplotlib.org/examples/color/colormaps_reference.html we will for example choose the color palette \u0026ldquo;seismic\u0026rdquo; via the option cmap  to change the shape and size of the points: use the marker and s options to save a graph: pyplot function: savefig ()  To add a color bar: colorbar () function To rotate the labels in x: function xticks ()   plt.rcParams[\u0026#34;figure.figsize\u0026#34;]=[16,9] plt.scatter(year_to_plot.index,year_to_plot[\u0026#39;Temperature moyenne\u0026#39;], c=list(year_to_plot[\u0026#39;Temperature moyenne\u0026#39;]), cmap=\u0026#34;seismic\u0026#34;, marker=\u0026#34;D\u0026#34;, s=100) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Temperature\u0026#34;) plt.title(\u0026#34;Temperature\u0026#34;, y=1.05) plt.colorbar() plt.xticks(rotation=45) plt.show() plt.savefig(\u0026#34;figures/my_graph.png\u0026#34;, bbox_inches=\u0026#34;tight\u0026#34;) \u0026lt;Figure size 1152x648 with 0 Axes\u0026gt;  Matplotlib classes When creating a graph, matplotlib:\n stores a container for all the graphics stores a container so that the graphic is positioned on a grid stores visual symbols on the graph  You can plot different things in the same figure using the subplot function. Here is an example:\nfig = plt.figure() ax1 = fig.add_subplot(2,2,1) # up and left  ax2 = fig.add_subplot(2,2,2) # up and right  ax3 = fig.add_subplot(2,2,3) # down and left  ax4 = fig.add_subplot(2,2,4) # down and right plt.show() Here is a documentation on subplot: https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot\nAdd data fig = plt.figure() ax1 = fig.add_subplot(2,1,1) ax2 = fig.add_subplot(2,1,2) ax1.plot(dataframe_UQAM[\u0026#39;2017\u0026#39;].index,dataframe_UQAM[\u0026#39;2017\u0026#39;][\u0026#39;Temperature moyenne\u0026#39;]) ax2.plot(dataframe_UQAM[\u0026#39;2016\u0026#39;].index,dataframe_UQAM[\u0026#39;2016\u0026#39;][\u0026#39;Temperature moyenne\u0026#39;]) [\u0026lt;matplotlib.lines.Line2D at 0xa516dd8\u0026gt;]  Improvement of the graph fig = plt.figure(figsize=(15, 8)) colors = [\u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;] for i in range(3): ax = fig.add_subplot(3,1,i+1) year = str(2014+i) label=year plt.plot(dataframe_UQAM[year].index,dataframe_UQAM[year][\u0026#39;Temperature moyenne\u0026#39;], c=colors[i], label = label) plt.legend(loc=\u0026#39;upper left\u0026#39;) plt.show() Example Objectives: to draw a meteogram of the UQAM station for the day of 14/12/2018 import pandas as pd dataframe_UQAM = pd.read_csv(\u0026#39;./DATA/UQAM_DATA_STATION.csv\u0026#39;) print(dataframe_UQAM.head())  Time Precipitation pressure humidex Rosee Temperature Chill \\ 0 18-12-09_09 0.14 1038.0 -8.0 -7.8 -5.1 -8.0 1 18-12-09_10 0.00 1039.0 -7.0 -6.7 -3.9 -7.0 2 18-12-09_11 0.00 1033.0 -5.0 -6.0 -2.7 -5.0 3 18-12-09_12 0.00 1018.0 -2.0 -5.8 -0.2 -3.0 4 18-12-09_13 0.00 1106.0 -2.0 -6.0 0.0 -5.0 Humidite Dir_wind Mod_wind 0 81.0 208.0 6.0 1 81.0 201.0 6.0 2 78.0 184.0 5.0 3 66.0 208.0 8.0 4 64.0 268.0 18.0  dataframe_UQAM2 = pd.read_csv(\u0026#39;./DATA/UQAM_DATA_STATION.csv\u0026#39;, parse_dates=[\u0026#34;Time\u0026#34;],date_parser=lambda x: pd.to_datetime(x, format=\u0026#34;%y-%m-%d_%H\u0026#34;)) dataframe_UQAM2.head()     Time Precipitation pressure humidex Rosee Temperature Chill Humidite Dir_wind Mod_wind     0 2018-12-09 09:00:00 0.14 1038 -8 -7.8 -5.1 -8 81 208 6   1 2018-12-09 10:00:00 0 1039 -7 -6.7 -3.9 -7 81 201 6   2 2018-12-09 11:00:00 0 1033 -5 -6 -2.7 -5 78 184 5   3 2018-12-09 12:00:00 0 1018 -2 -5.8 -0.2 -3 66 208 8   4 2018-12-09 13:00:00 0 1106 -2 -6 0 -5 64 268 18    # We just want data for 14/12/2018 start_date = \u0026#39;2018-12-14\u0026#39; end_date = \u0026#39;2018-12-15\u0026#39; df= dataframe_UQAM2 mask = (df[\u0026#39;Time\u0026#39;] \u0026gt; start_date) \u0026amp; (df[\u0026#39;Time\u0026#39;] \u0026lt;= end_date) dataframe_jour = dataframe_UQAM2.loc[mask] print(dataframe_jour.head()) print(len(dataframe_jour))  Time Precipitation pressure humidex Rosee Temperature \\ 112 2018-12-14 01:00:00 0.0 1039.0 -10.0 -10.0 -6.3 113 2018-12-14 02:00:00 0.0 1039.0 -9.0 -9.0 -5.8 114 2018-12-14 03:00:00 0.0 1036.0 -9.0 -8.5 -5.4 115 2018-12-14 04:00:00 0.0 1036.0 -9.0 -8.4 -5.3 116 2018-12-14 05:00:00 0.0 1037.0 -8.0 -7.5 -4.8 Chill Humidite Dir_wind Mod_wind 112 -8.0 75.0 208.0 4.0 113 -8.0 78.0 117.0 4.0 114 -7.0 79.0 89.0 4.0 115 -7.0 79.0 79.0 3.0 116 -7.0 81.0 92.0 4.0 24  from matplotlib.font_manager import FontProperties import matplotlib.dates as mdates from datetime import datetime from datetime import timedelta as td daylabels = [] i = 0 for index, row in dataframe_jour.iterrows(): daylabels.append(dataframe_jour.iloc[i][0].replace(minute=0, second=0, microsecond=0).strftime(\u0026#39;%Hh\u0026#39;)) i += 1 print(daylabels) start_date = \u0026#39;2018-12-14\u0026#39; end_date = \u0026#39;2018-12-15\u0026#39; fig = plt.figure(figsize=(20, 8)) fontP = FontProperties() fontP.set_size(\u0026#39;xx-small\u0026#39;) t = np.arange(0, 24, 1) ##### on trace les températures ax1 = plt.subplot(411) ax1.grid(True) plt.plot(t, dataframe_jour[\u0026#39;Temperature\u0026#39;], \u0026#39;r-\u0026#39;, label=\u0026#39;Temperature de l\\\u0026#39;air\u0026#39;, linewidth=2) plt.plot(t, dataframe_jour[\u0026#39;Rosee\u0026#39;], \u0026#39;r--\u0026#39;,label=\u0026#39;Temperature du point de rosee\u0026#39;, linewidth=2) plt.plot(t, dataframe_jour[\u0026#39;Chill\u0026#39;], \u0026#39;g--\u0026#39;,label=\u0026#39;Wind Chill\u0026#39;, linewidth=2) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 1, 1, 0),fontsize =15) plt.ylabel(\u0026#39;Température ($^\\circ$C)\u0026#39;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 15}) plt.setp(ax1.get_xticklabels(), fontsize=15) plt.title(\u0026#39;Meteogram station UQAM:\u0026#39;+ start_date + \u0026#39;/ \u0026#39; + end_date, weight=\u0026#39;bold\u0026#39;).set_fontsize(\u0026#39;20\u0026#39;) plt.setp(ax1.get_xticklabels(), visible=False) ########## TRACE DES ACCUMULATIONS PRECIPITATION ##########  # share x only ax2 = plt.subplot(412, sharex=ax1) ax2.grid(True) t = np.arange(0, 24, 1) width=1 ax2.bar(t,dataframe_jour[\u0026#39;Precipitation\u0026#39;].values,width,color=\u0026#39;b\u0026#39;, label=\u0026#39;Pluie\u0026#39;, linewidth=2) plt.setp(ax2.get_xticklabels(), visible=False) plt.ylim(0, np.round(dataframe_jour[\u0026#39;Precipitation\u0026#39;].max() ) +1) plt.ylabel(\u0026#39;Accumulation (mm)\u0026#39;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 15}) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 0, 1, 1), fontsize =15) # share x only ##### on trace l humidite ax2 = plt.subplot(413, sharex=ax1) ax2.grid(True) plt.plot(t, dataframe_jour[\u0026#39;Humidite\u0026#39;], \u0026#39;b-\u0026#39;, linewidth=2) plt.setp(ax2.get_xticklabels(), visible=False) plt.ylabel(\u0026#39;Humidite relative (%)\u0026#39;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 15}) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 0, 1, 1), fontsize =15) ##### on trace la pression ax3 = plt.subplot(414, sharex=ax1) plt.plot(t, dataframe_jour[\u0026#39;pressure\u0026#39;], \u0026#39;g-\u0026#39;, linewidth=2) plt.xlim(0.01, 24) plt.ylim(np.round(min(dataframe_jour[\u0026#39;pressure\u0026#39;])) - 2, np.round(max(dataframe_jour[\u0026#39;pressure\u0026#39;])) + 2) plt.ylabel(\u0026#39;Pression (hPa)\u0026#39;, {\u0026#39;color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;fontsize\u0026#39;: 15}) plt.legend(loc=\u0026#39;upper left\u0026#39;, ncol=1, bbox_to_anchor=(0, 0, 1, 1), fontsize =15) ax3.grid(True) for label in ax3.get_yticklabels(): label.set_color(\u0026#34;black\u0026#34;) ax3.set(xticks=np.arange(0,len(daylabels),1), xticklabels=daylabels) #Same as plt.xticks spacing = 1 visible = ax3.xaxis.get_ticklabels()[::spacing] for label in ax3.xaxis.get_ticklabels(): if label not in visible: label.set_visible(False) fig.autofmt_xdate() fig.set_size_inches(18.5, 10.5) fileout=\u0026#39;figures/Meteogram_UQAM.png\u0026#39; plt.savefig(fileout) fig.autofmt_xdate() plt.show() ['01h', '02h', '03h', '04h', '05h', '06h', '07h', '08h', '09h', '10h', '11h', '12h', '13h', '14h', '15h', '16h', '17h', '18h', '19h', '20h', '21h', '22h', '23h', '00h']  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"26fc5414d1b2e3baeb0d5ef9e55d56ff","permalink":"/courses/tutorial_python/8-matplotlib_library/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/8-matplotlib_library/","section":"courses","summary":"The Matplotlib library is one of the most used libraries for plotting data in Python.\nMany types of graphics can be developed with this library: https://matplotlib.org/gallery/index.html\nTo illustrate some features of the Python matplotlib.pyplot module (which provides a plotting system similar to that of MATLAB), we will use a database from the UQAM station.\nimport pandas as pd import numpy as np import warnings; warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;) dataframe_UQAM = pd.read_csv(\u0026#39;./DATA/UQAM_DATA_STATION_2018.csv\u0026#39;) dataframe_UQAM[\u0026#39;Date\u0026#39;]=pd.to_datetime(dataframe_UQAM[\u0026#39;Date\u0026#39;]) dataframe_UQAM = dataframe_UQAM.","tags":null,"title":"8 Matplotlib library","type":"docs"},{"authors":null,"categories":null,"content":"1: ECCC temperature data In previous sections, we presented how to use the Pandas library which allowed us to process and manipulate data sets. Combining this with Python's Datetime and Matplotlib libraries, we were able to quickly visualize our data.\nWe will continue to discover the functionality of these libraries in a practical case by analyzing the daily temperature data recorded by one of the Environment and Climate Change Canada stations located in Montreal / McTavish between the period 1948 and 2017 (file named \u0026lsquo;MONTREAL_tasmoy_1948_2017.txt\u0026rsquo; in ./DATA directory)\nTo complete and enrich our analysis, a new Python library will be presented: Seaborn .\n the Seaborn library is based on matplotlib. it allows to draw more complex graphs  For more information:\nhttps://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html\nThis link presents a gallery of chart types to be realized with Seaborn: https://seaborn.pydata.org/examples/index.html\n1- Opening and reading our time series import numpy as np import pandas as pd import datetime from datetime import date import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) df = pd.DataFrame() # We open ascii file and store information in a new DataFrame with open(\u0026#39;./DATA/MONTREAL_tasmoy_1948_2017.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() data_EC_Montreal = [float(row) for row in rows.split()] We have created a 1D field but we have no temporal information in our file. Knowing that our registration covers the period 1948 - 2017, we will format our dates with the datetime module of Python.\n# We know that the time series starts on January 1, 1948 and ends on December 31, 2017 inclusively # We create a Datetime object instance to complete our DataFrame start = date(1948, 1, 1) end = date(2017, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) # We will use DateTime object as DataFrame index df[\u0026#39;datetime\u0026#39;] = rng df.index = df[\u0026#39;datetime\u0026#39;] df[\u0026#39;Temperature Montreal\u0026#39;] = data_EC_Montreal df.head()    datetime datetime Temperature Montreal     1948-01-01 00:00:00 1948-01-01 00:00:00 -12   1948-01-02 00:00:00 1948-01-02 00:00:00 -8.9   1948-01-03 00:00:00 1948-01-03 00:00:00 -3.4   1948-01-04 00:00:00 1948-01-04 00:00:00 -3.4   1948-01-05 00:00:00 1948-01-05 00:00:00 -3.1    dir(df) # the dir () function allows to list the functions that are applicable to our DataFrame object We assigned our time series in a Pandas DataFrame and then formatted the date as a Datetime object.\nIt is now easy to manipulate the dataset and apply some simple functions.\n2- Calculation of indices on the temperature data   We will develop and apply a function to calculate the quantiles of our distribution\n  By resampling our series with the .resample () method of Pandas, we will see how to apply native functions of numpy and apply our own function.\n  # Creating our index that calculates the quantile of the distribution # We use the numpy .percentile () function def percentile(n): def percentile_(x): return np.nanpercentile(x, n) percentile_.__name__ = \u0026#39;percentile_%s\u0026#39; % n return percentile_  .resample () and .agg () methods of Pandas  The .resample () method is very useful for frequency conversion and time series resampling. The object (here DataFrame) must have a data / time index (DatetimeIndex) in order to be used. Several resampling frequencies are available (time, week, month, season, year \u0026hellip;)\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.resample.html\nThe .agg () method is used for aggregation of data according to a list of functions to be applied to each column, resulting in an aggregated result with a hierarchical index.\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html\nIn our example, we will resample our data set by month and calculate for each month the average, the minimum, the maximum and the 90th and 95th quantiles.\nresamp_Montreal = df.resample(\u0026#39;M\u0026#39;).agg([np.mean, np.min, np.max, percentile(90), percentile(95)]) resamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo month     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4 1948 1 Jan   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32 1948 2 Feb   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4 1948 3 Mar   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13 1948 4 Apr   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35 1948 5 May    # simple step to remove the row \u0026#39;Temperature Montreal\u0026#39;  resamp_Montreal = resamp_Montreal.loc[:,\u0026#39;Temperature Montreal\u0026#39;] resamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35    3- Some examples of graphics with the Seaborn library Now that we have some statistics on our DataFrame, we will use Python's Seaborn library to visualize them.\n Example: Heatmap  https://seaborn.pydata.org/generated/seaborn.heatmap.html\nFor example, we would like to observe the variation of the average temperature for all the months of the year and all the years. We are going to define two new columns in our DataFrame in which will be assigned only the years and the months respectively.\nresamp_Montreal[\u0026#39;year\u0026#39;] = resamp_Montreal.index.year resamp_Montreal[\u0026#39;MonthNo\u0026#39;] = resamp_Montreal.index.month resamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4 1948 1   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32 1948 2   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4 1948 3   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13 1948 4   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35 1948 5    Before we plot our heatmap, we need to reorganize our dataframe.\n The .pivot_table () method: this method allows you to cross tables dynamically.  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html\nWe would like to visualize only the average monthly temperatures, so work with the \u0026lsquo;mean\u0026rsquo; column, put the year in Index and have one month per column. The .pivot_table () method allows us to do this.\nMontreal_pivot = resamp_Montreal.pivot_table(values=\u0026#39;mean\u0026#39;,index=\u0026#39;year\u0026#39;,columns=[\u0026#39;MonthNo\u0026#39;]) Montreal_pivot.head()    year 1 2 3 4 5 6 7 8 9 10 11 12     1948 -10.9903 -9.58621 -2.46129 6.64 12.6645 17.7467 21.4903 21.1065 17.15 9.1 5.98333 -2.52258   1949 -5.77419 -5.69286 -2.19677 7.51667 13.5129 20.56 22.871 21.5452 14.62 12.0258 -0.193333 -3.21935   1950 -5.34839 -9.79643 -4.89355 4.71 14.1161 18.9567 21.3258 19.1129 13.6167 10.1 3.95667 -4.83548   1951 -7.35484 -6.48571 -0.490323 7.29667 14.4097 18.27 21.3129 18.6677 15.1333 10.371 0.18 -5.3129   1952 -7.80323 -5.27241 -0.987097 7.99667 12.4065 19.6 23.0806 20.7323 16.03 7.67419 3.65667 -3.18387    We then apply .heatmap()function on our DataFrame.\nimport seaborn as sns import matplotlib.pyplot as plt ax = plt.axes() sns.heatmap(Montreal_pivot) figure = ax.get_figure() figure.set_size_inches(15, 10) plt.show() We can improve our display.\nax = plt.axes() sns.heatmap(Montreal_pivot, cmap=\u0026#39;RdYlGn_r\u0026#39;, linewidths=0.5, annot=True , ax = ax,vmin=-30, vmax=30,center=0, fmt=\u0026#39;.1f\u0026#39;,yticklabels=True, cbar_kws={\u0026#39;label\u0026#39;: \u0026#39;Celcius\u0026#39;}) ax.set_title(\u0026#39;Mean temperature\u0026#39;, weight=\u0026#39;bold\u0026#39;, fontsize=\u0026#34;x-large\u0026#34;) figure = ax.get_figure() figure.set_size_inches(22, 15) plt.show()  Other examples: Boxplot, violin plot, line plot  https://seaborn.pydata.org/generated/seaborn.boxplot.html\nhttps://seaborn.pydata.org/generated/seaborn.violinplot.html\nhttps://seaborn.pydata.org/generated/seaborn.lineplot.html\nAt first, we create a new variable containing the months but in string of characters. For this we apply the .strftime () method of datetime.\nresamp_Montreal[\u0026#39;month\u0026#39;] = resamp_Montreal.index.strftime(\u0026#34;%b\u0026#34;) resamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo month     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4 1948 1 Jan   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32 1948 2 Feb   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4 1948 3 Mar   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13 1948 4 Apr   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35 1948 5 May     Boxplot:  ax = plt.axes() sns.boxplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;mean\u0026#34;, data=resamp_Montreal, palette=\u0026#34;Set1\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show()  Violin plot:  ax = plt.axes() sns.violinplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;mean\u0026#34;, data=resamp_Montreal, palette=\u0026#34;Set1\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show()  Line plot:  ax = plt.axes() sns.lineplot(x=resamp_Montreal.index.year, y=\u0026#34;mean\u0026#34;, hue=\u0026#34;month\u0026#34;, data=resamp_Montreal, palette=\u0026#34;tab10\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show() sns.catplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;percentile_90\u0026#34;, data=resamp_Montreal, kind=\u0026#34;swarm\u0026#34;) plt.show()  We can combine several Seaborn charts:  ax = plt.axes() ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2) sns.lineplot(x=resamp_Montreal.index.year, y=\u0026#34;mean\u0026#34;, hue=\u0026#34;month\u0026#34;, data=resamp_Montreal, palette=\u0026#34;tab10\u0026#34;) ax2 = plt.subplot2grid((2, 2), (1, 0), colspan=1) sns.boxplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;mean\u0026#34;, data=resamp_Montreal, palette=\u0026#34;tab10\u0026#34;) ax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1) sns.violinplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;mean\u0026#34;, data=resamp_Montreal, palette=\u0026#34;tab10\u0026#34;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show() 4- Fonction groupby We previously see .groupby()method from Pandas.\nhttps://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm\nThis method is very useful using datetime objects.\nresamp_Montreal.head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo month     1948-01-31 00:00:00 -10.9903 -23.4 -3.1 -3.6 -3.4 1948 1 Jan   1948-02-29 00:00:00 -9.58621 -20.8 3.6 -2.64 -0.32 1948 2 Feb   1948-03-31 00:00:00 -2.46129 -17.8 7.5 5.6 6.4 1948 3 Mar   1948-04-30 00:00:00 6.64 -1.7 14.5 12.08 13.13 1948 4 Apr   1948-05-31 00:00:00 12.6645 6.7 21.7 16.7 18.35 1948 5 May    We want to group our dataframe by month with .groupeby()method.\nresamp_Montreal_grouped = resamp_Montreal.groupby(\u0026#34;month\u0026#34;) resamp_Montreal_grouped.groups # Pour voir les groupes {'Apr': DatetimeIndex(['1948-04-30', '1949-04-30', '1950-04-30', '1951-04-30', '1952-04-30', '1953-04-30', '1954-04-30', '1955-04-30', '1956-04-30', '1957-04-30', '1958-04-30', '1959-04-30', '1960-04-30', '1961-04-30', '1962-04-30', '1963-04-30', '1964-04-30', '1965-04-30', '1966-04-30', '1967-04-30', '1968-04-30', '1969-04-30', '1970-04-30', '1971-04-30', '1972-04-30', '1973-04-30', '1974-04-30', '1975-04-30', '1976-04-30', '1977-04-30', '1978-04-30', '1979-04-30', '1980-04-30', '1981-04-30', '1982-04-30', '1983-04-30', '1984-04-30', '1985-04-30', '1986-04-30', '1987-04-30', '1988-04-30', '1989-04-30', '1990-04-30', '1991-04-30', '1992-04-30', '1993-04-30', '1994-04-30', '1995-04-30', '1996-04-30', '1997-04-30', '1998-04-30', '1999-04-30', '2000-04-30', '2001-04-30', '2002-04-30', '2003-04-30', '2004-04-30', '2005-04-30', '2006-04-30', '2007-04-30', '2008-04-30', '2009-04-30', '2010-04-30', '2011-04-30', '2012-04-30', '2013-04-30', '2014-04-30', '2015-04-30', '2016-04-30', '2017-04-30'], dtype='datetime64[ns]', name='datetime', freq='12M'), 'Aug': DatetimeIndex(['1948-08-31', '1949-08-31', '1950-08-31', '1951-08-31', '1952-08-31', '1953-08-31', '1954-08-31', '1955-08-31', '1956-08-31', '1957-08-31', '1958-08-31', '1959-08-31', '1960-08-31', '1961-08-31', '1962-08-31', '1963-08-31', '1964-08-31', '1965-08-31', '1966-08-31', '1967-08-31', '1968-08-31', '1969-08-31', '1970-08-31', '1971-08-31', '1972-08-31', '1973-08-31', '1974-08-31', '1975-08-31', '1976-08-31', '1977-08-31', '1978-08-31', '1979-08-31', '1980-08-31', '1981-08-31', '1982-08-31', '1983-08-31', '1984-08-31', '1985-08-31', '1986-08-31', '1987-08-31', '1988-08-31', '1989-08-31', '1990-08-31', '1991-08-31', '1992-08-31', '1993-08-31', '1994-08-31', '1995-08-31', '1996-08-31', '1997-08-31', '1998-08-31', '1999-08-31', '2000-08-31', '2001-08-31', '2002-08-31', '2003-08-31', '2004-08-31', '2005-08-31', '2006-08-31', '2007-08-31', '2008-08-31', '2009-08-31', '2010-08-31', '2011-08-31', '2012-08-31', '2013-08-31', '2014-08-31', '2015-08-31', '2016-08-31', '2017-08-31'], ... }   To iterate through groups:  for MonthNo,group in resamp_Montreal_grouped : print(MonthNo) print(group) Apr mean amin amax percentile_90 percentile_95 year \\ datetime 1948-04-30 6.640000 -1.7 14.5 12.08 13.130 1948 1949-04-30 7.516667 2.0 15.8 12.43 14.370 1949 1950-04-30 4.710000 -4.2 11.7 8.56 10.770 1950 1951-04-30 7.296667 1.4 15.0 11.78 13.105 1951 1952-04-30 7.996667 -1.4 15.3 14.01 15.000 1952 ... [70 rows x 8 columns] Aug mean amin amax percentile_90 percentile_95 year \\ datetime 1948-08-31 21.106452 15.0 27.8 26.40 27.400 1948 1949-08-31 21.545161 15.0 27.0 25.60 26.700 1949 1950-08-31 19.112903 12.5 23.4 22.50 23.050 1950 1951-08-31 18.667742 13.1 23.4 22.00 22.950 1951 1952-08-31 20.732258 14.2 25.6 23.40 25.300 1952 1953-08-31 20.480645 15.6 27.2 25.00 27.000 1953 1954-08-31 19.312903 12.8 24.5 22.00 23.350 1954 ...   To select a group: get_group()  # we grouped our dataframe using month index resamp_Montreal_grouped.get_group(\u0026#39;Dec\u0026#39;).head()    datetime mean amin amax percentile_90 percentile_95 year MonthNo month     1948-12-31 00:00:00 -2.52258 -15 7 2.2 3.9 1948 12 Dec   1949-12-31 00:00:00 -3.21935 -13.9 8.9 5.8 6.7 1949 12 Dec   1950-12-31 00:00:00 -4.83548 -23.4 3.9 0 1.95 1950 12 Dec   1951-12-31 00:00:00 -5.3129 -19.2 13.1 7 11.15 1951 12 Dec   1952-12-31 00:00:00 -3.18387 -17 7.8 2 4.6 1952 12 Dec     Aggregation: Python has several methods are available to perform aggregations on data. It is done using the pandas and numpy libraries. The data must be available or converted to a dataframe to apply the aggregation functions.  # to calculate \u0026#39;percentile_90\u0026#39; in each group.  resamp_Montreal_grouped[\u0026#39;percentile_90\u0026#39;].agg(np.mean) month Apr 12.434848 Aug 24.283582 Dec 1.790000 Feb -0.098529 Jan -1.046324 Jul 25.119242 Jun 23.557385 Mar 5.021493 May 19.483846 Nov 8.615147 Oct 15.280294 Sep 21.236765 Name: percentile_90, dtype: float64  resamp_Montreal_grouped[\u0026#39;percentile_90\u0026#39;].agg(np.size) month Apr 70.0 Aug 70.0 Dec 70.0 Feb 70.0 Jan 70.0 Jul 70.0 Jun 70.0 Mar 70.0 May 70.0 Nov 70.0 Oct 70.0 Sep 70.0 Name: percentile_90, dtype: float64  Exercice : Compute day degres index for Montreal station 1- open and read file \u0026ldquo;MONTREAL_tasmoy_1948_2017.txt\u0026rdquo;\n2- Define datetime index (time range: 01/01/1948 : 31/12/2017 )\n3- Write a function to compute the index\n4- For each year, compute temperature mean, min , max and DG0 index\n5- Make a plot\n1-\nimport numpy as np with open(\u0026#39;./data/MONTREAL_tasmoy_1948_2017.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() data_EC_Montreal = [float(row) for row in rows.split()] 2-\nimport pandas as pd import datetime from datetime import date df = pd.DataFrame() start = date(1948, 1, 1) end = date(2017, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) df[\u0026#39;datetime\u0026#39;] = rng df.index = df[\u0026#39;datetime\u0026#39;] df[\u0026#39;Temperature Montreal\u0026#39;] = data_EC_Montreal 3-\ndef DG0(S): ind_DGO=[] ind_DGO = sum(x for x in S if x \u0026gt;= 0) return ind_DGO 4-\nresamp = df.resample(\u0026#39;AS\u0026#39;) dataset = resamp.agg([np.mean, np.min, np.max, DG0]) dataset.head()    datetime (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;mean\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;amin\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;amax\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;DG0\u0026rsquo;)     1948-01-01 00:00:00 7.23388 -23.4 27.8 3506.1   1949-01-01 00:00:00 8.04767 -15 28.4 3602.9   1950-01-01 00:00:00 6.84877 -23.6 25.9 3318.9   1951-01-01 00:00:00 7.24521 -24.2 25.3 3404.9   1952-01-01 00:00:00 7.85546 -20 27.5 3474.2    5-\ndataset.columns = dataset.columns.droplevel(0) dataset[\u0026#39;DG0\u0026#39;].head() datetime 1948-01-01 3506.1 1949-01-01 3602.9 1950-01-01 3318.9 1951-01-01 3404.9 1952-01-01 3474.2 Freq: AS-JAN, Name: DG0, dtype: float64  import matplotlib.pyplot as plt dataset[\u0026#39;DG0\u0026#39;].plot(figsize=(12,5)) plt.show() # we must filter missing values def DG0bis(S): ind_DGObis=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if ((N2/N) \u0026lt; 0.8): ind_DGObis = np.empty(1) ind_DGObis = np.nan else: ind_DGObis = sum(x for x in S if x \u0026gt;= 0) return ind_DGObis dataset = resamp.agg([np.mean, np.min, np.max, DG0, DG0bis]) dataset.head()    datetime (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;mean\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;amin\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;amax\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;DG0\u0026rsquo;) (\u0026lsquo;Temperature Montreal\u0026rsquo;, \u0026lsquo;DG0bis\u0026rsquo;)     1948-01-01 00:00:00 7.23388 -23.4 27.8 3506.1 3506.1   1949-01-01 00:00:00 8.04767 -15 28.4 3602.9 3602.9   1950-01-01 00:00:00 6.84877 -23.6 25.9 3318.9 3318.9   1951-01-01 00:00:00 7.24521 -24.2 25.3 3404.9 3404.9   1952-01-01 00:00:00 7.85546 -20 27.5 3474.2 3474.2    dataset.columns = dataset.columns.droplevel(0) dataset[\u0026#39;DG0\u0026#39;].plot(figsize=(12,5)) dataset[\u0026#39;DG0bis\u0026#39;].plot(figsize=(12,5)) plt.show() ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"cec6dfc0e49e5bd66e97e1ef6b926e47","permalink":"/courses/tutorial_python/project1-eccc_temperature/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/project1-eccc_temperature/","section":"courses","summary":"1: ECCC temperature data In previous sections, we presented how to use the Pandas library which allowed us to process and manipulate data sets. Combining this with Python's Datetime and Matplotlib libraries, we were able to quickly visualize our data.\nWe will continue to discover the functionality of these libraries in a practical case by analyzing the daily temperature data recorded by one of the Environment and Climate Change Canada stations located in Montreal / McTavish between the period 1948 and 2017 (file named \u0026lsquo;MONTREAL_tasmoy_1948_2017.","tags":null,"title":"Project 1 ECCC temperature data","type":"docs"},{"authors":null,"categories":null,"content":"ECCC Precipitation Data We will continue to discover the functionality of these libraries in a practical case by now analyzing the daily precipitation data recorded by the ECCC stations located at the Montreal Trudeau Airport between the period 1961 and 2010.\nWe will use \u0026lsquo;MONTREAL_preacc_1961_2010.dat\u0026rsquo; file in .data/ directory.\n1- Opening and reading our time series import numpy as np import pandas as pd import datetime from datetime import date import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) df_Precipitation = pd.DataFrame() # Ouverture du fichier text  with open(\u0026#39;./DATA/MONTREAL_preacc_1961_2010.dat\u0026#39;, \u0026#39;r\u0026#39;) as file: rows = file.read() data_EC_Montreal = [float(row) for row in rows.split()] # We know that the time series begins on January 1, 1961 # and ends on December 31, 2010 # We can rebuild the index of Dataframes start = date(1961, 1, 1) end = date(2010, 12, 31) delta=(end-start) nb_days = delta.days + 1 rng = pd.date_range(start, periods=nb_days, freq=\u0026#39;D\u0026#39;) # Create a column from the datetime datatype df_Precipitation[\u0026#39;datetime\u0026#39;] = rng # Set the datetime column as the index df_Precipitation.index = df_Precipitation[\u0026#39;datetime\u0026#39;] # Create a column from the numeric score variable df_Precipitation[\u0026#39;Precipitation Montreal\u0026#39;] = data_EC_Montreal df_Precipitation.head()    datetime datetime Precipitation Montreal     1961-01-01 00:00:00 1961-01-01 00:00:00 22.66   1961-01-02 00:00:00 1961-01-02 00:00:00 1.05   1961-01-03 00:00:00 1961-01-03 00:00:00 1.05   1961-01-04 00:00:00 1961-01-04 00:00:00 0.66   1961-01-05 00:00:00 1961-01-05 00:00:00 0    2- Applying functions to DataFram: precipitation indices:  CDD: calculation of the maximum number of consecutive dry days (Precipitation \u0026lt;1mm) CWD: calculation of the maximum number of consecutive wet days (Precipitation\u0026gt; 1mm) Prcp1: calculation of the percentage of precipitation days (Precipitation\u0026gt; 1mm) SDII: calculation of the precipitation intensity average for wet days (Precipitation\u0026gt; 1mm  # calculation of the maximum number of consecutive dry days (inf to 1mm)  # on the incoming signal (less than 20% of missing values)  def CDD(S): import numpy as np ind_CDD=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if ((N2/N) \u0026lt; 0.8): ind_CDD = np.empty(1) ind_CDD = np.nan else: temp = 0 ind_CDD = 0 j =0 while (j \u0026lt; N2): while (j \u0026lt; N2 ) and (S_no_nan[j] \u0026lt; 1.0 ): j += 1 temp +=1 if ind_CDD \u0026lt; temp: ind_CDD = temp temp = 0 j += 1 return ind_CDD # calculation of the maximum number of consecutive wet days (sup to 1mm)  # on the incoming signal (less than 20% of missing values)  def CWD(S): import numpy as np ind_CWD=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if ((N2/N) \u0026lt; 0.8): ind_CWD = np.empty(1) ind_CWD = np.nan else: temp = 0 ind_CWD = 0 j =0 while (j \u0026lt; N2): while (j \u0026lt; N2 ) and (S_no_nan[j] \u0026gt; 1.0 ): j += 1 temp +=1 if ind_CWD \u0026lt; temp: ind_CWD = temp temp = 0 j += 1 return ind_CWD # Calculation of the percentage of precipitation days  # on the incoming signal (less than 20% of missing values) def Prcp1(S): import numpy as np ind_Prcp1=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if (N2 == 0): N2=1 if ((N2/N) \u0026lt; 0.8): ind_Prcp1 = np.empty(1) ind_Prcp1 = np.nan else: ind_Prcp1 = 0 for row in S_no_nan: if row \u0026gt;= 1 : ind_Prcp1 += 1 ind_Prcp1 = 100 * (ind_Prcp1/N2) return ind_Prcp1 # average precipitation intensity for wet days (PR greater than 1mm) (less than 20% missing values) def SDII(S): import numpy as np ind_SDII=[] S_no_nan = S[~np.isnan(S)] N = len(S) N2 = len(S_no_nan) if ((N2/N) \u0026lt; 0.8): ind_SDII = np.empty(1) ind_SDII = np.nan else: SS = S[S \u0026gt; 1] ind_SDII = np.nanmean(SS) return ind_SDII The previous functions make it possible to calculate precipitation indices on a daily signal. We want to apply these functions per month.\nSo we have to work with a subset of our df_Precipitation dataframe grouped by month.\nIn the same way as the previous section on temperatures, we will apply the .resample (\u0026lsquo;M\u0026rsquo;) method: \u0026ldquo;M\u0026rdquo; for months (it's possible here to work at week , season or year scale) As a reminder, at this stage there is no calculation, the data are simply sorted on a monthly basis.\ndf_Precipitation_resamp = df_Precipitation.resample(\u0026#39;M\u0026#39;) df_Precipitation_resamp.count().head()    datetime datetime Precipitation Montreal     1961-01-31 00:00:00 31 31   1961-02-28 00:00:00 28 28   1961-03-31 00:00:00 31 31   1961-04-30 00:00:00 30 30   1961-05-31 00:00:00 31 31    We can now apply our functions defined previously for each month of our DataFrame. For this, we will use the .agg () method on our subset grouped by month.\nindice_precipitation = df_Precipitation_resamp.agg([CDD, CWD, Prcp1, SDII, np.sum, np.mean]) indice_precipitation.head()    datetime (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;CDD\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;CWD\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;Prcp1\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;SDII\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;sum\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;mean\u0026rsquo;)     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516    3- Filter the data directly on a dataframe  the data can be filtered according to a condition on a column.  We will for example only extract precipitation data greater than or equal to 1mm.\nSDII2=df_Precipitation[df_Precipitation[\u0026#34;Precipitation Montreal\u0026#34;]\u0026gt;=1.0] SDII2.head()    datetime datetime Precipitation Montreal     1961-01-01 00:00:00 1961-01-01 00:00:00 22.66   1961-01-02 00:00:00 1961-01-02 00:00:00 1.05   1961-01-03 00:00:00 1961-01-03 00:00:00 1.05   1961-01-06 00:00:00 1961-01-06 00:00:00 8.65   1961-01-07 00:00:00 1961-01-07 00:00:00 4.05    We first created a Boolean type mask on the column \u0026lsquo;Precipitation Montreal\u0026rsquo; which was then applied to our DataFrame.\n(df_Precipitation[\u0026#34;Precipitation Montreal\u0026#34;]\u0026gt;1.0).head() datetime 1961-01-01 True 1961-01-02 True 1961-01-03 True 1961-01-04 False 1961-01-05 False Name: Precipitation Montreal, dtype: bool  We then group our DataFrame by month and we apply a function by calling .agg() method.\nSDII2 = SDII2.resample(\u0026#39;M\u0026#39;).agg({\u0026#39;SDII2\u0026#39;: lambda x: x.mean()}) SDII2.head()    datetime (\u0026lsquo;SDII2\u0026rsquo;, \u0026lsquo;Precipitation Montreal\u0026rsquo;)     1961-01-31 00:00:00 5.69111   1961-02-28 00:00:00 9.895   1961-03-31 00:00:00 11.8688   1961-04-30 00:00:00 6.68333   1961-05-31 00:00:00 5.13    4- Updating our initial DataFrame We want to add to our initial DataFrame the variable SDII2 previously created.\nindice_precipitation[\u0026#34;SDII2\u0026#34;]=SDII2[:].values indice_precipitation.head()    datetime (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;CDD\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;CWD\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;Prcp1\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;SDII\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;sum\u0026rsquo;) (\u0026lsquo;Precipitation Montreal\u0026rsquo;, \u0026lsquo;mean\u0026rsquo;) (\u0026lsquo;SDII2\u0026rsquo;, \u0026lsquo;')     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13    indice_precipitation.columns = indice_precipitation.columns.droplevel(0) indice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean      1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13    Using index datetime column, we want to create new columns with year and month.\nindice_precipitation[\u0026#39;year\u0026#39;] = indice_precipitation.index.year indice_precipitation[\u0026#39;month\u0026#39;] = indice_precipitation.index.month indice_precipitation[\u0026#39;month\u0026#39;] = indice_precipitation.index.strftime(\u0026#39;%b\u0026#39;) indice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean  year month     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111 1961 Jan   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895 1961 Feb   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688 1961 Mar   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333 1961 Apr   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13 1961 May    We want to save our Dataframe indice_precipitation.\nindice_precipitation.to_csv(\u0026#39;indice_precipitation.csv\u0026#39;) 4- Viewing the results We will visualize our final dataframe index_precipitation with the Seaborn library. However, we will see some examples with multi-dimensional graphics.\n Boxplot: boxplot()function  Let's use our precipitation DataFrame:\nindice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean  year month     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111 1961 Jan   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895 1961 Feb   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688 1961 Mar   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333 1961 Apr   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13 1961 May    We want to visualize the nomber of days with precipitation by month. We will then plot Prcp1 variable from our DataFrame.\nimport seaborn as sns import matplotlib.pyplot as plt ax = plt.axes() sns.boxplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;Prcp1\u0026#34;, data=indice_precipitation, palette=\u0026#34;Set1\u0026#34;) ax.set_title(\u0026#39;Number of days with precipitation in Montreal from 1948 to 2017\u0026#39;) ax.set_ylabel(\u0026#39;%\u0026#39;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show()  Histogram: barplot()function  For this example, we will plot the mean accumulation of prepicipitation by month, to do this we must use \u0026lsquo;sum\u0026rsquo; variable.\nax = plt.axes() sns.barplot(x=\u0026#34;month\u0026#34;, y=\u0026#34;sum\u0026#34;, data=indice_precipitation) ax.set_title(\u0026#39;Monthly mean of cumulated precipitation in Montréal from 1948 to 2017\u0026#39;) ax.set_ylabel(\u0026#39;mm\u0026#39;) figure = ax.get_figure() figure.set_size_inches(12, 8) plt.show()  bivariate graph: jointplot()function  Jointplot displays a relationship between 2 variables (bivariate)\nhttps://seaborn.pydata.org/generated/seaborn.jointplot.html\nsns.jointplot(x=\u0026#34;Prcp1\u0026#34;, y=\u0026#34;sum\u0026#34;, data=indice_precipitation,kind=\u0026#39;reg\u0026#39;, space=0, height=6, ratio=4) plt.show() # kind = \u0026#39;kde\u0026#39;  # kind=\u0026#34;hex\u0026#34; # kind=\u0026#34;reg\u0026#34;  bivariate graph: pairplot()function https://seaborn.pydata.org/generated/seaborn.pairplot.html  # Attributes of interest cols = [\u0026#39;CWD\u0026#39;, \u0026#39;SDII\u0026#39;, \u0026#39;sum\u0026#39;, \u0026#39;Prcp1\u0026#39;, \u0026#39;mean\u0026#39;] pp = sns.pairplot(indice_precipitation[cols], height=1.8, aspect=1.2, markers=\u0026#34;+\u0026#34;, plot_kws=dict(edgecolor=\u0026#34;k\u0026#34;, linewidth=0.5), diag_kws=dict(shade=True), # \u0026#34;diag\u0026#34; adjusts/tunes the diagonal plots diag_kind=\u0026#34;kde\u0026#34;) # use \u0026#34;kde\u0026#34; for diagonal plots fig = pp.fig fig.subplots_adjust(top=0.93, wspace=0.3) fig.suptitle(\u0026#39;Indice de précipitation Pairwise Plots\u0026#39;, fontsize=14, fontweight=\u0026#39;bold\u0026#39;) plt.show()  Correlation matrix: The Pandas library has a method for calculating the correlations between each column of a DataFrame.  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\nindice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean  year month     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111 1961 Jan   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895 1961 Feb   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688 1961 Mar   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333 1961 Apr   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13 1961 May    We first drop \u0026lsquo;year\u0026rsquo; column and then apply .corr() method.\nindice_precipitation2 = indice_precipitation.drop([\u0026#34;year\u0026#34;], axis=1) corr_matrix = indice_precipitation2.corr() corr_matrix     CDD CWD Prcp1 SDII sum mean      CDD 1 -0.288579 -0.687758 0.0253577 -0.480416 -0.481019 0.0251715   CWD -0.288579 1 0.631665 -0.0400715 0.41894 0.417954 -0.0396542   Prcp1 -0.687758 0.631665 1 -0.0645136 0.661685 0.663935 -0.0645538   SDII 0.0253577 -0.0400715 -0.0645136 1 0.659944 0.661113 0.999642   sum -0.480416 0.41894 0.661685 0.659944 1 0.997694 0.660287   mean -0.481019 0.417954 0.663935 0.661113 0.997694 1 0.661422    0.0251715 -0.0396542 -0.0645538 0.999642 0.660287 0.661422 1    We can call the .heatmap () function to visualize our correlation matrix and thus facilitate our interpretations.\nfig, (ax) = plt.subplots(1, 1, figsize=(10,6)) hm = sns.heatmap(corr_matrix, ax=ax, # Axes in which to draw the plot, otherwise use the currently-active Axes. cmap=\u0026#34;coolwarm\u0026#34;, # Color Map. #square=True, # If True, set the Axes aspect to “equal” so each cell will be square-shaped. annot=True, fmt=\u0026#39;.2f\u0026#39;, # String formatting code to use when adding annotations. #annot_kws={\u0026#34;size\u0026#34;: 14}, linewidths=.05) fig.subplots_adjust(top=0.93) fig.suptitle(\u0026#39;Correlation Matrix: Precipitation Indices Montreal Station\u0026#39;, fontsize=14, fontweight=\u0026#39;bold\u0026#39;) plt.show()  Graphiques 3D:  Bonus: add correlation indices with the scipy library The stats module of the Scipy library has many statistical functions. https://docs.scipy.org/doc/scipy/reference/stats.html\ndef corr_pearson(x, y, **kws): r, p = stats.pearsonr(x, y) p_stars = \u0026#39;\u0026#39; if p \u0026lt;= 0.05: p_stars = \u0026#39;*\u0026#39; if p \u0026lt;= 0.01: p_stars = \u0026#39;**\u0026#39; if p \u0026lt;= 0.001: p_stars = \u0026#39;***\u0026#39; # r, _ = stats.spearmanr(x, y) ax = plt.gca() pos = (0, .9) color2=\u0026#39;red\u0026#39; ax.annotate(\u0026#34;Pearson = {:.2f}\u0026#34;.format(r) + p_stars, xy=pos, xycoords=ax.transAxes, color=color2, fontweight=\u0026#39;bold\u0026#39;) def corr_spearman(x, y, **kws): r, p = stats.spearmanr(x, y) p_stars = \u0026#39;\u0026#39; if p \u0026lt;= 0.05: p_stars = \u0026#39;*\u0026#39; if p \u0026lt;= 0.01: p_stars = \u0026#39;**\u0026#39; if p \u0026lt;= 0.001: p_stars = \u0026#39;***\u0026#39; # r, _ = stats.spearmanr(x, y) ax = plt.gca() pos = (0, .6) color2=\u0026#39;darkgreen\u0026#39; ax.annotate(\u0026#34;Spearman = {:.2f}\u0026#34;.format(r) + p_stars, xy=pos, xycoords=ax.transAxes, color=color2, fontweight=\u0026#39;bold\u0026#39;) def corr_kendall(x, y, **kws): r, p = stats.kendalltau(x, y) p_stars = \u0026#39;\u0026#39; if p \u0026lt;= 0.05: p_stars = \u0026#39;*\u0026#39; if p \u0026lt;= 0.01: p_stars = \u0026#39;**\u0026#39; if p \u0026lt;= 0.001: p_stars = \u0026#39;***\u0026#39; # r, _ = stats.spearmanr(x, y) ax = plt.gca() pos = (0, .3) color2=\u0026#39;darkorange\u0026#39; ax.annotate(\u0026#34;Kendall = {:.2f}\u0026#34;.format(r) + p_stars, xy=pos, xycoords=ax.transAxes, color=color2, fontweight=\u0026#39;bold\u0026#39;) indice_precipitation.head()    datetime CDD CWD Prcp1 SDII sum mean  year month     1961-01-31 00:00:00 14 3 29.0323 5.69111 55.09 1.7771 5.69111 1961 Jan   1961-02-28 00:00:00 12 3 28.5714 9.895 80.87 2.88821 9.895 1961 Feb   1961-03-31 00:00:00 16 3 25.8065 11.8687 97.59 3.14806 11.8688 1961 Mar   1961-04-30 00:00:00 4 3 50 6.68333 102.7 3.42333 6.68333 1961 Apr   1961-05-31 00:00:00 5 3 41.9355 5.13 70.22 2.26516 5.13 1961 May    indice_month=indice_precipitation[[\u0026#34;CWD\u0026#34;, \u0026#34;SDII\u0026#34;, \u0026#34;sum\u0026#34;, \u0026#34;Prcp1\u0026#34;]].loc[(indice_precipitation[\u0026#34;month\u0026#34;]==\u0026#34;Jan\u0026#34;)] from scipy import stats import seaborn as sns; sns.set() g = sns.PairGrid(indice_month[[\u0026#34;CWD\u0026#34;, \u0026#34;SDII\u0026#34;, \u0026#34;sum\u0026#34;, \u0026#34;Prcp1\u0026#34;g]]) g.map_upper(corr_pearson) g.map_upper(corr_spearman) g.map_upper(corr_kendall) g.map_lower(sns.regplot) g.map_diag(plt.hist) g.set(alpha=0.5) plt.show() ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ca3308d7dc0eddd7d50033bd96585e4c","permalink":"/courses/tutorial_python/project2-eccc_precipitation/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/tutorial_python/project2-eccc_precipitation/","section":"courses","summary":"ECCC Precipitation Data We will continue to discover the functionality of these libraries in a practical case by now analyzing the daily precipitation data recorded by the ECCC stations located at the Montreal Trudeau Airport between the period 1961 and 2010.\nWe will use \u0026lsquo;MONTREAL_preacc_1961_2010.dat\u0026rsquo; file in .data/ directory.\n1- Opening and reading our time series import numpy as np import pandas as pd import datetime from datetime import date import numpy as np import warnings warnings.","tags":null,"title":"Project 2 ECCC precipitation data","type":"docs"},{"authors":["Victoria Ng","Aamir Fazil","Philippe Gachon","Guillaume Dueymes","Milka Radojević","Mariola Mascarenhas","Sophiya Garasia","Michael A. Johansson","Nicholas H. Ogden"],"categories":["2"],"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"4158d0dfcc50d2770479a4ef0027a9eb","permalink":"/publication/article8/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/article8/","section":"publication","summary":"Background: Chikungunya virus (CHIKV) is a reemerging pathogen transmitted by Aedes aegypti and Aedes albopictus mosquitoes. The ongoing Caribbean outbreak is of concern due to the potential for infected travelers to spread the virus to countries where vectors are present and the population is susceptible. Although there has been no autochthonous transmission of CHIKV in Canada, there is concern that both Ae. albopictus and CHIKV will become established, particularly under projected climate change. We developed risk maps for autochthonous CHIKV transmission in Canada under recent (1981–2010) and projected climate (2011–2040 and 2041–2070). Methods: The risk for CHIKV transmission was the combination of the climatic suitability for CHIKV transmission potential and the climatic suitability for the presence of Ae. albopictus; the former was assessed using a stochastic model to calculate R0 and the latter was assessed by deriving a suitability indicator (SIG) that captures a set of climatic conditions known to influence the ecology of Ae. albopictus. R0 and SIG were calculated for each grid cell in Canada south of 60°N, for each time period and for two emission scenarios, and combined to produce overall risk categories that were mapped to identify areas suitable for transmission and the duration of transmissibility. Findings: The risk for autochthonous CHIKV transmission under recent climate is very low with all of Canada classified as unsuitable or rather unsuitable for transmission. Small parts of southern coastal British Columbia become progressively suitable with short-term and long-term projected climate; the duration of potential transmission is limited to 1–2 months of the year. Interpretation: Although the current risk for autochthonous CHIKV transmission in Canada is very low, our study could be further supported by the routine surveillance of Ae. albopictus in areas identified as potentially suitable for transmission given our uncertainty on the current distribution of this species in Canada.","tags":[],"title":"Assessment of the Probability of Autochthonous Transmission of Chikungunya Virus in Canada under Recent and Projected Climate Change","type":"publication"},{"authors":["E. D. Poan"," 'P. Gachon'"," 'R. Laprise'"," 'R. Aider'"," 'G. Dueymes'"],"categories":["2"],"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"f1739c472160d063dac6ebe9ca6a6ef8","permalink":"/publication/article7/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/article7/","section":"publication","summary":"Extratropical Cyclone (EC) characteristics depend on a combination of large-scale factors and regional processes. However, the latter are considered to be poorly represented in global climate models (GCMs), partly because their resolution is too coarse. This paper describes a framework using possibilities given by regional climate models (RCMs) to gain insight into storm activity during winter over North America (NA). Recent past climate period (1981–2005) is considered to assess EC activity over NA using the NCEP regional reanalysis (NARR) as a reference, along with the European reanalysis ERA-Interim (ERAI) and two CMIP5 GCMs used to drive the Canadian Regional Climate Model—version 5 (CRCM5) and the corresponding regional-scale simulations. While ERAI and GCM simulations show basic agreement with NARR in terms of climatological storm track patterns, detailed bias analyses show that, on the one hand, ERAI presents statistically significant positive biases in terms of EC genesis and therefore occurrence while capturing their intensity fairly well. On the other hand, GCMs present large negative intensity biases in the overall NA domain and particularly over NA eastern coast. In addition, storm occurrence over the northwestern topographic regions is highly overestimated. When the CRCM5 is driven by ERAI, no significant skill deterioration arises and, more importantly, all storm characteristics near areas with marked relief and over regions with large water masses are significantly improved with respect to ERAI. Conversely, in GCMdriven simulations, the added value contributed by CRCM5 is less prominent and systematic, except over western NA areas with high topography and over the Western Atlantic coastlines where the most frequent and intense ECs are located. Despite this significant added-value on seasonal mean characteristics, a caveat is raised on the RCM ability to handle storm temporal ‘seriality’, as a measure of their temporal variability at a given location. In fact, the driving models induce some significant footprints on the RCM skill to reproduce the intra-seasonal pattern of storm activity.","tags":[],"title":"Investigating added value of regional climate modeling in North American winter storm track simulations","type":"publication"},{"authors":["Philippe Gachon","Louise Bussières","Pierre Gosselin","Marie Raphoz","Ray Bustinza","Philippe Martin","Guillaume Dueymes","Denis Gosselin","Sylvain Labrecque","Sharon Jeffers","Abderrahmane Yagouti"],"categories":["4"],"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"fefd7849ab83658d135df5051d64595b","permalink":"/publication/article6/","publishdate":"2016-09-01T00:00:00Z","relpermalink":"/publication/article6/","section":"publication","summary":"Among natural-disaster risks, heat waves are responsible for a large number of deaths, diseases and economic losses around the world. As they will increase in severity, duration and frequency over the decades to come within the context of climate change, these extreme events constitute a genuine danger to human health, and heat-warning systems are strongly recommended by public health authorities to reduce this risk of diseases and of excessive mortality and morbidity. Thus, evidence-based public alerting criteria are needed to reduce impacts on human health before and during persistent hot weather conditions. The goal of this guide is to identify alert thresholds for heat waves in Canada based on evidence, and to propose an approach for better defining heat waves in the Canadian context in order to reduce the risks to human health and contribute to the well-being of Canadians. This guide is the result of the collaboration among various research and public institutions working on: 1) meteorological and climate aspects, i.e. the Meteorological Service of Canada (MSC, Environment and Climate Change Canada), and the ESCER centre at the Université du Québec à Montréal, and 2) public health, i.e. Health Canada and the Institut National de Santé Publique du Québec.","tags":[],"title":"Guide to identifying alert thresholds for heat waves in Canada based on evidence","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["E. D. Poan","P. Gachon","G.Dueymes","E. Diaconescu","R. Laprise","I. Seidou Sanda"],"categories":["2"],"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"2a86ded52b77b30f18f35e33be53fc7f","permalink":"/publication/article5/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/publication/article5/","section":"publication","summary":"The West African monsoon intraseasonal variability has huge socio-economic impacts on local populations but understanding and predicting it still remains a challenge for the weather prediction and climate scientific community. This paper analyses an ensemble of simulations from six regional climate models (RCMs) taking part in the coordinated regional downscaling experiment, the ECMWF ERA-Interim reanalysis (ERAI) and three satellite-based and observationally-constrained daily precipitation datasets, to assess the performance of the RCMs with regard to the intraseasonal variability. A joint analysis of seasonal-mean precipitation and the total column water vapor (also called precipitable water—PW) suggests the existence of important links at different timescales between these two variables over the Sahel and highlights the relevance of using PW to follow the monsoon seasonal cycle. RCMs that fail to represent the seasonal-mean position and amplitude of the meridional gradient of PW show the largest discrepancies with respect to seasonal-mean observed precipitation. For both ERAI and RCMs, spectral decompositions of daily PW as well as rainfall show an overestimation of low-frequency activity (at timescales longer than 10 days) at the expense of the synoptic (timescales shorter than 10 days) activity. Consequently, the effects of the African Easterly Waves and the associated mesoscale convective systems are substantially underestimated, especially over continental regions. Finally, the study investigates the skill of the models with respect to hydro-climatic indices related to the occurrence, intensity and frequency of precipitation events at the intraseasonal scale. Although most of these indices are generally better reproduced with RCMs than reanalysis products, this study indicates that RCMs still need to be improved (especially with respect to their subgrid-scale parameterization schemes) to be able to reproduce the intraseasonal variance spectrum adequately..","tags":[],"title":"West African monsoon intraseasonal activity and its daily precipitation indices in regional climate models: diagnostics and challenges.","type":"publication"},{"authors":["P.Gachon","G.Dueymes","Pierre Gosselin","Olivier Gagnon"],"categories":["4"],"content":"","date":1409529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409529600,"objectID":"875b9a6ebbb9ed7c424c037e449aa037","permalink":"/publication/article4/","publishdate":"2014-09-01T00:00:00Z","relpermalink":"/publication/article4/","section":"publication","summary":"Renforcer la capacité d’intervention et d’adaptation en santé publique nécessite d’améliorer l’efficacité des systèmes d’alerte précoce vis-à-vis des risques climatiques en évolution. Ceci implique des ajustements aux activités en cours, voire de modifier les façons de faire au sein des organisations et entre les organisations en augmentant, notamment, leurs collaborations. L’interdisciplinarité au service de la santé publique est donc de mise.","tags":[],"title":"L’interdisciplinarité au service de la santé: cas du Québec. Territoires incubateurs de santé ?","type":"publication"},{"authors":["Éric Girard","Guillaume Dueymes","Ping Du","Allan K Bertram"],"categories":["2"],"content":"","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"28f40025f8865d42e9918ea6c2452713","permalink":"/publication/article2/","publishdate":"2013-03-01T00:00:00Z","relpermalink":"/publication/article2/","section":"publication","summary":"Owing to the large-scale transport of pollution-derived aerosols from the mid-latitudes to the Arctic, most of the aerosols are coated with acidic sulfate during winter in the Arctic. Recent laboratory experiments have shown that acid coating on dust particles substantially reduces the ability of these particles to nucleate ice crystals. Simulations performed using the Limited Area version of the Global Multiscale Environmental Model (GEM-LAM) are used to assess the potential effect of acid-coated ice nuclei on the Arctic cloud and radiation processes during January and February 2007. Ice nucleation is treated using a new parameterization based on laboratory experiments of ice nucleation on sulphuric acid-coated and uncoated kaolinite particles. Results show that acid coating on dust particles has an important effect on cloud microstructure, atmospheric dehydration, radiation and temperature over the Central Arctic, which is the coldest part of the Arctic. Mid and upper ice clouds are optically thinner while low-level mixed-phase clouds are more frequent and persistent. These changes in the cloud microstructures affect the radiation at the top of the atmosphere with longwave negative cloud forcing values ranging between 0 and −6 W m −2 over the region covered by the Arctic air mass..","tags":[],"title":"Assessment of the effects of acid-coated ice nuclei on the Arctic cloud microstructure, atmospheric dehydration, radiation and temperature during winter ","type":"publication"},{"authors":["P.Gachon","M.Radojevic","A. Harding","L.Benyahya","R.Laprise","N.Khaliq","P.Roy","H-II Eum","G.Dueymes"],"categories":["4"],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"afda9015bd1fb44241aa8afde7145eff","permalink":"/publication/article3/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/article3/","section":"publication","summary":"","tags":[],"title":"Evaluation of regional Climate Model simulations: intercomparaisonover Canada and specific region ","type":"publication"},{"authors":["Éric Girard","A. Stefanof","M. Peltier-Champigny","Rodrigo Munoz-Alpizar","Guillaume Dueymes","Jean-Pierre Blanchet"],"categories":["2"],"content":"","date":1193875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1193875200,"objectID":"7ed48ed44aaee1b5f50f1eb03514b9c0","permalink":"/publication/assessment_of_dehydration/","publishdate":"2007-11-01T00:00:00Z","relpermalink":"/publication/assessment_of_dehydration/","section":"publication","summary":"The effect of pollution-derived sulphuric acid aerosols on the aerosol-cloud-radiation interactions is investigated over the Arctic for February 1990. Observations suggest that acidic aerosols can decrease the heterogeneous nucleation rate of ice crystals and lower the homogeneous freezing temperature of haze droplets. Based on these observations, we hypothesize that the cloud thermodynamic phase is modified in polluted air mass (Arctic haze). Cloud ice number concentration is reduced, thus promoting further ice crystal growth by the Bergeron-Findeisen process. Hence, ice crystals reach larger sizes and low-level ice crystal precipitation from mixed-phase clouds increases. Enhanced dehydration of the lower troposphere contributes to decrease the water vapour greenhouse effect and cool the surface. A positive feedback is created between surface cooling and air dehydration, accelerating the cold air production. This process is referred to as the dehydration-greenhouse feedback (DGF). Simulations performed using an arctic regional climate model for February 1990, February and March 1985 and 1995 are used to assess the potential effect of the DGF on the Arctic climate. Results show that the DGF has an important effect over the Central and Eurasian Arctic, which is the coldest part of the Arctic with a surface cooling ranging between 0 and -3K. Moreover, the lower tropospheric cooling over the Eurasian and Central Arctic strengthens the atmospheric circulation at upper level, thus increasing the aerosol transport from the mid-latitudes and enhancing the DGF. Over warmer areas, the increased aerosol concentration (caused by the DGF) leads to longer cloud lifetime, which contributes to warm these areas. It is also shown that the maximum ice nuclei reduction must be of the order of 100 to get a significant effect.","tags":[],"title":"Assessment of the Dehydration-Greenhouse Feedback Over the Arctic During Winter","type":"publication"}]